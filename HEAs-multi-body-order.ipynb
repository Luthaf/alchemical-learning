{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cProfile\n",
    "import time\n",
    "\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "\n",
    "from utils.combine import CombineRadial, CombineRadialSpecies, CombineSpecies\n",
    "from utils.dataset import AtomisticDataset, create_dataloader\n",
    "from utils.linear import LinearModel\n",
    "from utils.operations import SumStructures, remove_gradient\n",
    "from utils.soap import PowerSpectrum, CompositionFeatures\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 10000 training frames\n"
     ]
    }
   ],
   "source": [
    "n_test = 1000\n",
    "n_train = 10000\n",
    "\n",
    "frames = ase.io.read(\"data/data_shuffle.xyz\", f\":{n_test + n_train}\")\n",
    "\n",
    "train_frames = frames[:n_train]\n",
    "test_frames = frames[-n_test:]\n",
    "\n",
    "train_energies = torch.tensor(\n",
    "    [frame.info[\"energy\"] for frame in train_frames]\n",
    ").reshape(-1, 1).to(dtype=torch.get_default_dtype())\n",
    "\n",
    "test_energies = torch.tensor(\n",
    "    [frame.info[\"energy\"] for frame in test_frames]\n",
    ").reshape(-1, 1).to(dtype=torch.get_default_dtype())\n",
    "\n",
    "train_forces = [\n",
    "    torch.tensor(frame.arrays[\"forces\"]).to(dtype=torch.get_default_dtype())\n",
    "    for frame in train_frames\n",
    "]\n",
    "\n",
    "test_forces = [\n",
    "    torch.tensor(frame.arrays[\"forces\"]).to(dtype=torch.get_default_dtype()) \n",
    "    for frame in test_frames\n",
    "]\n",
    "\n",
    "print(f\"using {n_train} training frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_species = set()\n",
    "for frame in frames:\n",
    "    all_species.update(frame.numbers)\n",
    "\n",
    "all_species = list(map(lambda u: int(u), all_species))\n",
    "\n",
    "# HYPERS_FROM_PAPER = {\n",
    "#     \"interaction_cutoff\": 5.0,\n",
    "#     \"max_angular\": 9,\n",
    "#     \"max_radial\": 12,\n",
    "#     \"gaussian_sigma_constant\": 0.3,\n",
    "#     \"gaussian_sigma_type\": \"Constant\",\n",
    "#     \"cutoff_smooth_width\": 0.5,\n",
    "#     \"radial_basis\": \"GTO\",\n",
    "#     \"compute_gradients\": False,\n",
    "#     \"expansion_by_species_method\": \"user defined\",\n",
    "#     \"global_species\": all_species,\n",
    "# }\n",
    "\n",
    "HYPERS_SMALL = {\n",
    "    \"cutoff\": 4.0,\n",
    "    \"max_angular\": 3,\n",
    "    \"max_radial\": 6,\n",
    "    \"atomic_gaussian_width\": 0.5,\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"radial_basis\": {\"SplinedGto\": {\"accuracy\": 1e-6}},\n",
    "    \"gradients\": False,\n",
    "    \"center_atom_weight\": 1.0,\n",
    "    # # TODO: implement this in rascaline itself\n",
    "    # \"radial_per_angular\": {\n",
    "    #     # l: n\n",
    "    #     0: 10,\n",
    "    #     1: 8,\n",
    "    #     2: 8,\n",
    "    #     3: 4,\n",
    "    #     4: 4,\n",
    "    # }\n",
    "}\n",
    "\n",
    "HYPERS_RADIAL = {\n",
    "    \"cutoff\": 5.0,\n",
    "    \"max_angular\": 0,\n",
    "    \"max_radial\": 10,\n",
    "    \"atomic_gaussian_width\": 0.5,\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"radial_basis\": {\"SplinedGto\": {\"accuracy\": 1e-6}},\n",
    "    \"gradients\": False,\n",
    "    \"center_atom_weight\": 1.0,    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop, energies only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AtomisticDataset(train_frames, all_species, \n",
    "                                 {\"radial_spectrum\": HYPERS_RADIAL, \"spherical_expansion\":HYPERS_SMALL}, train_energies)\n",
    "test_dataset = AtomisticDataset(test_frames, all_species, \n",
    "                                {\"radial_spectrum\": HYPERS_RADIAL, \"spherical_expansion\":HYPERS_SMALL}, test_energies)\n",
    "\n",
    "do_gradients = False\n",
    "if do_gradients is True:\n",
    "    HYPERS_GRAD = copy.deepcopy(HYPERS_SMALL)\n",
    "    HYPERS_GRAD[\"gradients\"] = do_gradients\n",
    "    train_dataset_grad = AtomisticDataset(train_frames, all_species, HYPERS_GRAD, train_energies, train_forces, radial_spectrum_n_max=12)\n",
    "    test_dataset_grad = AtomisticDataset(test_frames, all_species, HYPERS_GRAD, test_energies, test_forces, radial_spectrum_n_max=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=1000,\n",
    "    shuffle=True,    \n",
    "    device=device,\n",
    ")\n",
    "\n",
    "train_dataloader_no_batch = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=len(train_dataset),\n",
    "    shuffle=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "train_dataloader_single_frame = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "test_dataloader = create_dataloader(\n",
    "    test_dataset,\n",
    "    batch_size=200,\n",
    "    shuffle=False,\n",
    "    device=device,\n",
    ")\n",
    "if do_gradients is True:\n",
    "    train_dataloader_grad = create_dataloader(\n",
    "        train_dataset_grad,\n",
    "        batch_size=50,\n",
    "        shuffle=True,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    train_dataloader_grad_no_batch = create_dataloader(\n",
    "        train_dataset_grad,\n",
    "        batch_size=len(train_dataset_grad),\n",
    "        shuffle=False,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    test_dataloader_grad = create_dataloader(\n",
    "        test_dataset_grad,\n",
    "        batch_size=50,\n",
    "        shuffle=False,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mae(predicted, actual):\n",
    "    return torch.sum(torch.abs(predicted.flatten() - actual.flatten()))\n",
    "\n",
    "def loss_mse(predicted, actual):\n",
    "    return torch.sum((predicted.flatten() - actual.flatten())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedPowerSpectrum(torch.nn.Module):\n",
    "    def __init__(self, combiner):\n",
    "        super().__init__()\n",
    "\n",
    "        self.combiner = combiner\n",
    "        self.power_spectrum = PowerSpectrum()\n",
    "\n",
    "    def forward(self, spherical_expansion):\n",
    "        combined = self.combiner(spherical_expansion)\n",
    "\n",
    "        return self.power_spectrum(combined)\n",
    "\n",
    "        \n",
    "class MultiBodyOrderModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        power_spectrum,\n",
    "        composition_regularizer,\n",
    "        radial_spectrum_regularizer,\n",
    "        power_spectrum_regularizer,        \n",
    "        optimizable_weights,\n",
    "        random_initial_weights,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sum_structure = SumStructures()\n",
    "\n",
    "        # optimizable_weights = False is not very well tested ...\n",
    "        assert optimizable_weights\n",
    "\n",
    "        if composition_regularizer is None:\n",
    "            self.composition_model = None\n",
    "        else:\n",
    "            self.composition_model=LinearModel(\n",
    "            regularizer=composition_regularizer,\n",
    "            optimizable_weights=optimizable_weights,\n",
    "            random_initial_weights=random_initial_weights,\n",
    "        )\n",
    "        \n",
    "        if radial_spectrum_regularizer is None:\n",
    "            self.radial_spectrum_model = None\n",
    "        else:\n",
    "            self.radial_spectrum_model = LinearModel(\n",
    "                regularizer=radial_spectrum_regularizer,\n",
    "                optimizable_weights=optimizable_weights,\n",
    "                random_initial_weights=random_initial_weights,\n",
    "            )\n",
    "\n",
    "        if power_spectrum_regularizer is None:\n",
    "            self.power_spectrum_model = None\n",
    "        else:\n",
    "            self.power_spectrum = power_spectrum\n",
    "            self.power_spectrum_model = LinearModel(\n",
    "                regularizer=power_spectrum_regularizer,\n",
    "                optimizable_weights=optimizable_weights,\n",
    "                random_initial_weights=random_initial_weights,\n",
    "            )\n",
    "\n",
    "        self.combiner = combiner, \n",
    "        self.optimizable_weights = optimizable_weights\n",
    "        self.random_initial_weights = random_initial_weights\n",
    "\n",
    "    def forward(self, composition, radial_spectrum, spherical_expansion, forward_forces=False):\n",
    "        if not forward_forces:\n",
    "            # remove gradients if we don't need them\n",
    "            spherical_expansion = remove_gradient(spherical_expansion)\n",
    "            if radial_spectrum is not None:\n",
    "                radial_spectrum = remove_gradient(radial_spectrum)                \n",
    "    \n",
    "        energies, forces = None, None\n",
    "        \n",
    "        if self.composition_model is not None:\n",
    "            energies_cmp, _ = self.composition_model(composition)\n",
    "            energies = energies_cmp\n",
    "            forces = None\n",
    "    \n",
    "        if self.radial_spectrum_model is not None:\n",
    "            radial_spectrum_per_structure = radial_spectrum #self.sum_structure(radial_spectrum)\n",
    "            energies_rs, forces_rs = self.radial_spectrum_model(radial_spectrum_per_structure, with_forces=forward_forces)\n",
    "            \n",
    "            if energies is None:\n",
    "                energies = energies_rs  \n",
    "            else:\n",
    "                energies += energies_rs              \n",
    "            if forces_rs is not None:\n",
    "                if forces is None:\n",
    "                    forces = forces_rs\n",
    "                else:\n",
    "                    forces += forces_rs\n",
    "\n",
    "        if self.power_spectrum_model is not None:\n",
    "            power_spectrum = self.power_spectrum(spherical_expansion)\n",
    "            power_spectrum_per_structure = self.sum_structure(power_spectrum)\n",
    "\n",
    "            energies_ps, forces_ps = self.power_spectrum_model(power_spectrum_per_structure, with_forces=forward_forces)\n",
    "            if energies is None:\n",
    "                energies = energies_ps\n",
    "            else:\n",
    "                energies += energies_ps\n",
    "            if forces_ps is not None:\n",
    "                if forces is None:\n",
    "                    forces = forces_ps\n",
    "                else:\n",
    "                    forces += forces_ps\n",
    "        \n",
    "        return energies, forces\n",
    "\n",
    "    def initialize_model_weights(self, composition, radial_spectrum, spherical_expansion, energies, forces=None, seed=None):\n",
    "        if forces is None:\n",
    "            # remove gradients if we don't need them\n",
    "            spherical_expansion = remove_gradient(spherical_expansion)\n",
    "            if radial_spectrum is not None:\n",
    "                radial_spectrum = remove_gradient(radial_spectrum)\n",
    "            \n",
    "        if self.composition_model is not None:\n",
    "            self.composition_model.initialize_model_weights(composition, energies, forces, seed)\n",
    "        \n",
    "        if self.radial_spectrum_model is not None:\n",
    "            radial_spectrum_per_structure = self.sum_structure(radial_spectrum)\n",
    "            self.radial_spectrum_model.initialize_model_weights(radial_spectrum_per_structure, energies, forces, seed)\n",
    "        \n",
    "        if self.power_spectrum_model is not None:        \n",
    "            power_spectrum = self.power_spectrum(spherical_expansion)\n",
    "            power_spectrum_per_structure = self.sum_structure(power_spectrum)\n",
    "            self.power_spectrum_model.initialize_model_weights(power_spectrum_per_structure, energies, forces, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# species combination only\n",
    "N_PSEUDO_SPECIES = 4\n",
    "combiner = CombineSpecies(species=all_species, n_pseudo_species=N_PSEUDO_SPECIES)\n",
    "\n",
    "# # species combination and then radial basis combination\n",
    "# N_COMBINED_RADIAL = 4\n",
    "# combiner = torch.nn.Sequential(\n",
    "#     CombineSpecies(species=all_species, n_pseudo_species=N_PSEUDO_SPECIES),\n",
    "#     CombineRadial(max_radial=HYPERS_SMALL[\"max_radial\"], n_combined_radial=N_COMBINED_RADIAL),\n",
    "# )\n",
    "\n",
    "# # combine both radial and species information at the same time\n",
    "# combiner = CombineRadialSpecies(\n",
    "#     n_species=len(all_species), \n",
    "#     max_radial=HYPERS_SMALL[\"max_radial\"], \n",
    "#     n_combined_basis=N_COMBINED_RADIAL*N_PSEUDO_SPECIES,\n",
    "# )\n",
    "\n",
    "composition=CompositionFeatures(all_species, device=device)\n",
    "power_spectrum = CombinedPowerSpectrum(combiner)\n",
    "\n",
    "LINALG_REGULARIZER_ENERGIES = 1e-2\n",
    "LINALG_REGULARIZER_FORCES = 1e-1\n",
    "\n",
    "model = MultiBodyOrderModel(\n",
    "    power_spectrum=power_spectrum, \n",
    "    composition_regularizer=[1e-10],\n",
    "    radial_spectrum_regularizer=[LINALG_REGULARIZER_ENERGIES, LINALG_REGULARIZER_FORCES],\n",
    "    power_spectrum_regularizer=[LINALG_REGULARIZER_ENERGIES, LINALG_REGULARIZER_FORCES],\n",
    "    optimizable_weights=True, \n",
    "    random_initial_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.optimizable_weights:\n",
    "    TORCH_REGULARIZER_COMPOSITION = 1e-8\n",
    "    TORCH_REGULARIZER_RADIAL_SPECTRUM = 1e-4\n",
    "    TORCH_REGULARIZER_POWER_SPECTRUM = 1e-3\n",
    "else:\n",
    "    TORCH_REGULARIZER_RADIAL_SPECTRUM = 0.0\n",
    "    TORCH_REGULARIZER_POWER_SPECTRUM = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiBodyOrderModel(\n",
       "  (sum_structure): SumStructures()\n",
       "  (composition_model): LinearModel()\n",
       "  (radial_spectrum_model): LinearModel()\n",
       "  (power_spectrum): CombinedPowerSpectrum(\n",
       "    (combiner): CombineSpecies()\n",
       "    (power_spectrum): PowerSpectrum()\n",
       "  )\n",
       "  (power_spectrum_model): LinearModel()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device=device, dtype=torch.get_default_dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.random_initial_weights:\n",
    "    dataloader_initialization = train_dataloader_single_frame\n",
    "else:\n",
    "    dataloader_initialization = train_dataloader_no_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "with torch.no_grad():\n",
    "    for composition, radial_spectrum, spherical_expansions, energies, _ in dataloader_initialization:\n",
    "        # we want to intially train the model on all frames, to ensure the\n",
    "        # support points come from the full dataset.\n",
    "        model.initialize_model_weights(composition, radial_spectrum, spherical_expansions, energies, seed=12345)\n",
    "        break\n",
    "\n",
    "del radial_spectrum, spherical_expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'active_bytes.all.current'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-95a1def1732e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'active_bytes.all.current'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'active_bytes.all.current'"
     ]
    }
   ],
   "source": [
    "torch.cuda.memory_stats()['active_bytes.all.current']/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'allocated_bytes.all.current'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-38d5cb4c7560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allocated_bytes.all.current'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'allocated_bytes.all.current'"
     ]
    }
   ],
   "source": [
    "torch.cuda.memory_stats()['allocated_bytes.all.current']/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reserved_bytes.all.current'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-857b37fe3c5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reserved_bytes.all.current'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'reserved_bytes.all.current'"
     ]
    }
   ],
   "source": [
    "torch.cuda.memory_stats()['reserved_bytes.all.current']/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.2\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=lr, line_search_fn=\"strong_wolfe\", history_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)#, line_search_fn=\"strong_wolfe\", history_size=128)\n",
    "\n",
    "all_losses = []\n",
    "all_tests=[]\n",
    "\n",
    "filename = f\"{model.__class__.__name__}-{N_PSEUDO_SPECIES}-mixed-{n_train}-train\"\n",
    "if model.optimizable_weights:\n",
    "    filename += \"-opt-weights\"\n",
    "\n",
    "if model.random_initial_weights:\n",
    "    filename += \"-random-weights\"\n",
    "\n",
    "output = open(f\"{filename}.dat\", \"w\")\n",
    "output.write(\"# epoch  train_loss  test_mae\\n\")\n",
    "n_epochs_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.detach().cpu().numpy().nbytes for p in model.parameters())\n",
    "train_dataset._collatetime = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426124.9856723694 10466.428263161522\n",
      "421352.56854786153 10419.469529047206\n",
      "400997.24681594 10214.299612973076\n",
      "360811.1345686371 9770.902628768723\n",
      "328000.3286096472 9376.020644526277\n",
      "300099.9759131623 9012.8930650611\n",
      "275777.74434368475 8671.374927603829\n",
      "253391.4426191513 8331.945268218135\n",
      "230878.4323264811 7960.683639754946\n",
      "205400.42343358707 7493.656422717106\n",
      "174182.61938425136 6816.415994914378\n",
      "144292.44461825935 5936.285677639763\n",
      "125639.97053016625 5227.521135296979\n",
      "105615.42884063619 4355.5552338982725\n",
      "91596.52152874946 3671.2997466564398\n",
      "80752.03926054374 3128.018874338242\n",
      "68717.37443537012 2504.0486800575864\n",
      "58893.679668214434 1989.719464870589\n",
      "49637.44414552271 1587.4608400146008\n",
      "39353.778019418554 1241.6980533897156\n",
      "29196.216024818623 986.4136077537904\n",
      "20787.404583509553 783.0672738332369\n",
      "norms 1.459934582072727 45.00363043105861 28.36796565718368\n",
      "gradients 783.0672738332369 1019.5087925581904 13852.917246791842\n",
      "epoch 0 took 525.6s, optimizer loss=4.261e+05, test mae=104.9\n",
      "20787.404583509553 783.0672738332369\n",
      "14987.975547203165 610.0016227743539\n",
      "12412.028033754648 521.6816742707097\n",
      "10611.905707622713 454.77894179477227\n",
      "9083.808121365591 393.30703014526347\n",
      "7868.506819976559 352.18451965015635\n",
      "6877.865101468361 309.4969314213409\n",
      "5989.214489317999 278.20805484306504\n",
      "5122.443523717913 232.53143009133572\n",
      "4358.109439278146 195.4186610895137\n",
      "3878.7406628951644 184.15952031280074\n",
      "3497.1058225889096 157.3667255930739\n",
      "3177.2243684326936 162.63297661398366\n",
      "2921.7023764958553 158.0835842404938\n",
      "2637.9403910296155 163.7173125649893\n",
      "2380.4769739271464 157.31666964104772\n",
      "2115.995380940544 158.5933453522108\n",
      "1904.0211452129367 143.35772231064678\n",
      "1723.8427331237308 127.75900010994295\n",
      "1535.4089048397364 115.5732011692055\n",
      "1425.555631183423 94.20384340593527\n",
      "norms 1.5187096469889525 44.78300493925366 28.38803407085106\n",
      "gradients 94.20384340593527 102.52795473829654 634.0034832844146\n",
      "epoch 1 took 493.0s, optimizer loss=2.079e+04, test mae=29.82\n",
      "1425.555631183423 94.20384340593527\n",
      "1344.4537036835584 88.66161451144824\n",
      "1266.3295445439394 58.74696863506723\n",
      "1218.8751124282046 58.426154210409344\n",
      "1176.8927637122151 45.02738847530874\n",
      "1138.243193302649 47.889738015757445\n",
      "1103.9072808751976 43.73272670661374\n",
      "1059.6025216259034 47.27588177171642\n",
      "1009.558378574868 47.044049969888256\n",
      "962.1202275763142 45.9610453906502\n",
      "920.2856763523515 44.69873619717745\n",
      "887.2733623421134 43.07248997154638\n",
      "856.9343415936454 34.0689691622654\n",
      "833.3446072638512 35.05471380385796\n",
      "814.1410888591649 28.854261936917712\n",
      "797.2944659479738 29.379788294170314\n",
      "781.1025702277765 24.720110242413725\n",
      "766.6575157050225 24.712499170056024\n",
      "751.8248607042548 21.774264724000393\n",
      "734.3485687530407 22.535628940029632\n",
      "716.2756773623389 21.068417714067994\n",
      "norms 1.6228426089215355 44.62638547036303 28.358454572743568\n",
      "gradients 21.068417714067994 49.97968820246735 209.82040954844706\n",
      "epoch 2 took 480.8s, optimizer loss=1.426e+03, test mae=21.84\n",
      "716.2756773623389 21.068417714067994\n",
      "701.6380891558293 18.794274320084448\n",
      "687.4871937064789 22.514264574376284\n",
      "673.4122879882991 18.812505460146152\n",
      "661.2157235304044 21.131248534343246\n",
      "649.4086767672045 20.189863857051776\n",
      "639.2507441436054 20.31318533022734\n",
      "630.2856772442419 20.420371056082757\n",
      "621.1812017080092 22.31189031391125\n",
      "613.607540702285 21.828638749855216\n",
      "605.0147992839893 22.90802611217139\n",
      "595.387583179809 23.5844713167246\n",
      "586.0360077113379 21.62627133746896\n",
      "575.507224521262 23.275941942879587\n",
      "565.4719704760934 22.263317604922772\n",
      "555.5025858154298 19.957645480801133\n",
      "546.295023482718 20.477090554858478\n",
      "537.409259269007 18.364436047888756\n",
      "529.7715479427725 17.412713558400057\n",
      "521.2341900547468 20.17444310868911\n",
      "513.1944543860187 17.65860394205438\n",
      "norms 1.8689668625517557 44.361325208374105 28.412059124721765\n",
      "gradients 17.65860394205438 40.855824172484716 129.74191640581276\n",
      "epoch 3 took 482.5s, optimizer loss=716.3, test mae=18.25\n",
      "513.1944543860187 17.65860394205438\n",
      "505.9045929027155 18.112804571274573\n",
      "496.602082783159 13.388456500630003\n",
      "490.1470806494995 13.929433434507363\n",
      "482.6434877298419 12.304336715063775\n",
      "473.7285771833016 12.16962387442234\n",
      "465.90842020738455 12.198421484187694\n",
      "457.6311491021854 12.237147551119799\n",
      "449.6333959621519 11.802744531892872\n",
      "440.62303517919463 14.377489815322557\n",
      "431.0553162530761 12.759287498825511\n",
      "421.4970559164962 15.623416610740025\n",
      "412.6547234274482 13.95260723950881\n",
      "404.62617808875746 15.455452819012416\n",
      "396.70923095542844 12.928418169546614\n",
      "390.63534224312406 13.508006272711896\n",
      "385.32368842083145 11.945198829756677\n",
      "380.2981940762068 12.712472438890494\n",
      "375.4945674852177 11.003295337991833\n",
      "370.9942314841554 11.759312510338905\n",
      "366.6903413877719 10.84535096404574\n",
      "norms 2.462495822161247 43.96705550615059 28.6465286932384\n",
      "gradients 10.84535096404574 25.76151130602523 77.01869631766579\n",
      "epoch 4 took 480.7s, optimizer loss=513.2, test mae=15.58\n",
      "366.6903413877719 10.84535096404574\n",
      "361.8729873670623 11.45242584902237\n",
      "356.0419302862141 10.773129099883212\n",
      "349.676222608167 11.08616204278671\n",
      "342.3308305127652 12.067127596219938\n",
      "335.87397790167705 11.829741116465648\n",
      "329.616502883362 12.491337933265187\n",
      "322.9535971452176 11.617929077411496\n",
      "317.6546560793893 12.568156231599222\n",
      "312.5096437593658 8.810643418054697\n",
      "307.8713682766972 12.176278295898799\n",
      "303.9159961592916 9.67021617797519\n",
      "300.918013272082 10.513108808020913\n",
      "298.07228568901365 9.641545846537289\n",
      "295.0629164612835 9.9229466944568\n",
      "291.76873266714256 9.93153498566764\n",
      "288.48383629557634 9.152770523225238\n",
      "284.9928977228126 11.129663441117637\n",
      "281.50840114301724 9.902554995931578\n",
      "278.074339676503 10.53457741479664\n",
      "274.09784842770983 9.536116475626212\n",
      "norms 3.347906605830994 43.52903147218795 28.931200087166477\n",
      "gradients 9.536116475626212 21.074228459640537 101.79241094653405\n",
      "epoch 5 took 481.4s, optimizer loss=366.7, test mae=13.24\n",
      "274.09784842770983 9.536116475626212\n",
      "269.5814551109643 10.458114882803727\n",
      "264.9732440498976 9.048779596326893\n",
      "260.4433100677041 10.443414963263438\n",
      "256.3906670945678 9.167181062635974\n",
      "252.58828744755937 10.737583475888112\n",
      "249.16692075557643 9.509336912483167\n",
      "245.9964722725183 10.420912705861017\n",
      "242.83151893284636 8.669202184520884\n",
      "239.89902063741872 9.740753640906348\n",
      "237.15739113976298 8.06906619866929\n",
      "234.5597739775547 9.809951403401017\n",
      "232.2902710312537 8.575878046829372\n",
      "230.29394284966654 9.516664320933215\n",
      "228.3256689766103 9.068058890308913\n",
      "226.03040196799842 9.407887454836507\n",
      "223.10401941777496 9.198059452085229\n",
      "219.6432905785257 9.223185697288823\n",
      "215.99822616095656 9.342268318837256\n",
      "212.26509372653027 8.58088541622934\n",
      "208.61965540161697 8.623750713778511\n",
      "norms 4.500725798503175 43.09445883249471 29.310751428480334\n",
      "gradients 8.623750713778511 16.25740261938506 65.40426342082601\n",
      "epoch 6 took 479.6s, optimizer loss=274.1, test mae=11.2\n",
      "208.61965540161697 8.623750713778511\n",
      "205.08858161621868 8.099722320485776\n",
      "201.70691621666356 7.846340259878471\n",
      "198.40922104318537 7.253131810627083\n",
      "195.52157606643226 7.394848795333984\n",
      "192.77001502748675 6.2759138196788955\n",
      "190.2137159463911 7.584978341502805\n",
      "187.98393873455873 5.586342191905367\n",
      "185.98331556369473 7.585406967679549\n",
      "184.34897452406057 6.329880721348222\n",
      "182.97946502938424 7.105360468653741\n",
      "181.65784243071988 6.525358710762171\n",
      "180.18712535574505 7.1826786391549495\n",
      "178.58563125547792 7.103403074630371\n",
      "177.043521248011 7.0438558390554995\n",
      "175.52518263394276 7.198513754087824\n",
      "173.91845625698863 7.737082521535086\n",
      "172.09850764210793 6.8538918070623716\n",
      "170.22464919733372 7.9824469671782055\n",
      "168.33726789820741 7.37243140140728\n",
      "166.27420007435035 8.2336148299252\n",
      "norms 5.428285259292798 42.80756047464242 29.53555228629283\n",
      "gradients 8.2336148299252 15.453962653898918 40.993647061917606\n",
      "epoch 7 took 483.4s, optimizer loss=208.6, test mae=9.714\n",
      "166.27420007435035 8.2336148299252\n",
      "164.0071850482133 7.51872445537597\n",
      "161.58570843905096 8.23307720590998\n",
      "159.04275555665308 7.412922932228897\n",
      "156.48105419491148 7.481292801374071\n",
      "153.90474228144865 7.461816891973317\n",
      "151.36414417732968 5.948622455108106\n",
      "149.13737395880557 7.0850755898995335\n",
      "147.06928290631498 5.3082554481489925\n",
      "145.2852481901592 6.519927311121091\n",
      "143.74389096541856 4.998231124344615\n",
      "142.35109738758317 6.050503819552836\n",
      "141.12692394958987 5.409007153745949\n",
      "139.9322395238176 5.268093222868863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.76778511633108 5.595861653218456\n",
      "137.6388391996674 5.02206230823314\n",
      "136.33821071156555 5.476029272635356\n",
      "134.76917856769575 4.586647809535454\n",
      "133.40091974222034 4.4939361283405965\n",
      "132.02773310603473 4.3257449559636605\n",
      "130.4055711551555 4.619252854356618\n",
      "norms 6.728034299412007 42.46703306413216 29.933464299507385\n",
      "gradients 4.619252854356618 14.515121478169917 56.00704817940101\n",
      "epoch 8 took 479.6s, optimizer loss=166.3, test mae=8.789\n",
      "130.4055711551555 4.619252854356618\n",
      "128.9589915941088 4.200702203277205\n",
      "127.44348638413015 4.5729894033862655\n",
      "125.98965168904043 4.572747851903665\n",
      "124.68004953845714 4.2754438453570085\n",
      "123.19971714702542 5.5226243925807355\n",
      "121.76820124103506 3.81510569995339\n",
      "120.27369148490982 6.438526403281603\n",
      "119.23448008190624 5.544783784233943\n",
      "118.36423531436157 5.892553870727492\n",
      "117.38135135906339 5.690828699644514\n",
      "116.17281054768043 5.7443231301009625\n",
      "114.77879188779063 5.80964883986482\n",
      "113.63075280928085 5.459216434967857\n",
      "112.52044526323297 6.009902796988683\n",
      "111.47498213488348 5.249065291019001\n",
      "110.6076248245266 5.525563023748741\n",
      "109.76139447103021 4.8816515608801865\n",
      "108.92168184855889 5.256157597109204\n",
      "108.09998314818719 4.5172904578281345\n",
      "107.27663619112008 5.108457132752748\n",
      "norms 7.5397264296481765 42.256295315419194 30.12309360248713\n",
      "gradients 5.108457132752748 11.672448002489988 29.90852413454305\n",
      "epoch 9 took 480.7s, optimizer loss=130.4, test mae=8.093\n",
      "107.27663619112008 5.108457132752748\n",
      "106.46448663683582 4.265203060087639\n",
      "105.64313405119633 4.89988279879836\n",
      "104.81008152015181 4.344882435326647\n",
      "103.89022917846918 4.7468282347185085\n",
      "102.73514738523414 4.4764473526143\n",
      "101.36355429498771 4.394564053307247\n",
      "100.04405608229115 4.7873426786854685\n",
      "98.50887751109359 4.409357025088496\n",
      "97.16787016164453 4.331251087656592\n",
      "95.89009164570612 4.489840452159017\n",
      "94.72015489935582 4.0847250654048635\n",
      "93.69447588498328 4.468589521236394\n",
      "92.80121345055683 3.7531450892381275\n",
      "92.06971652340101 4.3935952350244385\n",
      "91.47317559823878 3.715803931773898\n",
      "90.97340892025109 4.07488239049439\n",
      "90.51355483366633 3.7144956118030437\n",
      "90.03509505912812 3.708349541367569\n",
      "89.44482135792414 4.165947713235795\n",
      "88.93073975283343 3.585412692512779\n",
      "norms 8.267247127929735 42.06953300293496 30.309403178744798\n",
      "gradients 3.585412692512779 10.826959462477468 24.610761901769642\n",
      "epoch 10 took 480.9s, optimizer loss=107.3, test mae=7.526\n",
      "88.93073975283343 3.585412692512779\n",
      "88.43212348094424 4.366171471952031\n",
      "87.90217942593708 3.3272746239003252\n",
      "87.41901069888526 4.004517451986757\n",
      "86.99004006636473 3.7094222057162276\n",
      "86.51953212232847 3.7561882867879333\n",
      "85.93697193413429 4.043443677827574\n",
      "85.30251299757784 3.6819737339401177\n",
      "84.73555596011418 4.140672698606143\n",
      "84.20154364703349 3.8331810111750726\n",
      "83.68542718847694 4.2358579250742\n",
      "83.1978823100268 3.9844532180026335\n",
      "82.70555979841897 4.4479254414187706\n",
      "82.2179771530051 3.9860632858521168\n",
      "81.75370884269057 4.660836632689599\n",
      "81.34191177343246 4.189870704873174\n",
      "80.96914198372808 4.607930010297555\n",
      "80.62041482378581 4.344213132514557\n",
      "80.25760004293076 4.522414432050454\n",
      "79.82388666592662 4.162597325431528\n",
      "79.29665145941655 4.368016517628287\n",
      "norms 8.841935284541842 41.92435042425662 30.455567793577654\n",
      "gradients 4.368016517628287 9.884618085103165 15.969380140764315\n",
      "epoch 11 took 482.5s, optimizer loss=88.93, test mae=6.974\n",
      "79.29665145941655 4.368016517628287\n",
      "78.71484652500591 3.835367329834536\n",
      "78.22301120987878 3.715806118911318\n",
      "77.72982975431904 3.5340634372755386\n",
      "77.09293846146385 3.7178874432017\n",
      "76.59061610460569 3.25321136468803\n",
      "76.05231235144534 3.3375927695561756\n",
      "75.43787129064853 2.8018788217973696\n",
      "74.88076709746105 2.8241196304384952\n",
      "74.2662992664444 2.6233731470986963\n",
      "73.62713412084489 2.497362010314472\n",
      "72.92338785010902 2.820367988588345\n",
      "72.27787059155916 2.9615894941706062\n",
      "71.59127584122221 2.5501345366032218\n",
      "70.81881179728946 3.98750925057898\n",
      "70.18050111656939 3.0553009344188182\n",
      "69.69919332666097 3.152333697696469\n",
      "69.16842070183574 2.8359424238601134\n",
      "68.58622055907422 2.7614379794386235\n",
      "67.80890792137107 3.1405061170409847\n",
      "67.09426683889743 2.868507260855549\n",
      "norms 9.736709910177005 41.72348944332052 30.686129439410912\n",
      "gradients 2.868507260855549 8.91026174167116 96.8239704223467\n",
      "epoch 12 took 482.2s, optimizer loss=79.3, test mae=6.529\n",
      "67.09426683889743 2.868507260855549\n",
      "66.33146036428928 2.920158526046522\n",
      "65.52371748500771 2.6402568607568346\n",
      "64.71966393359635 2.784855055234858\n",
      "64.06574915864731 2.6237865633436823\n",
      "63.40579318008326 2.5497599580953865\n",
      "62.79051546848387 2.1670139585211396\n",
      "62.24009900906163 2.9676834148940223\n",
      "61.79653244721247 2.2562081793511055\n",
      "61.464228779005225 2.657380545937366\n",
      "61.19414567218191 2.5691672786840414\n",
      "60.81768838018545 2.5072506661858953\n",
      "60.3415608892823 2.727819401059968\n",
      "59.894252268305614 2.7007280539220218\n",
      "59.358428145315806 3.212262517255177\n",
      "58.900814599211955 3.2222716801709668\n",
      "58.422617782828446 3.719161773724616\n",
      "57.92082237480153 3.5781229265643204\n",
      "57.4602000980293 4.045542535521965\n",
      "57.00672506613695 3.8460794960823943\n",
      "56.56708090951475 4.133016699832891\n",
      "norms 10.779880419110969 41.509588648943435 30.976026170578805\n",
      "gradients 4.133016699832891 7.6495274495355865 20.318757597672317\n",
      "epoch 13 took 480.9s, optimizer loss=67.09, test mae=5.977\n",
      "56.56708090951475 4.133016699832891\n",
      "56.167359511968684 3.8602751097739163\n",
      "55.808180512551225 3.8249925012384605\n",
      "55.39112624721802 3.884630763952954\n",
      "55.032545365033016 3.4347946747593485\n",
      "54.70340994794655 3.6102471663283766\n",
      "54.40448216928108 2.993223288029849\n",
      "54.136230438467365 3.185797654060271\n",
      "53.918430527071386 2.7859877917002867\n",
      "53.726859263367686 2.7793054410369935\n",
      "53.516479181723916 2.5797336770161547\n",
      "53.25459531332378 2.3476508139451484\n",
      "52.97448309687186 2.399653286850051\n",
      "52.70653588898433 2.166496395773623\n",
      "52.460287611897876 2.2164465844484593\n",
      "52.166530901504146 2.223429158140266\n",
      "51.825306146392705 2.2308629992544304\n",
      "51.44962527788671 2.618860901512033\n",
      "51.122046173896706 2.538553647269799\n",
      "50.77112466391057 2.6741648460062835\n",
      "50.38822618384458 2.5510198715044132\n",
      "norms 11.7721945179513 41.32734811708269 31.281754741806946\n",
      "gradients 2.5510198715044132 7.28122017232976 33.579571536665625\n",
      "epoch 14 took 481.1s, optimizer loss=56.57, test mae=5.795\n",
      "50.38822618384458 2.5510198715044132\n",
      "49.99494200174324 2.6756201663183345\n",
      "49.62123058479841 2.618557895553737\n",
      "49.2371154984148 2.5301830612744856\n",
      "48.89401363450219 2.809536322419575\n",
      "48.651673119430825 2.4095792150576143\n",
      "48.42225381922726 2.880371629292786\n",
      "48.21255938593213 2.411444547112158\n",
      "48.0341660467504 2.676151583851278\n",
      "47.881259813384155 2.5844371376126705\n",
      "47.6837947219069 2.2392090667417377\n",
      "47.46975070291873 2.4588347602557414\n",
      "47.24368178426223 2.2634280324315506\n",
      "46.98846102385925 2.504314696643506\n",
      "46.69584647002842 2.192954533672409\n",
      "46.43020310787909 2.230226465405644\n",
      "46.18094880496091 2.3139229280528157\n",
      "45.947745056785784 2.2302854001211023\n",
      "45.735097694578556 2.253698269940139\n",
      "45.48685931594913 2.173674479649792\n",
      "45.256359915556224 2.132874222454236\n",
      "norms 12.639550230092635 41.1692184017145 31.544948926099096\n",
      "gradients 2.132874222454236 6.646585360894113 11.007051018171106\n",
      "epoch 15 took 487.1s, optimizer loss=50.39, test mae=5.549\n",
      "45.256359915556224 2.132874222454236\n",
      "45.04269922070991 2.164577873438786\n",
      "44.805158569614086 2.142537711684332\n",
      "44.6169599517667 2.0953017088659154\n",
      "44.40342636613381 2.3823832825365034\n",
      "44.17139505753169 2.2961788808759502\n",
      "43.934323864335944 2.4021852240724844\n",
      "43.637871533594634 2.416573970461227\n",
      "43.35033605064081 2.440496932459963\n",
      "43.038167812904106 2.4816564633286307\n",
      "42.674824962850046 2.508777255234683\n",
      "42.314943201079956 2.4530726115167236\n",
      "41.95728169489552 2.5377684325365157\n",
      "41.61274167901648 2.4335538760117306\n",
      "41.26297607077282 2.45207198039155\n",
      "40.878435693034035 2.542902415939086\n",
      "40.57252464118096 2.2013624740073987\n",
      "40.281522828900606 2.605859124973018\n",
      "40.00201050611677 1.7847096546714925\n",
      "39.75517666523594 2.3533050488363876\n",
      "39.56966276286198 2.002056190230179\n",
      "norms 13.71881131157382 40.964939110017056 31.85969046664431\n",
      "gradients 2.002056190230179 6.2794208808583045 7.612772615987214\n",
      "epoch 16 took 482.4s, optimizer loss=45.26, test mae=5.225\n",
      "39.56966276286198 2.002056190230179\n",
      "39.40493697412631 2.097951375412165\n",
      "39.20949948700696 2.027546808016272\n",
      "38.92372999844422 2.7822569323901067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.62862330697124 2.496697878934802\n",
      "38.326764206507065 2.458327074332493\n",
      "37.94094506027382 2.208036665208764\n",
      "37.56054685053332 2.2860706048790473\n",
      "37.19953045205286 2.171609314644394\n",
      "36.84243396925882 2.321428553753455\n",
      "36.515530260236815 2.273795525441806\n",
      "36.20148972123929 2.409916579260264\n",
      "35.910137921481194 2.399907601269486\n",
      "35.621183755104056 2.6952164373186487\n",
      "35.417500417376964 2.6161988242727015\n",
      "35.25022298330503 2.5897755119378805\n",
      "35.03755182773075 2.5951499168047243\n",
      "34.854150862138596 2.4000734423839636\n",
      "34.662161426237866 2.7204341974339505\n",
      "34.50390480875271 2.4252939256504367\n",
      "34.345414362298676 2.4757412897706543\n",
      "norms 15.12056566319255 40.73819540245832 32.3395651469473\n",
      "gradients 2.4757412897706543 6.475879545984747 23.200045343821035\n",
      "epoch 17 took 481.1s, optimizer loss=39.57, test mae=4.875\n",
      "34.345414362298676 2.4757412897706543\n",
      "34.161049509395426 2.0179713845340195\n",
      "33.96383299513482 2.2355503806450576\n",
      "33.77715984641125 1.8308983552791687\n",
      "33.60545511394772 1.9479028619000363\n",
      "33.44186206716952 1.7176031369571942\n",
      "33.27916010604208 1.770354901240421\n",
      "33.108348761246546 1.648824415762913\n",
      "32.93704414244723 1.541789447355909\n",
      "32.78872901533089 1.6052211159889649\n",
      "32.61753966527394 1.545371630720793\n",
      "32.41074115513709 1.596629900514461\n",
      "32.25243121978782 1.6629482878443282\n",
      "32.10188572736346 1.5454842545721652\n",
      "31.927058043074588 1.8117521095084217\n",
      "31.745843071119587 1.6733163238675781\n",
      "31.54820964330303 1.9320667443573443\n",
      "31.32913324847215 1.9946681395879677\n",
      "31.120828147898155 1.995271092380083\n",
      "30.894119796995284 2.3375310733413706\n",
      "30.68580434682306 2.260283998259289\n",
      "norms 16.083435534124114 40.583865196695776 32.67230212712927\n",
      "gradients 2.260283998259289 6.044819926827221 33.601593236064815\n",
      "epoch 18 took 480.9s, optimizer loss=34.35, test mae=4.716\n",
      "30.68580434682306 2.260283998259289\n",
      "30.460455929001036 2.7671568436951564\n",
      "30.283880036593175 2.7069435415111283\n",
      "30.090216552700998 2.5642900808561633\n",
      "29.861279321769615 2.4943412320646012\n",
      "29.658676861392266 2.475610396335759\n",
      "29.432167222586113 2.1517207753187915\n",
      "29.228140363747425 2.271801653321795\n",
      "29.016607512526036 1.8297799312325609\n",
      "28.80403334818946 1.967535919897289\n",
      "28.602423848634555 1.6531988239053825\n",
      "28.414876845517313 1.7986406937079706\n",
      "28.238011785356775 1.6121419437804367\n",
      "28.062917312884792 1.8380695293643183\n",
      "27.889789864353112 1.6672653031899685\n",
      "27.731602923141576 1.832517917004993\n",
      "27.57748435650537 1.8521947999212443\n",
      "27.394963064780733 1.8928085750443904\n",
      "27.222262506386908 1.9927004472442862\n",
      "27.073861085742855 1.870679826223245\n",
      "26.88491001575486 2.1258904633549243\n",
      "norms 17.053435438978827 40.42083584438942 33.001597133198985\n",
      "gradients 2.1258904633549243 6.044422222348679 17.84648028929011\n",
      "epoch 19 took 482.8s, optimizer loss=30.69, test mae=4.327\n",
      "26.88491001575486 2.1258904633549243\n",
      "26.686822783689195 1.8860551540043882\n",
      "26.490603434197787 1.9519908978992253\n",
      "26.29275184011741 1.7277336796800755\n",
      "26.106587032956757 1.7274527177026024\n",
      "25.922173975135657 1.5976504266385572\n",
      "25.74211803675436 1.5699134985221532\n",
      "25.56249277070339 1.6265234759687202\n",
      "25.397560517209648 1.5638152400296308\n",
      "25.246972314311883 1.643576271934463\n",
      "25.099875873822345 1.6320751107407727\n",
      "24.947510570966084 1.7544135676244754\n",
      "24.79029737471939 1.6897807371209341\n",
      "24.653041597333996 1.7895913082126842\n",
      "24.514997764472618 1.7129276786791807\n",
      "24.36194882952057 1.8158516383809886\n",
      "24.19551662164744 1.8341476928529188\n",
      "24.06759511743459 1.6972743423743089\n",
      "23.942047012157246 1.7961223713862629\n",
      "23.795477915182005 1.4830124257731172\n",
      "23.674540470267345 1.5492051360823618\n",
      "norms 18.03847769462277 40.2663127882088 33.358657602351144\n",
      "gradients 1.5492051360823618 5.399471542314204 7.481828662909448\n",
      "epoch 20 took 485.4s, optimizer loss=26.88, test mae=4.131\n",
      "23.674540470267345 1.5492051360823618\n",
      "23.555785607467794 1.248835728735185\n",
      "23.434108631079273 1.4210175351782515\n",
      "23.32255272221957 1.1250495801438443\n",
      "23.220460524932218 1.2880704503851157\n",
      "23.126965309010334 1.084796795339605\n",
      "23.039574534271257 1.2214814008935897\n",
      "22.955550277385015 1.1235335017478578\n",
      "22.868137577669074 1.2429825401674297\n",
      "22.772369844602192 1.2118107497625492\n",
      "22.659372289441116 1.2730979869678578\n",
      "22.522178469675296 1.420633664971609\n",
      "22.425469588351156 1.3669435446419589\n",
      "22.32126035758419 1.4666573255862074\n",
      "22.19099782822063 1.4891710491013475\n",
      "22.06530018698746 1.549339783674116\n",
      "21.926446307398965 1.617675095235901\n",
      "21.775962189141662 1.6143309659804812\n",
      "21.63816810770147 1.56514847639924\n",
      "21.487414195464513 1.6923966405731956\n",
      "21.352433950406724 1.6092499937264766\n",
      "norms 18.66042491066433 40.15125071863969 33.5701392814057\n",
      "gradients 1.6092499937264766 5.556300103409881 22.331700684663403\n",
      "epoch 21 took 481.9s, optimizer loss=23.67, test mae=3.964\n",
      "21.352433950406724 1.6092499937264766\n",
      "21.21088744370214 1.701434274184575\n",
      "21.064492702285296 1.624335628161401\n",
      "20.9179766735844 1.6159854222662187\n",
      "20.759360329850555 1.7378364480335295\n",
      "20.652138685793677 1.6182908708257269\n",
      "20.556567571095485 1.5747807267452942\n",
      "20.396655134645776 1.6898657564616544\n",
      "20.263802713723997 1.6497558257178138\n",
      "20.075204828035726 1.6342884898600485\n",
      "19.870625347074792 1.6524786472606516\n",
      "19.69798032835427 1.5746239720309885\n",
      "19.525289152411723 1.8412721540586383\n",
      "19.40585158455934 1.6815199877601137\n",
      "19.30983325342869 1.7145103322149144\n",
      "19.221980188791683 1.5991140901740506\n",
      "19.145741234859347 1.6138375723640952\n",
      "19.077472452512353 1.4714444643320659\n",
      "19.01441570915759 1.506446450177786\n",
      "18.95280181516397 1.322500924333227\n",
      "18.892821482463102 1.3423273020855118\n",
      "norms 19.404840124631523 40.02257116340847 33.88444946126998\n",
      "gradients 1.3423273020855118 4.77245921182216 5.66920598516168\n",
      "epoch 22 took 484.7s, optimizer loss=21.35, test mae=3.743\n",
      "18.892821482463102 1.3423273020855118\n",
      "18.829576062404843 1.1944575848747687\n",
      "18.761081613047033 1.1685377954521667\n",
      "18.67142203768894 1.1903199790405\n",
      "18.574434322714747 1.1104946056440907\n",
      "18.467734576767818 1.475075122223473\n",
      "18.36825767267025 1.3745330112356398\n",
      "18.26529653792888 1.2930109958415128\n",
      "18.141159868442262 1.2745793026553285\n",
      "18.041022454384503 1.1367868669405539\n",
      "17.96455899446189 1.0674955408608144\n",
      "17.873077189558472 1.0576771535609701\n",
      "17.765726503913456 0.9812018878539978\n",
      "17.67680115091508 1.0311444647819106\n",
      "17.58929693081127 1.0481116158990738\n",
      "17.493822953292987 1.2498897804644433\n",
      "17.414698524911845 1.2989282945632707\n",
      "17.32301162691234 1.364647756712136\n",
      "17.209676228026552 1.4350200218689544\n",
      "17.10562231136632 1.4278495650823568\n",
      "17.005082121049416 1.4392177562345156\n",
      "norms 20.12239615247866 39.891017297284726 34.192374860909354\n",
      "gradients 1.4392177562345156 3.894389960740127 7.774260850329534\n",
      "epoch 23 took 485.7s, optimizer loss=18.89, test mae=3.607\n",
      "17.005082121049416 1.4392177562345156\n",
      "16.91412644220338 1.4309026153115905\n",
      "16.83608106308393 1.3530613911916491\n",
      "16.76561798442811 1.298082520552745\n",
      "16.69598022341172 1.2513377213964652\n",
      "16.615584198624315 1.2733720544686762\n",
      "16.561039737585862 1.1558518333853147\n",
      "16.513088969307702 1.0995738362229313\n",
      "16.45818554769613 1.0347553678972052\n",
      "16.394602015259636 0.9832939090871552\n",
      "16.322501272307075 1.1528934270221514\n",
      "16.252599423757133 1.0622478425687776\n",
      "16.188460290624256 1.0850084625758711\n",
      "16.10286337213766 1.1813054503059943\n",
      "16.02039321880853 1.1912354585040748\n",
      "15.919859430302981 1.4187785917980207\n",
      "15.813851027929918 1.3483731461626967\n",
      "15.706022662506896 1.515353050513026\n",
      "15.60073963724534 1.4273787170645817\n",
      "15.511263245591675 1.2918130555664398\n",
      "15.414998006798728 1.326583784388605\n",
      "norms 20.463620017680697 39.77945056977807 34.318988858270394\n",
      "gradients 1.326583784388605 3.7688366422940334 13.684785074991824\n",
      "epoch 24 took 484.9s, optimizer loss=17.01, test mae=3.418\n",
      "15.414998006798728 1.326583784388605\n",
      "15.31754602453804 1.1017086765291706\n",
      "15.2250563646067 1.2229029565253007\n",
      "15.157473381246037 1.1825846267964533\n",
      "15.09295631660475 1.2096774998639832\n",
      "15.030091096455978 1.199110374426219\n",
      "14.962504866346041 1.300284005691535\n",
      "14.888217253002972 1.2789253570089054\n",
      "14.80781803098983 1.5320751139250286\n",
      "14.728301293785277 1.4852052205428659\n",
      "14.654556884369168 1.4674912486748957\n",
      "14.564990336796024 1.5304773605425401\n",
      "14.472589414242819 1.343874306216563\n",
      "14.389531004390816 1.4573195828325594\n",
      "14.324840832678596 1.334241731991201\n",
      "14.26664325248825 1.2278681528605402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.20624078504301 1.1445129185769451\n",
      "14.14554175448504 1.0302194312966015\n",
      "14.088455646708722 0.9420684193078954\n",
      "14.031481142355002 0.9378242307669904\n",
      "13.971489566751522 0.8193406697060013\n",
      "norms 21.310582314510548 39.617541167270176 34.708549918380136\n",
      "gradients 0.8193406697060013 3.5266582229461014 5.846697104653848\n",
      "epoch 25 took 481.2s, optimizer loss=15.41, test mae=3.356\n",
      "13.971489566751522 0.8193406697060013\n",
      "13.9262416052955 0.7631914298791266\n",
      "13.872286852117222 0.7997711095574483\n",
      "13.813776572810001 0.7644285522408585\n",
      "13.75166100590412 0.7701907006968675\n",
      "13.673908146794348 0.8142177987256759\n",
      "13.606363458825427 0.7608761031591887\n",
      "13.5379071538431 0.8498537018941197\n",
      "13.475514269664647 0.7979689139648994\n",
      "13.413207785352578 0.8569854902479291\n",
      "13.352619787623826 0.7984015513002642\n",
      "13.293901301449958 0.8186091076442245\n",
      "13.224781343662324 0.9300519749811981\n",
      "13.16841919116394 0.8815348760884977\n",
      "13.106724355316956 0.9768286296292679\n",
      "13.037375950097104 0.9259993366826466\n",
      "12.960665137467508 0.9791252227037015\n",
      "12.866010433177362 1.0276717322398177\n",
      "12.792038776091793 0.9390016777452533\n",
      "12.724958693907345 0.9507714239158419\n",
      "12.655873445846785 0.8716758936766241\n",
      "norms 21.475891108056537 39.500968186793415 34.78042628892306\n",
      "gradients 0.8716758936766241 3.4822776915552374 2.071086260969017\n",
      "epoch 26 took 484.0s, optimizer loss=13.97, test mae=3.15\n",
      "12.655873445846785 0.8716758936766241\n",
      "12.597140133171573 0.8610817602796061\n",
      "12.539556544125375 0.784424722646631\n",
      "12.484101365569407 0.7670760757774451\n",
      "12.424235726771654 0.8267618428114685\n",
      "12.374036921933914 0.8033115875132318\n",
      "12.321320628562855 0.9064772918249788\n",
      "12.272009202868343 0.8455668809483757\n",
      "12.222152719946006 0.8329363423833398\n",
      "12.161771386438685 0.9870999783412011\n",
      "12.108003204572203 0.902784607094988\n",
      "12.065346873203868 0.9296003129628481\n",
      "12.020400409938299 0.9432224454435155\n",
      "11.976948905111328 0.9472095673058288\n",
      "11.93325517459312 0.9435951143019231\n",
      "11.889411277972375 0.9404411853656636\n",
      "11.847500691617356 0.927299481234324\n",
      "11.810405481559185 0.9184878912522022\n",
      "11.773512270567556 0.9136456813382011\n",
      "11.739828737592497 0.9009984932720778\n",
      "11.70832459294503 0.8501003707883122\n",
      "norms 21.775134994890372 39.39378185080033 34.91650772402235\n",
      "gradients 0.8501003707883122 3.1116210931924653 3.517875326381303\n",
      "epoch 27 took 483.8s, optimizer loss=12.66, test mae=3.045\n",
      "11.70832459294503 0.8501003707883122\n",
      "11.675221018907022 0.8545843448583791\n",
      "11.64544320453936 0.7990087696422197\n",
      "11.617768827266445 0.8094577926925067\n",
      "11.591167065289403 0.7606313860918006\n",
      "11.564272172699875 0.7977102945779118\n",
      "11.536183585224727 0.7492881956303891\n",
      "11.506070415525793 0.8089156350593634\n",
      "11.471181750998614 0.837659258524525\n",
      "11.44008320274914 0.8368953675866092\n",
      "11.40752251288914 0.8714395699412885\n",
      "11.369093066007366 0.9355631069120338\n",
      "11.331253534190335 0.9308687139399082\n",
      "11.283120834870065 1.3136900301803789\n",
      "11.24171819685564 1.3079177089195333\n",
      "11.177741758790587 1.228516589115838\n",
      "11.125248397165306 1.2377965837696199\n",
      "11.055812649654 1.2086067185294858\n",
      "10.98136239835553 1.1338138712976338\n",
      "10.911081555651268 1.0700407246658428\n",
      "10.848241716772455 1.0137302218666293\n",
      "norms 21.9738967866554 39.26929315645905 35.00331831220216\n",
      "gradients 1.0137302218666293 3.0287871993526014 4.936596866890742\n",
      "epoch 28 took 483.9s, optimizer loss=11.71, test mae=2.962\n",
      "10.848241716772455 1.0137302218666293\n",
      "10.794952488621284 0.9130728625792414\n",
      "10.750192177707675 0.883863847988337\n",
      "10.708594507300909 0.825808948217888\n",
      "10.665644382540068 0.8388362477623825\n",
      "10.630387944652671 0.7947428456251051\n",
      "10.600341487863323 0.8298280959566191\n",
      "10.571603489545199 0.8274447334537073\n",
      "10.540941364894032 0.8921476097436805\n",
      "10.512102005320246 0.9156634566748911\n",
      "10.479788846295174 0.9255753951381753\n",
      "10.442245902430841 0.9635159958092868\n",
      "10.397912166833715 0.9474372385015094\n",
      "10.342159917901073 0.9846292978239528\n",
      "10.279207262165453 0.9465671896763492\n",
      "10.223640955533035 0.9248806968855543\n",
      "10.158358993755316 0.9710175172973498\n",
      "10.102822704923788 0.9796077816450933\n",
      "10.040941948379388 1.1177608192690567\n",
      "9.987737434106677 0.9996610823176192\n",
      "9.93321039034118 0.9124193910865003\n",
      "norms 22.061606975211834 39.17598789594742 35.03605824511564\n",
      "gradients 0.9124193910865003 2.9263421034275376 17.638746999741738\n",
      "epoch 29 took 486.2s, optimizer loss=10.85, test mae=2.797\n",
      "9.93321039034118 0.9124193910865003\n",
      "9.875008913125134 0.834703301873585\n",
      "9.820417396127803 0.736569928495301\n",
      "9.776561974991083 0.7436934372698579\n",
      "9.734636527077965 0.7354825373697532\n",
      "9.69084914344344 0.6789985125100225\n",
      "9.659159050255296 0.7079298642456584\n",
      "9.628127565322199 0.7234802608971701\n",
      "9.600329606024172 0.727102803059695\n",
      "9.572313905643034 0.7115207016834167\n",
      "9.544638828077927 0.704563901658269\n",
      "9.517790575916482 0.6849028580251202\n",
      "9.489147231483027 0.6737006764050262\n",
      "9.459672847482 0.6283330313343299\n",
      "9.431837943484036 0.609952353299606\n",
      "9.404088149076339 0.7029339319889224\n",
      "9.382457518912414 0.620937236926681\n",
      "9.362093268056151 0.6144673443860441\n",
      "9.33615562086923 0.5461801102233281\n",
      "9.311891513314057 0.5467171015181517\n",
      "9.278518472526443 0.46958381492479945\n",
      "norms 22.425975428499626 39.04764546335087 35.22634093422412\n",
      "gradients 0.46958381492479945 2.9143260980549583 3.9625819286696724\n",
      "epoch 30 took 481.3s, optimizer loss=9.933, test mae=2.696\n",
      "9.278518472526443 0.46958381492479945\n",
      "9.239640313197677 0.48813863793123674\n",
      "9.196770577753108 0.7047962135882874\n",
      "9.16060090862855 0.6380543813592751\n",
      "9.123351454809917 0.6856431357075465\n",
      "9.07912800585242 0.6464209859140435\n",
      "9.041786223431902 0.5908406802870102\n",
      "9.006570790007343 0.5846629648014376\n",
      "8.971070118250038 0.5613296945639167\n",
      "8.93557191845299 0.5403539937581353\n",
      "8.899485012216777 0.5765490292333808\n",
      "8.864380679502215 0.5535266183894351\n",
      "8.829636548917344 0.6224686192634253\n",
      "8.793463379170865 0.5936505668944132\n",
      "8.761832425012933 0.625831650841706\n",
      "8.728632477958048 0.6443250238065634\n",
      "8.699835156487346 0.6450572205762615\n",
      "8.667105651007175 0.7301635841809662\n",
      "8.626311131386972 0.6790882032316401\n",
      "8.590517926692575 0.7346156114556905\n",
      "8.553742006430003 0.7035649955248602\n",
      "norms 22.41673871963386 38.98212975108098 35.21476632247332\n",
      "gradients 0.7035649955248602 2.4513599535622883 4.553479161933456\n",
      "epoch 31 took 484.8s, optimizer loss=9.279, test mae=2.564\n",
      "8.553742006430003 0.7035649955248602\n",
      "8.515865339333114 0.7291647555938762\n",
      "8.482410946887907 0.7070308455002731\n",
      "8.448642065951638 0.7695622603166172\n",
      "8.41515539855811 0.6992241662736878\n",
      "8.384946833821948 0.7207562520366333\n",
      "8.352538565033605 0.7057375143255303\n",
      "8.320723043844312 0.6807237262546814\n",
      "8.28904135148461 0.6460363828319717\n",
      "8.259152287593862 0.6379784198735616\n",
      "8.228091883548158 0.6032422745188945\n",
      "8.196473868865993 0.5657004017248977\n",
      "8.165284279860964 0.5910333335588283\n",
      "8.130412219736412 0.5187555179252281\n",
      "8.096603460152163 0.5580667039934942\n",
      "8.065054905568259 0.5091583982956175\n",
      "8.03481570091319 0.5633061895928873\n",
      "8.007358268857002 0.510729518810362\n",
      "7.982952087822011 0.5610713757534286\n",
      "7.961452282968829 0.5195497492876747\n",
      "7.941389164265417 0.5555071974216073\n",
      "norms 22.715494271525163 38.837481825477695 35.37757303774914\n",
      "gradients 0.5555071974216073 2.5361761543755774 3.7058608358706446\n",
      "epoch 32 took 480.5s, optimizer loss=8.554, test mae=2.444\n",
      "7.941389164265417 0.5555071974216073\n",
      "7.920376071175537 0.500337712588312\n",
      "7.89820516002742 0.5007353274081369\n",
      "7.880369298337862 0.5060994301944091\n",
      "7.86322539783846 0.47374368247633847\n",
      "7.8468937636981835 0.47940999534999956\n",
      "7.830480169540491 0.45183395974909624\n",
      "7.811130163318063 0.4889534799712436\n",
      "7.792287352413526 0.4944199475537166\n",
      "7.7724906681995725 0.49043224950826114\n",
      "7.7576216766816195 0.5095634609771889\n",
      "7.742393315243786 0.5169774837537103\n",
      "7.7243398479978 0.5476123363237823\n",
      "7.703159340939331 0.5518543204409546\n",
      "7.677964751497546 0.8680641356144785\n",
      "7.659883204995639 0.8641992811993384\n",
      "7.638985299348849 0.8383936004467785\n",
      "7.617090869238665 0.8158897348783852\n",
      "7.59552856606281 0.7997600003430599\n",
      "7.570983533753387 0.7912879305321442\n",
      "7.546414207182223 0.7661210131015426\n",
      "norms 22.676496371607346 38.81434197846556 35.35079356117917\n",
      "gradients 0.7661210131015426 2.07403741447659 3.169341764654753\n",
      "epoch 33 took 481.7s, optimizer loss=7.941, test mae=2.405\n",
      "7.546414207182223 0.7661210131015426\n",
      "7.52021844875975 0.8167743821814872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.494756989132048 0.7662218754396204\n",
      "7.46774700693194 0.7700857352561122\n",
      "7.434136423561651 0.7001458183592669\n",
      "7.403895077142678 0.6802655016477138\n",
      "7.376515473347448 0.6287655618642418\n",
      "7.34991224430034 0.6706330387770862\n",
      "7.325989561963037 0.5641084239495293\n",
      "7.305940966152407 0.5728453386904458\n",
      "7.285540992444962 0.5072169730549262\n",
      "7.267556254252095 0.47488327250622214\n",
      "7.247223980954781 0.5046479179873644\n",
      "7.220966831076411 0.4712647274613927\n",
      "7.196821866634213 0.48098359748801117\n",
      "7.170066003919632 0.5177973357117067\n",
      "7.142305407399527 0.5230807610007625\n",
      "7.112465969227646 0.5899650494241022\n",
      "7.0840767014240535 0.5864679736864029\n",
      "7.058298181553267 0.5570926024414996\n",
      "7.0288922596356 0.8828723831854272\n",
      "norms 22.928692768565725 38.67126194343681 35.48997780770586\n",
      "gradients 0.8828723831854272 2.1803234845908537 23.09061742681919\n",
      "epoch 34 took 483.4s, optimizer loss=7.546, test mae=2.295\n",
      "7.0288922596356 0.8828723831854272\n",
      "7.008802072490316 0.8367475135948107\n",
      "6.985998948533246 0.7126896977717484\n",
      "6.962499003000519 0.755209484438508\n",
      "6.940217024125377 0.5699611493266089\n",
      "6.92136643060751 0.5805605033109928\n",
      "6.903017024614942 0.521109367129199\n",
      "6.884820840742191 0.4915907992912537\n",
      "6.866043988013562 0.6251061683624269\n",
      "6.8478601172824884 0.515215395522253\n",
      "6.834415857270396 0.5045045444616731\n",
      "6.815829309668793 0.5510041564089221\n",
      "6.7944753572417875 0.5135949362271122\n",
      "6.77444281158446 0.528684200421781\n",
      "6.749115458048495 0.5610535695902443\n",
      "6.726019794091562 0.5319086694554501\n",
      "6.70512919984585 0.5333877797816048\n",
      "6.684066931956161 0.5198692579182406\n",
      "6.666151368572331 0.4782280183480368\n",
      "6.651969310073383 0.4513435534847803\n",
      "6.6372357089453375 0.4148709084030263\n",
      "norms 23.123700489091508 38.605858693335016 35.59476882040553\n",
      "gradients 0.4148709084030263 1.9857898929065927 2.441836416389406\n",
      "epoch 35 took 483.1s, optimizer loss=7.029, test mae=2.254\n",
      "6.6372357089453375 0.4148709084030263\n",
      "6.623434535192842 0.3899493761441758\n",
      "6.608987548270691 0.3606418571340743\n",
      "6.591447298552777 0.38628899981503917\n",
      "6.575700214687288 0.3558164619167879\n",
      "6.562286354265594 0.3874826917701316\n",
      "6.548635477564153 0.3811188288635921\n",
      "6.533317864176778 0.49020198376600405\n",
      "6.517217434086861 0.5063662355345481\n",
      "6.502679216270313 0.5044237377605165\n",
      "6.488967863227906 0.5568490829885505\n",
      "6.473604618634133 0.5579396448498382\n",
      "6.4575997167755705 0.5746679836429884\n",
      "6.440489871598072 0.6095031221336896\n",
      "6.419654271421848 0.5674398798769728\n",
      "6.403494953709366 0.5620245680981674\n",
      "6.385358393509941 0.5597863655811078\n",
      "6.366510303144874 0.5469544955273962\n",
      "6.348221547889788 0.5368333831605568\n",
      "6.331819593219221 0.5313057365315442\n",
      "6.3133843465959965 0.4829511738004779\n",
      "norms 23.198298934756483 38.52522981559226 35.63761853262416\n",
      "gradients 0.4829511738004779 1.7591741898251985 8.486940649646245\n",
      "epoch 36 took 482.3s, optimizer loss=6.637, test mae=2.243\n",
      "6.3133843465959965 0.4829511738004779\n",
      "6.291825730725622 0.45452450061407146\n",
      "6.2726342873442045 0.45419329718428925\n",
      "6.255632772794889 0.42814106562805077\n",
      "6.239922113410513 0.43412080800486924\n",
      "6.224795213434976 0.42437712961309315\n",
      "6.2086321682352095 0.46276888198744565\n",
      "6.193730369779541 0.45303300272173963\n",
      "6.179643756807697 0.8692572443518096\n",
      "6.169272617902252 0.7841372377229191\n",
      "6.157675358579019 0.7317362122566453\n",
      "6.143811608800899 0.6252886346026384\n",
      "6.12922784064633 0.592866638739835\n",
      "6.115955522044982 0.5348358384862835\n",
      "6.102673331500704 0.5135883509827558\n",
      "6.088688231983019 0.5111071400452241\n",
      "6.0751276961890195 0.5081058784088152\n",
      "6.0627647389111 0.5100902372964153\n",
      "6.051204019336317 0.5262699442936382\n",
      "6.03993924614933 0.5284127135986748\n",
      "6.028530332603927 0.5406188002869614\n",
      "norms 23.28263184831748 38.476186271520774 35.68052866996147\n",
      "gradients 0.5406188002869614 1.8603500812095202 3.3308107810917233\n",
      "epoch 37 took 487.0s, optimizer loss=6.313, test mae=2.195\n",
      "6.028530332603927 0.5406188002869614\n",
      "6.016585944120999 0.5472722999942331\n",
      "6.00437625679755 0.5420105058150526\n",
      "5.99259452528208 0.5470216726537326\n",
      "5.980865982980217 0.5275898307828598\n",
      "5.970122368187979 0.5150678547859645\n",
      "5.9600154563163565 0.49652482323133185\n",
      "5.949672719826375 0.4714823790556375\n",
      "5.939372529381051 0.45373726119644686\n",
      "5.928694067093381 0.4307885035394161\n",
      "5.918304273414262 0.41101281322596095\n",
      "5.907281887108095 0.4024562770674803\n",
      "5.895105974250802 0.3832719377028828\n",
      "5.883129596746324 0.36565793476434755\n",
      "5.870981072360413 0.37208577489615013\n",
      "5.858526213462641 0.3454723017316959\n",
      "5.846390578250998 0.3653727051549457\n",
      "5.833634069602569 0.3610979273891689\n",
      "5.822588587575605 0.3521198625433189\n",
      "5.811643588939006 0.35824897547781376\n",
      "5.799604025804696 0.35469274090571495\n",
      "norms 23.413821173701184 38.407962623088345 35.75339428408219\n",
      "gradients 0.35469274090571495 1.8082639871604174 2.8425197386597643\n",
      "epoch 38 took 481.4s, optimizer loss=6.029, test mae=2.148\n",
      "5.799604025804696 0.35469274090571495\n",
      "5.787325184220822 0.3499542297555633\n",
      "5.775115400887992 0.3458310839516464\n",
      "5.764235047927115 0.34173630270550015\n",
      "5.75445835244002 0.3392293213683158\n",
      "5.7441950809494156 0.34610836724277144\n",
      "5.731872994669982 0.35810311531144495\n",
      "5.7172637192238245 0.3866268879571165\n",
      "5.70600118755646 0.37633790766682573\n",
      "5.694319689213601 0.41251500403555413\n",
      "5.683142930751821 0.41423964546190895\n",
      "5.671340589175664 0.42852217769936185\n",
      "5.658185887434268 0.43794743569537975\n",
      "5.644865888376681 0.4362140891627723\n",
      "5.631557784053605 0.4826714723171406\n",
      "5.617803253917652 0.4424637168810039\n",
      "5.605569297930547 0.48521627916399773\n",
      "5.594192027371292 0.4528071829904133\n",
      "5.582889004075301 0.4677176596339316\n",
      "5.573374424668089 0.47713728189836757\n",
      "5.5625192311337015 0.43882731918943485\n",
      "norms 23.56027369812942 38.345418022591495 35.83588292251368\n",
      "gradients 0.43882731918943485 1.7762661271654006 3.5461244265337637\n",
      "epoch 39 took 483.6s, optimizer loss=5.8, test mae=2.102\n",
      "5.5625192311337015 0.43882731918943485\n",
      "5.55286090735443 0.4547742796674127\n",
      "5.543384565612971 0.4261495690388513\n",
      "5.531453335589953 0.4830810017946509\n",
      "5.517435718218291 0.4210788522971156\n",
      "5.503741217473383 0.453817070921431\n",
      "5.490505739194249 0.45673900401822326\n",
      "5.47955638061784 0.43656963836289603\n",
      "5.468366149408935 0.4461304895923409\n",
      "5.455701947096643 0.4479876517803052\n",
      "5.443055992972479 0.42036883753986715\n",
      "5.431774464256661 0.43742817416405094\n",
      "5.419248582066031 0.39848438959206156\n",
      "5.406378575136402 0.43895716232910503\n",
      "5.395494172760745 0.39598027170840705\n",
      "5.385433223923155 0.4261200853356216\n",
      "5.3762714181447775 0.39934165761645335\n",
      "5.365767705992362 0.4343710328563332\n",
      "5.3542974162366 0.41015076546480095\n",
      "5.342641193538641 0.5803874474290222\n",
      "5.3341004181060985 0.533197255637537\n",
      "norms 23.524298964546915 38.317317978798414 35.80634244908608\n",
      "gradients 0.533197255637537 1.617407205789107 9.730399223660417\n",
      "epoch 40 took 482.8s, optimizer loss=5.563, test mae=2.041\n",
      "5.3341004181060985 0.533197255637537\n",
      "5.325099456069976 0.5138950562496424\n",
      "5.31462733864109 0.4523173247330187\n",
      "5.3037612071426565 0.43306718408469913\n",
      "5.293054035999807 0.385500508218436\n",
      "5.283042224961325 0.36925802477196806\n",
      "5.273161277695946 0.3701897325956602\n",
      "5.262598175289553 0.33396210437278717\n",
      "5.25287402742867 0.3603808144664775\n",
      "5.244617331112445 0.35002362949443616\n",
      "5.237729146866475 0.34926874673450087\n",
      "5.230781382996407 0.3649149841329812\n",
      "5.223186821158398 0.36202620952400316\n",
      "5.214491321528796 0.5178549815253553\n",
      "5.209069334772186 0.4826947650209957\n",
      "5.20369910547937 0.44246733427789536\n",
      "5.196605076745424 0.4186403661438791\n",
      "5.187796102670127 0.387321880154188\n",
      "5.1775961771061825 0.36936361596213707\n",
      "5.165951494710591 0.3858361429889835\n",
      "5.152673339837648 0.37476481654078636\n",
      "norms 23.633815674541676 38.25899088852943 35.86954262448142\n",
      "gradients 0.37476481654078636 1.6725388976628879 3.413043332768142\n",
      "epoch 41 took 482.5s, optimizer loss=5.334, test mae=2.004\n",
      "5.152673339837648 0.37476481654078636\n",
      "5.14161907971787 0.3859588778653006\n",
      "5.131598489968574 0.4160622190409923\n",
      "5.119838303836818 0.42079722135335373\n",
      "5.108843821595589 0.43297011951531794\n",
      "5.098929239034407 0.441519094020488\n",
      "5.088175444831304 0.466077550393017\n",
      "5.078588525582187 0.4524359530121818\n",
      "5.06975481342495 0.4711858919934474\n",
      "5.060541440652754 0.4359318076156732\n",
      "5.050045798076455 0.5054556246066154\n",
      "5.042760266036555 0.4902886178052931\n",
      "5.035973874147753 0.49809416039448334\n",
      "5.029391012442038 0.4936976504988049\n",
      "5.023040013966786 0.50175942153056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.016770380768949 0.5065833834339937\n",
      "5.01004677822665 0.5149037037331604\n",
      "5.002257444051662 0.532808128707055\n",
      "4.992729439317348 0.5439293134951503\n",
      "4.983675676757684 0.5523742758566202\n",
      "4.974400277305902 0.5663490461717279\n",
      "norms 23.652488548929597 38.226967898555344 35.87321431776054\n",
      "gradients 0.5663490461717279 1.6115722692461538 3.026260310061431\n",
      "epoch 42 took 483.0s, optimizer loss=5.153, test mae=1.965\n",
      "4.974400277305902 0.5663490461717279\n",
      "4.961752522117778 0.6762568599655917\n",
      "4.950727441434729 0.6736464135047577\n",
      "4.937516645787298 0.6632966687386502\n",
      "4.922029657320385 0.6274490787717695\n",
      "4.908357046524167 0.6260289695875201\n",
      "4.892575783662576 0.5681972203832222\n",
      "4.880767778784179 0.5327357485751577\n",
      "4.8691639406850715 0.5157191613642532\n",
      "4.8554380393817445 0.5236477878061044\n",
      "4.845134362470128 0.4927537835068309\n",
      "4.835918890986374 0.5047379982468557\n",
      "4.825995219755122 0.4871364123255694\n",
      "4.815830992222351 0.49329316925252925\n",
      "4.806448336534555 0.4812498025630103\n",
      "4.796863975297421 0.5218152995459033\n",
      "4.789105251526042 0.48791093489280607\n",
      "4.781355304408381 0.4927570172869451\n",
      "4.771578417185676 0.5118536093433719\n",
      "4.75924623871046 0.4211426634862775\n",
      "4.750323477604253 0.40297993994046705\n",
      "norms 23.81241647975035 38.16864762282567 35.95602173650554\n",
      "gradients 0.40297993994046705 1.5310370610198465 4.867624159735653\n",
      "epoch 43 took 482.8s, optimizer loss=4.974, test mae=1.95\n",
      "4.750323477604253 0.40297993994046705\n",
      "4.742171389512085 0.3692270665582758\n",
      "4.73330766075517 0.36249321737562884\n",
      "4.724417427547428 0.2971092887829047\n",
      "4.7169405881282325 0.332661126744134\n",
      "4.709996062493318 0.28233974590465555\n",
      "4.7041144992671216 0.31731085802838654\n",
      "4.6993483384758274 0.31706148242169174\n",
      "4.695247680871883 0.3338721973566386\n",
      "4.690710381788083 0.3438014388187236\n",
      "4.684351012280782 0.4310489344276267\n",
      "4.678986672645528 0.41038961227433207\n",
      "4.673396318111013 0.4056974710543404\n",
      "4.665750633807284 0.39084917497093974\n",
      "4.658270608263151 0.3768020022505457\n",
      "4.650686926141202 0.34919606919328583\n",
      "4.643400446194142 0.33459010151830876\n",
      "4.6358366384058 0.3176464353856794\n",
      "4.6270217906546245 0.3057025880302476\n",
      "4.619195842337337 0.29813116436123965\n",
      "4.611829605921691 0.2911534039117412\n",
      "norms 23.938405508742314 38.123189610900894 36.02628278751417\n",
      "gradients 0.2911534039117412 1.4760983076801768 1.752659891863754\n",
      "epoch 44 took 482.6s, optimizer loss=4.75, test mae=1.91\n",
      "4.611829605921691 0.2911534039117412\n",
      "4.605003903066202 0.2906720820291239\n",
      "4.597891062631804 0.2954285245392093\n",
      "4.590949516056639 0.282987515489195\n",
      "4.58406977098944 0.3160549905585066\n",
      "4.57734186087665 0.29051971328880255\n",
      "4.570652924985607 0.3317644863213561\n",
      "4.562705200563936 0.28450833532231556\n",
      "4.55631076373056 0.28693756425462125\n",
      "4.5502059675128415 0.2869744561205614\n",
      "4.542872386890808 0.3111555034352214\n",
      "4.535559424649918 0.30308742382936077\n",
      "4.527874847287375 0.3447384970701849\n",
      "4.519990038262431 0.3320940095461878\n",
      "4.512267794201433 0.366305459039571\n",
      "4.504533975621583 0.3469983668644421\n",
      "4.496561325821116 0.4233492167255875\n",
      "4.489597895862164 0.405741597881692\n",
      "4.483897787174252 0.3901033242542612\n",
      "4.4773659888133075 0.42247418419972604\n",
      "4.4708318193189065 0.39299821678130126\n",
      "norms 24.039994452195568 38.08223165639368 36.080061562947336\n",
      "gradients 0.39299821678130126 1.384837953359156 2.3950280625906823\n",
      "epoch 45 took 481.0s, optimizer loss=4.612, test mae=1.862\n",
      "4.4708318193189065 0.39299821678130126\n",
      "4.4645820632542375 0.41105027041370246\n",
      "4.458654034536532 0.37856200544350577\n",
      "4.453260872478291 0.3849599665945144\n",
      "4.448125524461044 0.36236545118955393\n",
      "4.442905991485167 0.3602959521249659\n",
      "4.4371050367972975 0.3475013603785475\n",
      "4.431690468033685 0.33221156020963105\n",
      "4.426321222796261 0.33539250734058934\n",
      "4.420815284212634 0.30802771008847707\n",
      "4.415240397496579 0.32007575064181504\n",
      "4.409627317406611 0.28741979698627657\n",
      "4.403397129217193 0.2892379607501022\n",
      "4.395275656477267 0.2423939184421971\n",
      "4.387167213803131 0.24213547456178208\n",
      "4.378787747085037 0.22198199125720458\n",
      "4.370681419547346 0.2213573389474217\n",
      "4.362514602652405 0.24660394797541635\n",
      "4.354160794145163 0.2610821976584707\n",
      "4.346713217967361 0.2830596843561307\n",
      "4.339902227172708 0.3135277261624335\n",
      "norms 24.12549786149434 38.043418161138554 36.12374702241871\n",
      "gradients 0.3135277261624335 1.3308467403558826 2.3385492177360825\n",
      "epoch 46 took 480.8s, optimizer loss=4.471, test mae=1.873\n",
      "4.339902227172708 0.3135277261624335\n",
      "4.33346565056514 0.3346318459176195\n",
      "4.326767717711817 0.35773731457234353\n",
      "4.320443075443311 0.3919140664609119\n",
      "4.314447431093634 0.40020479713606266\n",
      "4.3083791822605075 0.440792153123842\n",
      "4.302639544390785 0.437547707429039\n",
      "4.297546540103945 0.45803070174663674\n",
      "4.2926350712548995 0.4562137975350431\n",
      "4.28730417246935 0.48000983830087557\n",
      "4.2807162375622445 0.45654129082636025\n",
      "4.275865462396947 0.4613460077664189\n",
      "4.270543756839996 0.4344012361259116\n",
      "4.264927744602965 0.42167718810610716\n",
      "4.258729717530231 0.40199991980973815\n",
      "4.251722644656657 0.37654261599831457\n",
      "4.243639506906018 0.3896468206250441\n",
      "4.234705491547269 0.3538026430124292\n",
      "4.227165336712942 0.3295364491161776\n",
      "4.220179573856378 0.3369892170976069\n",
      "4.213402691727429 0.31696501863357884\n",
      "norms 24.214179460437148 38.01526695304834 36.165720036279204\n",
      "gradients 0.31696501863357884 1.307163323424038 1.6093087300965339\n",
      "epoch 47 took 481.4s, optimizer loss=4.34, test mae=1.848\n",
      "4.213402691727429 0.31696501863357884\n",
      "4.207200417815732 0.32108400143743376\n",
      "4.200681831912191 0.34430835548041594\n",
      "4.194939169955732 0.3238924672903655\n",
      "4.189554469597837 0.3486454055890498\n",
      "4.184126370587284 0.31571429750318775\n",
      "4.179385800479356 0.3177764776897345\n",
      "4.174598782507155 0.2963484478303976\n",
      "4.170014707596046 0.28909054817706087\n",
      "4.1651955652814046 0.2869580844615048\n",
      "4.159423968190902 0.2663138385982555\n",
      "4.153812074511334 0.25858301810285583\n",
      "4.148918325571511 0.2551411581354523\n",
      "4.143370980343491 0.2506846292232696\n",
      "4.136897148158979 0.25733089159664524\n",
      "4.1299086939482885 0.25309382514185286\n",
      "4.122743615923062 0.2621828516160193\n",
      "4.115664536013095 0.2593252229353988\n",
      "4.108998830916024 0.2648596206745199\n",
      "4.1027846550521785 0.26388251635862536\n",
      "4.096830361097502 0.25872678650096215\n",
      "norms 24.26482414145579 37.9796841957986 36.19216088095289\n",
      "gradients 0.25872678650096215 1.2998607472338992 2.707200774761745\n",
      "epoch 48 took 480.8s, optimizer loss=4.213, test mae=1.831\n",
      "4.096830361097502 0.25872678650096215\n",
      "4.090445190681126 0.2880841539551509\n",
      "4.085677320853061 0.26681120908580125\n",
      "4.0814969714524025 0.26602720978508093\n",
      "4.0762309702767565 0.2562429048048089\n",
      "4.070293556444096 0.2527807885073638\n",
      "4.064131646152157 0.26579483605812715\n",
      "4.057376700506954 0.27454458633000695\n",
      "4.0504348940740185 0.2820680762933896\n",
      "4.044590976268612 0.3073847129318422\n",
      "4.039336019889254 0.3406288367066229\n",
      "4.033295058666692 0.3450242156986757\n",
      "4.029112128162767 0.3536890457733494\n",
      "4.024644624417644 0.39618968617648065\n",
      "4.0186749636359815 0.362335019363893\n",
      "4.01377992416535 0.3781390443916353\n",
      "4.008405806886307 0.4151449434363589\n",
      "4.001266060313859 0.3879410890200884\n",
      "3.995663778806017 0.392439595422749\n",
      "3.9900294194531183 0.3993895764379824\n",
      "3.983544978890703 0.37476293604033134\n",
      "norms 24.38805033561337 37.94552559493996 36.25845142866077\n",
      "gradients 0.37476293604033134 1.1809864673119548 1.6997185265060344\n",
      "epoch 49 took 479.4s, optimizer loss=4.097, test mae=1.818\n",
      "3.983544978890703 0.37476293604033134\n",
      "3.9781284378224915 0.3695353156422498\n",
      "3.9722392876475086 0.3693180466269155\n",
      "3.966887331781058 0.3467679862061413\n",
      "3.9613629938557673 0.36815796356141334\n",
      "3.9551469437153104 0.3289653470362383\n",
      "3.949596954374818 0.3478513867892393\n",
      "3.9448885403895027 0.3211806610172854\n",
      "3.9401277558271786 0.3284540363768893\n",
      "3.9354971775281458 0.29713488040290714\n",
      "3.931459561317657 0.29534876849649006\n",
      "3.9275785998227746 0.2759146338413646\n",
      "3.9241826648818137 0.26848285272754546\n",
      "3.9210616084116925 0.2616078549738144\n",
      "3.917993560806074 0.2538439546127505\n",
      "3.9141067876160185 0.26945147659428637\n",
      "3.9091059200370846 0.26341996147431695\n",
      "3.9049545700760375 0.24272839055702441\n",
      "3.9007976370658506 0.2737964473644173\n",
      "3.897003929303223 0.2569213985364498\n",
      "3.893017491131161 0.28521351489336355\n",
      "norms 24.401698970159085 37.922170760696225 36.25858938669988\n",
      "gradients 0.28521351489336355 1.1470594565566292 1.5834935364346794\n",
      "epoch 50 took 482.8s, optimizer loss=3.984, test mae=1.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.893017491131161 0.28521351489336355\n",
      "3.8889034118285926 0.28053345284197506\n",
      "3.885069764136094 0.2732632682987365\n",
      "3.881487856665155 0.2799166074795175\n",
      "3.8777942634364653 0.2590799099121306\n",
      "3.8737512585865748 0.27915734224916544\n",
      "3.8699092635185353 0.26307233573204436\n",
      "3.8661056715220985 0.2803442236162248\n",
      "3.8620013670107634 0.2775900117423964\n",
      "3.8580092575039653 0.28814657136343286\n",
      "3.853564055556647 0.306447667656884\n",
      "3.8494663409169774 0.3055052094710466\n",
      "3.8450596693110213 0.3337322633790063\n",
      "3.8401885083776435 0.31669645545356806\n",
      "3.8355883911092876 0.3316333853256272\n",
      "3.831248182642403 0.3242038604788061\n",
      "3.827115402621885 0.2959695122617209\n",
      "3.8228738368227164 0.3024991754126496\n",
      "3.819740824036354 0.28184698913145106\n",
      "3.8164534353868205 0.29086651820076903\n",
      "3.8133316677321782 0.27935152268595465\n",
      "norms 24.494326219645924 37.8856315040917 36.31232390785696\n",
      "gradients 0.27935152268595465 1.0360792071910983 1.62180884834735\n",
      "epoch 51 took 482.2s, optimizer loss=3.893, test mae=1.776\n",
      "3.8133316677321782 0.27935152268595465\n",
      "3.8102328952862226 0.26643539259328547\n",
      "3.806887693407646 0.2627511513475398\n",
      "3.8033189560349028 0.24214869993769947\n",
      "3.7995046000029955 0.24549303808399234\n",
      "3.7953485682907635 0.24011418231888845\n",
      "3.79159280253419 0.2154110228818476\n",
      "3.788025099946645 0.24082935855680457\n",
      "3.784277481059718 0.21969733994211998\n",
      "3.780403326120354 0.24101879705998525\n",
      "3.776319059805748 0.22648386554444824\n",
      "3.771994506244531 0.2434577614244321\n",
      "3.767344961020519 0.22541302963667323\n",
      "3.762479219250924 0.240243406166844\n",
      "3.757739803088329 0.23131908710764051\n",
      "3.7533055579990338 0.22424647442817663\n",
      "3.7493481856488406 0.23590439563379811\n",
      "3.745807940696668 0.22604829429416023\n",
      "3.7430589347247696 0.21126640413487258\n",
      "3.7403469956917648 0.21591986747374398\n",
      "3.7379184489907202 0.20732491425317015\n",
      "norms 24.528061433078577 37.865473030325774 36.32546164393681\n",
      "gradients 0.20732491425317015 1.114131037278406 1.5043315686059449\n",
      "epoch 52 took 490.9s, optimizer loss=3.813, test mae=1.768\n",
      "3.7379184489907202 0.20732491425317015\n",
      "3.7357417720111887 0.18943695641431643\n",
      "3.733422051485996 0.21449309972030217\n",
      "3.731539935052429 0.19852667211349964\n",
      "3.729241588658377 0.24017288853438012\n",
      "3.726865166164728 0.2334485944512409\n",
      "3.7238357739474615 0.22388347062227876\n",
      "3.7192712497440215 0.22716429853227382\n",
      "3.714393961101865 0.22515786737264662\n",
      "3.709679447032125 0.2290869672585804\n",
      "3.704280955731988 0.24683887552139055\n",
      "3.699284829203435 0.24136879303634606\n",
      "3.69427493387985 0.2614823600878071\n",
      "3.6886740570137 0.24986001061964236\n",
      "3.6837124979577536 0.2561981190125739\n",
      "3.6784838459756406 0.26366020695775466\n",
      "3.6742028099239676 0.2596423379285068\n",
      "3.6694850892408337 0.26034284075982916\n",
      "3.663854686678754 0.2782731095416387\n",
      "3.6586239732449592 0.23358394806004937\n",
      "3.6553005549370283 0.23956455708609842\n",
      "norms 24.626153229670475 37.83359867538412 36.38048673041765\n",
      "gradients 0.23956455708609842 1.1034985910158284 1.75362254011926\n",
      "epoch 53 took 484.5s, optimizer loss=3.738, test mae=1.768\n",
      "3.6553005549370283 0.23956455708609842\n",
      "3.652190363452199 0.22944335623048026\n",
      "3.6491304701705536 0.23494312493744507\n",
      "3.646273794456076 0.23418114758991287\n",
      "3.6434466299798713 0.25061212077130895\n",
      "3.6407866242840954 0.2419859479955201\n",
      "3.638453595058564 0.26479381045287687\n",
      "3.6361344187929214 0.2603088816472922\n",
      "3.633625809884342 0.29759693347034044\n",
      "3.6307553194181894 0.28118630654290816\n",
      "3.628188358391532 0.29601609865185324\n",
      "3.625498747467767 0.30544416924991963\n",
      "3.6223539287267066 0.31056545744085146\n",
      "3.6186999208696493 0.3236841823565876\n",
      "3.6145666904872766 0.3233573498145632\n",
      "3.610164955675814 0.3304690450546877\n",
      "3.6055771910938663 0.332166973344089\n",
      "3.6009433044676227 0.3300446329850504\n",
      "3.596184975351131 0.3318925343706332\n",
      "3.591020921553598 0.31651227446600705\n",
      "3.5858162733272483 0.32258727056240205\n",
      "norms 24.712680457705478 37.81007847176598 36.42478110507383\n",
      "gradients 0.32258727056240205 1.0419741978065442 1.5481496513389255\n",
      "epoch 54 took 483.3s, optimizer loss=3.655, test mae=1.747\n",
      "3.5858162733272483 0.32258727056240205\n",
      "3.580889490780359 0.30996729940488155\n",
      "3.5766545567942156 0.3162897354183496\n",
      "3.5727835926205804 0.3118114246377845\n",
      "3.5685502844241563 0.3315504196874996\n",
      "3.5642081494705278 0.31711480990570456\n",
      "3.560011736678731 0.3412554889564694\n",
      "3.555864627157508 0.3136394282185557\n",
      "3.5518526382403257 0.3446690159496405\n",
      "3.547685491409366 0.2904847560084895\n",
      "3.5440988985904736 0.3027095803634188\n",
      "3.5404205868019707 0.29275386887213484\n",
      "3.537576628659979 0.2838847362923066\n",
      "3.5345694043189515 0.2786099772862112\n",
      "3.531219824871902 0.2671672286043007\n",
      "3.5278025649373697 0.25459662271660216\n",
      "3.5245335471411607 0.2408809022824351\n",
      "3.5214374292746915 0.22795952973699013\n",
      "3.5185218216632004 0.21720329911945993\n",
      "3.515598730152183 0.20831664036385997\n",
      "3.5126108701499064 0.2050791954104351\n",
      "norms 24.81871980051177 37.78125455289152 36.48375798806811\n",
      "gradients 0.2050791954104351 1.0217309453288652 1.6787783107540866\n",
      "epoch 55 took 486.1s, optimizer loss=3.586, test mae=1.737\n",
      "3.5126108701499064 0.2050791954104351\n",
      "3.5094123735666782 0.20687282313746133\n",
      "3.5064902827810536 0.20624222538221307\n",
      "3.5039054899377224 0.19549966882884137\n",
      "3.5007936878756394 0.2297198312750678\n",
      "3.4982424652571473 0.22692682158220887\n",
      "3.495674017291317 0.2383170828153353\n",
      "3.4930643051443235 0.2338064501891097\n",
      "3.490317499580279 0.277781336017112\n",
      "3.487539687697348 0.25057738004300995\n",
      "3.4851721938890496 0.2611441954654048\n",
      "3.482649868790893 0.24804975059484663\n",
      "3.4804056872407583 0.2421071327734073\n",
      "3.4784254955764786 0.24501915485987463\n",
      "3.476289260164317 0.2150560153965238\n",
      "3.4744598853871635 0.22198095773757373\n",
      "3.472950000729438 0.2141660903957311\n",
      "3.471158006781304 0.24135832855355113\n",
      "3.4691470185731976 0.24627881572304325\n",
      "3.467409381127888 0.2389126433942414\n",
      "3.4653961767452572 0.24571544000499673\n",
      "norms 24.87045649561159 37.76511595796001 36.51017430668732\n",
      "gradients 0.24571544000499673 1.0557650512215313 1.8942495128617527\n",
      "epoch 56 took 479.2s, optimizer loss=3.513, test mae=1.728\n",
      "3.4653961767452572 0.24571544000499673\n",
      "3.463036815246198 0.23620297998412784\n",
      "3.460320272139642 0.24574600352501863\n",
      "3.457253170198291 0.2298431710106206\n",
      "3.454063971094054 0.24048867225814174\n",
      "3.4508530908238058 0.2323031041726762\n",
      "3.447601327854952 0.2397395655792097\n",
      "3.4440709680515367 0.23718306140296846\n",
      "3.4401444590401984 0.24122666669054244\n",
      "3.4360425692973835 0.26445502872214627\n",
      "3.432306166057448 0.25900090491733013\n",
      "3.4286741262741494 0.27943315319629825\n",
      "3.425826215331487 0.2738442351050998\n",
      "3.4229449889847716 0.2742221332728394\n",
      "3.4193120856571264 0.26952076306054024\n",
      "3.41653456169908 0.2558630660725279\n",
      "3.4140983182625955 0.25002062507154893\n",
      "3.411250784792027 0.31053546868449794\n",
      "3.4090239947011347 0.29511798175912\n",
      "3.4062440008073307 0.2759144869407964\n",
      "3.4029296133418923 0.2647205937145263\n",
      "norms 24.926422382749358 37.74449412251809 36.539211829916134\n",
      "gradients 0.2647205937145263 0.9736117123850876 3.911090467330719\n",
      "epoch 57 took 480.2s, optimizer loss=3.465, test mae=1.707\n",
      "3.4029296133418923 0.2647205937145263\n",
      "3.399177232578525 0.25739325487230624\n",
      "3.395754479043981 0.2367699133982439\n",
      "3.3924344000563473 0.26594991094966247\n",
      "3.3891551424226525 0.24246519253678425\n",
      "3.386073292187951 0.24339773640541904\n",
      "3.382349489902019 0.27532530191258736\n",
      "3.3791176825734452 0.25331334639764497\n",
      "3.37595438970227 0.2565688700081164\n",
      "3.3721391508932412 0.22781751433759828\n",
      "3.3683742025408256 0.23036767393076074\n",
      "3.365175978917073 0.22590792630288367\n",
      "3.3612135900330427 0.25718096348290437\n",
      "3.3576895397552695 0.25619401795462693\n",
      "3.3531232573597674 0.27327036804436866\n",
      "3.3482045024222735 0.2452974634473988\n",
      "3.3432547581479675 0.2847172389742007\n",
      "3.33867173770871 0.24998697595036287\n",
      "3.3346029880575103 0.25436548933178244\n",
      "3.330502931727507 0.2224509032095402\n",
      "3.3265782926991903 0.22664309523000245\n",
      "norms 25.048886729207776 37.71296559826092 36.60509289515448\n",
      "gradients 0.22664309523000245 0.971291996017156 2.013876770589247\n",
      "epoch 58 took 480.3s, optimizer loss=3.403, test mae=1.689\n",
      "3.3265782926991903 0.22664309523000245\n",
      "3.3228967171359742 0.2125375449304048\n",
      "3.3193750779051006 0.22018922955053827\n",
      "3.3162532779844294 0.2210427041384564\n",
      "3.3133502455058674 0.22730119935974066\n",
      "3.310498384641747 0.2345840109818834\n",
      "3.3074316654997507 0.2379091299804158\n",
      "3.304198594256096 0.3053085391954014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.301034628932583 0.2974649472512173\n",
      "3.298038595191253 0.31959325729363675\n",
      "3.2947430754716676 0.3086744739031992\n",
      "3.291478300518285 0.3233794062139988\n",
      "3.2883041803245447 0.2922840646333836\n",
      "3.2854647614368897 0.2918291440390617\n",
      "3.2824588635895546 0.2558918795411774\n",
      "3.279976720922734 0.243815821218006\n",
      "3.27729521412883 0.22683985002359214\n",
      "3.2743650395406756 0.20909871088243065\n",
      "3.2716412625728406 0.20905415388414877\n",
      "3.269201546243861 0.19265311097126345\n",
      "3.2669184371345663 0.22886795414396344\n",
      "norms 25.177203066188763 37.68410569909363 36.678003259091284\n",
      "gradients 0.22886795414396344 0.9246366523143726 1.8470918136336332\n",
      "epoch 59 took 490.6s, optimizer loss=3.327, test mae=1.68\n",
      "3.2669184371345663 0.22886795414396344\n",
      "3.264832073447862 0.2154342635967741\n",
      "3.263211580305279 0.21396161528507857\n",
      "3.261346810352668 0.21826568962402657\n",
      "3.259267234144055 0.19942529803292355\n",
      "3.257270526181569 0.2478739597072439\n",
      "3.2557956759889404 0.23997068255346723\n",
      "3.253939043067701 0.23120433506052285\n",
      "3.2514447055034466 0.22628023733876812\n",
      "3.2488567968806503 0.22140492634517572\n",
      "3.246336010848144 0.21624651456438537\n",
      "3.244144211564685 0.21366617644018196\n",
      "3.2415823272572895 0.2355143560661978\n",
      "3.2394863524982265 0.23568895641324286\n",
      "3.2371696556624294 0.23716483999427082\n",
      "3.234288068985349 0.24793593348755233\n",
      "3.2318583097998657 0.24752328777853505\n",
      "3.228698785685803 0.3119298443217359\n",
      "3.2260184082283434 0.3100983168048983\n",
      "3.222071550450417 0.3495062278987091\n",
      "3.218059886186511 0.34364373354385036\n",
      "norms 25.197020107739647 37.67183913349663 36.68334153745252\n",
      "gradients 0.34364373354385036 0.9019727506818638 5.05230672673443\n",
      "epoch 60 took 486.5s, optimizer loss=3.267, test mae=1.666\n",
      "3.218059886186511 0.34364373354385036\n",
      "3.2134116680174722 0.3691076384069977\n",
      "3.208500919436118 0.3399229217480216\n",
      "3.203882500069668 0.3467672540871953\n",
      "3.1991122169169057 0.2997985279756256\n",
      "3.195559975151304 0.2908918442683999\n",
      "3.191262619661783 0.283537010747915\n",
      "3.1872724440561537 0.27729892472030204\n",
      "3.183076230728218 0.2581243073266754\n",
      "3.1798638522730616 0.25405610797670775\n",
      "3.176329732812279 0.23798679331785372\n",
      "3.1722572680177836 0.2636042059848637\n",
      "3.1688324191435737 0.2340715816149443\n",
      "3.1657432176548963 0.2549426438027497\n",
      "3.1628550819531487 0.2283732766577023\n",
      "3.160207568638341 0.2343121256455083\n",
      "3.157877767435769 0.20941364930857442\n",
      "3.1557964740329716 0.21033205816351938\n",
      "3.1538581471821634 0.1914557678630379\n",
      "3.151888237539232 0.2071391134419469\n",
      "3.14963707773238 0.19179402201644566\n",
      "norms 25.343014359298287 37.63592358899394 36.76759658518254\n",
      "gradients 0.19179402201644566 0.8036626211421919 3.0635739352134337\n",
      "epoch 61 took 489.0s, optimizer loss=3.218, test mae=1.653\n",
      "3.14963707773238 0.19179402201644566\n",
      "3.148108389075875 0.17416961055772198\n",
      "3.1467737033832988 0.17169593821352577\n",
      "3.1452028748791614 0.16671450372893246\n",
      "3.1433408573326282 0.16704517859366452\n",
      "3.1414994984423363 0.1680611069608201\n",
      "3.139075457273524 0.19599871102379363\n",
      "3.136259640755882 0.20500372496752126\n",
      "3.133149718696379 0.20606888321185\n",
      "3.1291336775876406 0.30117191586380726\n",
      "3.1264819474225893 0.29493525719464264\n",
      "3.123113555679017 0.2908263293214451\n",
      "3.119328803475152 0.29756539371281887\n",
      "3.1148940376651715 0.29423449452646067\n",
      "3.1121971996934104 0.2757534848422738\n",
      "3.109527911186693 0.25684567541693265\n",
      "3.1066007564453413 0.26742243947138317\n",
      "3.103451922912193 0.23793733490615573\n",
      "3.1007892233526593 0.21994853213191914\n",
      "3.0986606819075124 0.2181529666408684\n",
      "3.096486148415139 0.20928211654240894\n",
      "norms 25.354365073547896 37.620235483162965 36.76810262251541\n",
      "gradients 0.20928211654240894 0.8205290661985721 2.030077013884742\n",
      "epoch 62 took 486.7s, optimizer loss=3.15, test mae=1.641\n",
      "3.096486148415139 0.20928211654240894\n",
      "3.0946103159045713 0.2051382700808667\n",
      "3.092657161268262 0.26138478585703606\n",
      "3.0908942916174134 0.2493271441346699\n",
      "3.0889870187358173 0.23884947163462095\n",
      "3.086654367131402 0.22939388929243013\n",
      "3.083866632293933 0.22843707194224042\n",
      "3.0818973179640707 0.2125854986362998\n",
      "3.080040237836499 0.20708644611499655\n",
      "3.0777749266098926 0.201462350818346\n",
      "3.0760014842149235 0.1955813831259441\n",
      "3.074229393555816 0.19745793001474268\n",
      "3.072124537721367 0.19983389961135495\n",
      "3.069636224526192 0.20453279625309384\n",
      "3.066919755935527 0.21195278830019562\n",
      "3.063896996225494 0.2197335821642196\n",
      "3.0606451977897224 0.22532660585598838\n",
      "3.057097283611006 0.25545825925644394\n",
      "3.053683885189404 0.2527384788457082\n",
      "3.050333541202049 0.27530404449053536\n",
      "3.0468025955352753 0.24564489996496314\n",
      "norms 25.403799415367956 37.60446229505394 36.79501204331497\n",
      "gradients 0.24564489996496314 0.7993196668712943 2.9838812942397546\n",
      "epoch 63 took 489.0s, optimizer loss=3.096, test mae=1.624\n",
      "3.0468025955352753 0.24564489996496314\n",
      "3.0436828836422967 0.27970379413302343\n",
      "3.0411728555757227 0.258966080975354\n",
      "3.0386835524599296 0.24681128249589776\n",
      "3.035970150923719 0.24176782377053652\n",
      "3.0334827653177086 0.2024394740570968\n",
      "3.0317835008073306 0.19467476215825158\n",
      "3.0301427235552763 0.19240874752777007\n",
      "3.028868783030066 0.18197021798224125\n",
      "3.027208970541764 0.166527540269628\n",
      "3.025118653704106 0.16621613907815333\n",
      "3.0226801213255836 0.16727742412219018\n",
      "3.020433565415491 0.17833078034784627\n",
      "3.0181362285891904 0.18240825555478837\n",
      "3.0152708763702902 0.22741366211469827\n",
      "3.013039900553491 0.22594714006943528\n",
      "3.0106573875096316 0.24709784784080147\n",
      "3.0085778258149345 0.23571790328244838\n",
      "3.006507911035219 0.23032904725698503\n",
      "3.004028151822022 0.2138008370951544\n",
      "3.001232091985954 0.2009946444047089\n",
      "norms 25.467120399475316 37.58670969229742 36.83158898800656\n",
      "gradients 0.2009946444047089 0.7321384920636452 1.671766962279122\n",
      "epoch 64 took 486.2s, optimizer loss=3.047, test mae=1.611\n",
      "3.001232091985954 0.2009946444047089\n",
      "2.998800256692542 0.17599608898841818\n",
      "2.996989613146245 0.16533809851072584\n",
      "2.9954192149459846 0.16954495310974127\n",
      "2.993729736200696 0.15059186515735717\n",
      "2.9923034405020448 0.16037376280075227\n",
      "2.99080416547379 0.15454063246873234\n",
      "2.9894367324232554 0.1615487289049299\n",
      "2.987999301180898 0.17377095110568158\n",
      "2.986043157800889 0.18661516613729695\n",
      "2.984196666194099 0.19539726172349178\n",
      "2.9821395872351126 0.20608225188072526\n",
      "2.979719802347421 0.21112553567773581\n",
      "2.977346518806045 0.21238219290941496\n",
      "2.9749424287296375 0.21501815814235484\n",
      "2.9725192694963267 0.20715941248037645\n",
      "2.9702022004200948 0.20807789741725258\n",
      "2.9678630200293363 0.1917266261938407\n",
      "2.965747178597277 0.18238966668203574\n",
      "2.963902298329089 0.18589167635822776\n",
      "2.962193945334259 0.17491347614316985\n",
      "norms 25.474675242063732 37.57203572516569 36.82589976092584\n",
      "gradients 0.17491347614316985 0.7194529084593487 0.8895849386016346\n",
      "epoch 65 took 486.1s, optimizer loss=3.001, test mae=1.604\n",
      "2.962193945334259 0.17491347614316985\n",
      "2.9606882186551386 0.17830946377465107\n",
      "2.9591821345362272 0.17944504435719735\n",
      "2.957832401078622 0.18134707363754435\n",
      "2.9564037417393285 0.1986634019114052\n",
      "2.954904061426995 0.2004407570763052\n",
      "2.953394269574972 0.21449584331435181\n",
      "2.9516615754644353 0.22007728785867214\n",
      "2.949870134682297 0.23179022185509787\n",
      "2.9476263033672825 0.24448799436591256\n",
      "2.9449076050260454 0.25416598727309697\n",
      "2.9422225993873767 0.35176297096289855\n",
      "2.9398108648424883 0.34257804465297964\n",
      "2.9370147832131264 0.3245149702824055\n",
      "2.933895582213872 0.30794273557499247\n",
      "2.9306130197036393 0.28256404014363157\n",
      "2.927395750980245 0.27545799687977524\n",
      "2.924891525029472 0.24941635839621618\n",
      "2.9228184518805342 0.23592725978851153\n",
      "2.9206527695322144 0.2228996522875438\n",
      "2.9185259050131616 0.21096707555877003\n",
      "norms 25.57874290254624 37.55266594579016 36.89119410536856\n",
      "gradients 0.21096707555877003 0.7310829150287365 1.8095437978319\n",
      "epoch 66 took 486.1s, optimizer loss=2.962, test mae=1.59\n",
      "2.9185259050131616 0.21096707555877003\n",
      "2.9165501012690576 0.21633952023214542\n",
      "2.9145714414569337 0.2024052425549455\n",
      "2.9130497125155026 0.2196567924507656\n",
      "2.911866238467234 0.21831225306096508\n",
      "2.9106109994271407 0.2369852836989114\n",
      "2.9091688227363126 0.2349094401158906\n",
      "2.907975695599287 0.2382749509174053\n",
      "2.906729411343419 0.240349453518353\n",
      "2.905316925259243 0.24277058967576554\n",
      "2.903671562299764 0.22726724590601988\n",
      "2.902029880506897 0.2333482867779694\n",
      "2.9004091145969224 0.22152379371910078\n",
      "2.8985971416761105 0.2519777762153441\n",
      "2.896911029086474 0.2285721446390204\n",
      "2.895223613060207 0.23078073472324453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8931048430142248 0.1922174567208476\n",
      "2.8916068482154795 0.1859154028428729\n",
      "2.889590331996328 0.17822559721735395\n",
      "2.88729186562552 0.16775189332980936\n",
      "2.884997900705088 0.15329409194826674\n",
      "norms 25.58266149163657 37.542749179182834 36.89051227893414\n",
      "gradients 0.15329409194826674 0.7194228635813524 1.0733419400304431\n",
      "epoch 67 took 483.9s, optimizer loss=2.919, test mae=1.582\n",
      "2.884997900705088 0.15329409194826674\n",
      "2.8828739989956973 0.13703725297087627\n",
      "2.8808693216136882 0.13268276238222865\n",
      "2.8789570315644504 0.11252470658444251\n",
      "2.877164596066519 0.1082475470651955\n",
      "2.8755374360724897 0.09837500433287086\n",
      "2.8736877229002573 0.10479145082095749\n",
      "2.871774325036507 0.10340058131483097\n",
      "2.8695856089136753 0.11225964796515175\n",
      "2.8667791284120536 0.12691453925027005\n",
      "2.8639909499450433 0.12461279207570157\n",
      "2.86147811962106 0.1550603339760781\n",
      "2.859121774459706 0.1538535167420518\n",
      "2.85691061304132 0.1594136378415861\n",
      "2.854385249189587 0.18708657344073787\n",
      "2.8521993331569573 0.17743504541914995\n",
      "2.8499202268732327 0.1955657834830948\n",
      "2.8474952694017186 0.17298147696447777\n",
      "2.84539193789478 0.1698245690350219\n",
      "2.843610945876055 0.2569843272831171\n",
      "2.8421034650077375 0.2278384687839985\n",
      "norms 25.672034258718885 37.526305201368274 36.94692226017183\n",
      "gradients 0.2278384687839985 0.7394261344700365 5.304340159685147\n",
      "epoch 68 took 484.2s, optimizer loss=2.885, test mae=1.566\n",
      "2.8421034650077375 0.2278384687839985\n",
      "2.8405673533872835 0.20581670135235222\n",
      "2.8389871816319707 0.18626910571171162\n",
      "2.8373461002346207 0.17021392488016704\n",
      "2.8356586960006833 0.15664328969685265\n",
      "2.833933668758054 0.1589498239618396\n",
      "2.832005976900282 0.15641480213317024\n",
      "2.8306572711507774 0.16109955211631308\n",
      "2.8293378319859124 0.1724240060274619\n",
      "2.8277788773291728 0.1872943406436546\n",
      "2.826169922435078 0.1911796258005913\n",
      "2.824659455721425 0.21348915325003925\n",
      "2.8229113739517624 0.21251415183383532\n",
      "2.821203675321499 0.21732754881148084\n",
      "2.819649493410118 0.2179244511099819\n",
      "2.8179149896292266 0.21066726163765098\n",
      "2.8161581560905047 0.20315572248208102\n",
      "2.814308527656429 0.19792686083369576\n",
      "2.8120578935195986 0.18830680334804317\n",
      "2.810276159613833 0.17472491728304013\n",
      "2.80844751758874 0.17562330099700943\n",
      "norms 25.764588601681105 37.50616877605664 36.99920944950014\n",
      "gradients 0.17562330099700943 0.6836569960564219 0.8211101956855297\n",
      "epoch 69 took 483.6s, optimizer loss=2.842, test mae=1.556\n",
      "2.80844751758874 0.17562330099700943\n",
      "2.806245868540362 0.1594421177497003\n",
      "2.803840025997713 0.17017157944006198\n",
      "2.8018265738837522 0.1692034285975785\n",
      "2.7996187704245967 0.18334928773874587\n",
      "2.7971691552251596 0.18321233059491102\n",
      "2.794560082767713 0.1972093460304605\n",
      "2.7919290054616104 0.1902811288434912\n",
      "2.7892037355314376 0.2378814101287796\n",
      "2.7872461374167012 0.21957618064863674\n",
      "2.7855197031211656 0.21762012407251008\n",
      "2.7839186642093003 0.19504955098176766\n",
      "2.7826440318452077 0.18140140895698992\n",
      "2.7813528133015257 0.16586704305030195\n",
      "2.780214065036688 0.15171852891182536\n",
      "2.7794027936746355 0.13819029862724175\n",
      "2.7786052863335278 0.1468445228615413\n",
      "2.7778472944745602 0.1380140234942557\n",
      "2.7770621769781934 0.15282564316885636\n",
      "2.776175999700964 0.14337176353449224\n",
      "2.77527801854203 0.15929084144911793\n",
      "norms 25.753014803034265 37.50060978934012 36.99043565554387\n",
      "gradients 0.15929084144911793 0.6677706722147992 2.578052855791204\n",
      "epoch 70 took 484.1s, optimizer loss=2.808, test mae=1.555\n",
      "2.77527801854203 0.15929084144911793\n",
      "2.7742504471389355 0.15258601992655413\n",
      "2.773093571425638 0.1686339398634228\n",
      "2.77184505970553 0.17357174313602017\n",
      "2.7704472161807083 0.16134952484758283\n",
      "2.769140315485549 0.17126640582564404\n",
      "2.7675984730691288 0.1817075731812027\n",
      "2.7660791962925573 0.1812514648081\n",
      "2.764262872489481 0.19083180828642068\n",
      "2.7615156480075562 0.1766208363193737\n",
      "2.759023415617965 0.17368782987762696\n",
      "2.7560697931503078 0.23250181369037917\n",
      "2.7534720343429324 0.21716877858783287\n",
      "2.7504057121092584 0.20422369740606994\n",
      "2.746778579128559 0.1813726935229853\n",
      "2.7428617985419685 0.16998181990651265\n",
      "2.7388875002874014 0.16154266164546505\n",
      "2.735923760747025 0.13903505907109578\n",
      "2.7335605046211007 0.13587872397488837\n",
      "2.7312247146525177 0.17671768463263993\n",
      "2.7294574180925393 0.17089354128512882\n",
      "norms 25.839122230584856 37.48302214355775 37.04110314209393\n",
      "gradients 0.17089354128512882 0.6966590826017843 2.3104591432386687\n",
      "epoch 71 took 484.1s, optimizer loss=2.775, test mae=1.553\n",
      "2.7294574180925393 0.17089354128512882\n",
      "2.727639665333732 0.174283947072587\n",
      "2.7256935608936073 0.18482610638768635\n",
      "2.723935344611926 0.17233920624068902\n",
      "2.7225626752198417 0.17986558899430952\n",
      "2.7211956152777868 0.18540487846651393\n",
      "2.7197921893850667 0.19193972316022312\n",
      "2.7183630157489533 0.21421614888406437\n",
      "2.7169717893156125 0.21749282448468882\n",
      "2.715550789711559 0.23889887818468467\n",
      "2.7140829870246734 0.24375674168625625\n",
      "2.712555130296993 0.2508511269331381\n",
      "2.7104900409331836 0.25024217947006844\n",
      "2.7088227340683813 0.25214927429260703\n",
      "2.7068688164231713 0.24875665171730119\n",
      "2.704348847298287 0.25565533012353775\n",
      "2.701922680823601 0.2434366091148729\n",
      "2.699347788300338 0.24528109087670436\n",
      "2.6966150667909217 0.2213143734400914\n",
      "2.694113522593302 0.2116607736253252\n",
      "2.691561137061959 0.1926150490074454\n",
      "norms 25.819851188452173 37.47448060941682 37.01980224670473\n",
      "gradients 0.1926150490074454 0.7477426713141044 0.7359439619341032\n",
      "epoch 72 took 483.2s, optimizer loss=2.729, test mae=1.528\n",
      "2.691561137061959 0.1926150490074454\n",
      "2.689330154032091 0.17026917156123364\n",
      "2.687469893632016 0.29218243443399095\n",
      "2.686065368927333 0.24911371727048912\n",
      "2.684648215837895 0.21160067174189962\n",
      "2.6832346598050494 0.18012022820097232\n",
      "2.681853854344497 0.15927045327841205\n",
      "2.6805146169938663 0.15015143415740828\n",
      "2.67920433403927 0.15280503010122115\n",
      "2.6779004616772184 0.16236626112428967\n",
      "2.67657296447398 0.17775425160666317\n",
      "2.675169531554717 0.19352542357786248\n",
      "2.6738011369656913 0.20809965312607828\n",
      "2.6726628049291996 0.21761595934398933\n",
      "2.6714037627069254 0.22545950936341097\n",
      "2.6701579303688714 0.22366912616635384\n",
      "2.6688688909669884 0.2356955004235944\n",
      "2.6678613140069625 0.2228858055870269\n",
      "2.6668435068874428 0.21546008686241414\n",
      "2.6657545211314284 0.19364713086493593\n",
      "2.6648615937688764 0.17905892302198492\n",
      "norms 25.865897422748688 37.462435756740156 37.04381047129833\n",
      "gradients 0.17905892302198492 0.6824476665214634 0.7342842807421597\n",
      "epoch 73 took 484.1s, optimizer loss=2.692, test mae=1.523\n",
      "2.6648615937688764 0.17905892302198492\n",
      "2.6638909622667883 0.16298056080722012\n",
      "2.6629030893068264 0.1435679850353087\n",
      "2.6619914298100142 0.13914365946899857\n",
      "2.6611095054640685 0.1334903897465821\n",
      "2.6601900177355224 0.15226476862007204\n",
      "2.6592641389101326 0.15299023202176304\n",
      "2.6584062469352228 0.16922096368186698\n",
      "2.657358460400089 0.1806206835551636\n",
      "2.6560754317807063 0.2003439086378242\n",
      "2.6544712936010004 0.22862424870424822\n",
      "2.6526051478126726 0.2339249741407129\n",
      "2.650853003788194 0.25549584397168273\n",
      "2.6489096253167665 0.270976893433347\n",
      "2.646647159081315 0.26622396162353457\n",
      "2.6441716235705544 0.29032634054674256\n",
      "2.641999606660128 0.277388815305418\n",
      "2.639500610870818 0.27942282111785943\n",
      "2.6366369334561366 0.2568688860308034\n",
      "2.6336280851210696 0.24068967560402946\n",
      "2.6315984735773155 0.22066500272741701\n",
      "norms 25.86273155607123 37.457451330030075 37.03771721318353\n",
      "gradients 0.22066500272741701 0.6455344918288596 1.2607855080887151\n",
      "epoch 74 took 485.2s, optimizer loss=2.665, test mae=1.517\n",
      "2.6315984735773155 0.22066500272741701\n",
      "2.6298285380927435 0.20540339374440544\n",
      "2.6278660522120347 0.19262364463437082\n",
      "2.626461863828537 0.18079567272269229\n",
      "2.6252154095091105 0.1830878418618066\n",
      "2.623771407653941 0.16570742585811946\n",
      "2.6226606978907214 0.16889234467708142\n",
      "2.6214106672136364 0.17009302044256353\n",
      "2.619757379679512 0.18741309436201875\n",
      "2.6182470905224564 0.19104005932481563\n",
      "2.616574774962623 0.1971300187677597\n",
      "2.614399956659792 0.2078373956568451\n",
      "2.6122547371406584 0.20143523005343408\n",
      "2.610248478318703 0.20693549644289025\n",
      "2.6076630047264255 0.19487930402278014\n",
      "2.60487512337139 0.19451310830667817\n",
      "2.601953623916637 0.2589252848687907\n",
      "2.599606301077382 0.2409340303467697\n",
      "2.597145457770941 0.2246858941841006\n",
      "2.594442681662416 0.2140015486200331\n",
      "2.591758151444071 0.1881463887436066\n",
      "norms 25.912730408418835 37.44220113340243 37.061313360110965\n",
      "gradients 0.1881463887436066 0.6349458166729486 0.6684308146210465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75 took 480.4s, optimizer loss=2.632, test mae=1.504\n",
      "2.591758151444071 0.1881463887436066\n",
      "2.589480296244584 0.18234245899408177\n",
      "2.5875158119955937 0.16486066669328925\n",
      "2.585836803937396 0.15815292726274865\n",
      "2.584424345022028 0.15463016194349896\n",
      "2.5832963576802124 0.14719101077723337\n",
      "2.5822280124268375 0.14970829231345206\n",
      "2.5812200950633373 0.14788266419161591\n",
      "2.5802253696572133 0.14336160804343487\n",
      "2.579440361538121 0.14220808690505424\n",
      "2.5785200023251114 0.13850720057342336\n",
      "2.5774940228348693 0.13304290639989821\n",
      "2.5764301613238336 0.13610354349968048\n",
      "2.5752727877942534 0.1263607300279026\n",
      "2.5741752897626156 0.13057847939293968\n",
      "2.5730545217771366 0.12139414847527863\n",
      "2.5719489179037147 0.12380891846409837\n",
      "2.57085271533773 0.12184838598974011\n",
      "2.569612129105089 0.12687386014340482\n",
      "2.568576526144689 0.12360388427447996\n",
      "2.567493234049231 0.1283052405177475\n",
      "norms 26.012410216423735 37.43287880979582 37.12674462903542\n",
      "gradients 0.1283052405177475 0.6743188516789657 0.41407769417628354\n",
      "epoch 76 took 485.6s, optimizer loss=2.592, test mae=1.495\n",
      "2.567493234049231 0.1283052405177475\n",
      "2.566154022377848 0.13258655396684701\n",
      "2.5648707996110183 0.12435345612885752\n",
      "2.5636126097881133 0.1371070187381473\n",
      "2.562297341451113 0.13443147345849696\n",
      "2.5608792328678978 0.13964595044641548\n",
      "2.559292060922236 0.15432160397944628\n",
      "2.5575512828999987 0.14808057628102891\n",
      "2.5556921309712815 0.17494117216912847\n",
      "2.5538478562555946 0.17128713984545135\n",
      "2.5519674787938413 0.19156115032827856\n",
      "2.5500073147784748 0.19329381472434135\n",
      "2.547974379580285 0.20119804004546982\n",
      "2.545967747016658 0.2269445788598251\n",
      "2.5442747395880603 0.21480777320464606\n",
      "2.542669027874621 0.22738561273404076\n",
      "2.5408405460951444 0.20976145619965256\n",
      "2.539396244353008 0.20837692377168907\n",
      "2.5374889877924116 0.21287569325947786\n",
      "2.5346194831243634 0.19779791707461597\n",
      "2.5321546758453977 0.19005183833004524\n",
      "norms 26.00700710615059 37.423834407458244 37.11587490170419\n",
      "gradients 0.19005183833004524 0.6452500811265683 2.082718108750167\n",
      "epoch 77 took 481.2s, optimizer loss=2.567, test mae=1.48\n",
      "2.5321546758453977 0.19005183833004524\n",
      "2.5294341120977255 0.2072380841634321\n",
      "2.5261844657986265 0.18187769012327004\n",
      "2.5234009527385313 0.18469522684514916\n",
      "2.52106443909441 0.18066609180908103\n",
      "2.5186293571801754 0.18233682079618835\n",
      "2.516487052723831 0.17807247570387313\n",
      "2.5141214034114903 0.17419511873066326\n",
      "2.511214547522189 0.19950987255133748\n",
      "2.509163705640492 0.18743208899773023\n",
      "2.507495300295936 0.1861656515972332\n",
      "2.5052726568350185 0.1759654931633569\n",
      "2.5035244191021455 0.1619505906427091\n",
      "2.5016114286674362 0.15952437135280462\n",
      "2.4992619916680674 0.17106254159781886\n",
      "2.4972920152696734 0.16507490471500236\n",
      "2.494869498677083 0.15410589671433095\n",
      "2.4926502699346664 0.1539625637886995\n",
      "2.4903595397122484 0.18474373559868434\n",
      "2.4885761677844287 0.17994006087283637\n",
      "2.486859147842419 0.18195522593629931\n",
      "norms 26.063509385600934 37.41170489575306 37.14770684492645\n",
      "gradients 0.18195522593629931 0.6230551123999019 1.1202637332103391\n",
      "epoch 78 took 478.5s, optimizer loss=2.532, test mae=1.467\n",
      "2.486859147842419 0.18195522593629931\n",
      "2.4853943991376015 0.3000748459142914\n",
      "2.4840168853644036 0.26644685661338247\n",
      "2.4826239718163303 0.23406245916865695\n",
      "2.48128606256185 0.20497789832002608\n",
      "2.480048361274109 0.18100717001004807\n",
      "2.478917329106945 0.16124706641181533\n",
      "2.4778773749339535 0.14901728313751783\n",
      "2.476878003300434 0.1305837308962577\n",
      "2.4758714576105927 0.13858065569172787\n",
      "2.474964537398672 0.11969856072664357\n",
      "2.474367767909123 0.11891406153255991\n",
      "2.4737942569788034 0.1818256235496393\n",
      "2.4733516726627784 0.16947328119611588\n",
      "2.472815496326984 0.15637836598577035\n",
      "2.472170549309893 0.14236188500357902\n",
      "2.471439995493367 0.1284265409187668\n",
      "2.470656312491572 0.11652896136949063\n",
      "2.4698377188248783 0.10758501990385562\n",
      "2.4689944577312684 0.1026681801708965\n",
      "2.4681249492124357 0.1017287751929874\n",
      "norms 26.078150033742165 37.40578558532566 37.15199328836628\n",
      "gradients 0.1017287751929874 0.5679782405766841 0.5775121720508295\n",
      "epoch 79 took 484.9s, optimizer loss=2.487, test mae=1.463\n",
      "2.4681249492124357 0.1017287751929874\n",
      "2.46722280942766 0.10427495774390111\n",
      "2.4662662073550834 0.11019012557363947\n",
      "2.4652298082992345 0.11806400386169986\n",
      "2.464011694893725 0.17216520219974962\n",
      "2.46319498066713 0.16591415710402427\n",
      "2.462257803307419 0.16224262139656712\n",
      "2.46107979671173 0.1870168461502305\n",
      "2.4600517637195662 0.17898570196119248\n",
      "2.4590296171343775 0.17950397170821203\n",
      "2.457812967136817 0.17409326197518377\n",
      "2.4567251623034316 0.1702716992231121\n",
      "2.455777466350314 0.16743067259008051\n",
      "2.4547722140519443 0.1632023168072706\n",
      "2.453742007077309 0.15687958684201952\n",
      "2.4526651645010933 0.18418561442045372\n",
      "2.4518861198458546 0.17398781224604798\n",
      "2.450976866401777 0.15536824600077345\n",
      "2.4498921546192047 0.14962313566942118\n",
      "2.4486765437466045 0.13676834845858096\n",
      "2.4478056727728634 0.12682203099587738\n",
      "norms 26.12452135216866 37.400239151651235 37.1789456234825\n",
      "gradients 0.12682203099587738 0.5772796238385556 2.0066790872503963\n",
      "epoch 80 took 481.7s, optimizer loss=2.468, test mae=1.456\n",
      "2.4478056727728634 0.12682203099587738\n",
      "2.4468705711292063 0.130835423560706\n",
      "2.4458251214809197 0.11645291582297555\n",
      "2.4449605489237256 0.11994569017475741\n",
      "2.444027445717049 0.1193426349012229\n",
      "2.4432182829119826 0.1132828014347729\n",
      "2.4423710052207768 0.1646934478848971\n",
      "2.441799383279022 0.15637085973939613\n",
      "2.441155161799219 0.15801298948116008\n",
      "2.4402791810689086 0.12444890500176677\n",
      "2.439476395274691 0.1297480476035784\n",
      "2.4386188219624874 0.13338995992818614\n",
      "2.437472165404546 0.14181644083478992\n",
      "2.4360421195264506 0.15778317808740056\n",
      "2.4343917786473086 0.17167956103000795\n",
      "2.4331474738652026 0.17444196565998454\n",
      "2.4318951464619976 0.18416941377059318\n",
      "2.4303868212184065 0.17399783167482177\n",
      "2.4291795051176828 0.1774196662099717\n",
      "2.4276125450810904 0.18099862478844606\n",
      "2.425736224943287 0.17342343417035136\n",
      "norms 26.140837558622565 37.393764240691034 37.183945673648786\n",
      "gradients 0.17342343417035136 0.5376133795998347 1.0290732776729223\n",
      "epoch 81 took 493.0s, optimizer loss=2.448, test mae=1.442\n",
      "2.425736224943287 0.17342343417035136\n",
      "2.4239697238699316 0.17656359410832806\n",
      "2.42235180628834 0.16338253032850292\n",
      "2.4208190869830624 0.1661022551814309\n",
      "2.4193308055865046 0.14485132674603285\n",
      "2.4181295151057722 0.1383140341328322\n",
      "2.4169757233087044 0.17720112371115834\n",
      "2.4161767965942587 0.15849883582380186\n",
      "2.4154242789620035 0.14635356827744467\n",
      "2.4146286890754958 0.13689458108963384\n",
      "2.4138020272321135 0.13079565540111446\n",
      "2.412951984103373 0.12889445475805597\n",
      "2.4120769349210627 0.12511672645612612\n",
      "2.41111288403282 0.1658311532668349\n",
      "2.410460912161037 0.16064906226276185\n",
      "2.409765087366812 0.15596355660755154\n",
      "2.40892583288611 0.16098736101816483\n",
      "2.408088790713595 0.1509479951323924\n",
      "2.4073868318047262 0.14972682222422667\n",
      "2.4065654752875365 0.15234045508828084\n",
      "2.405907740891805 0.1433309144398901\n",
      "norms 26.190699965124423 37.388238165692805 37.21671330124222\n",
      "gradients 0.1433309144398901 0.5295173600550711 0.6329323860251751\n",
      "epoch 82 took 482.6s, optimizer loss=2.426, test mae=1.434\n",
      "2.405907740891805 0.1433309144398901\n",
      "2.405246934279358 0.1383647366844309\n",
      "2.4044212472955997 0.13328966342665813\n",
      "2.4035921314135975 0.13044537364882\n",
      "2.4027614417396133 0.14042144140417853\n",
      "2.4017658368670785 0.13487998336238433\n",
      "2.4008887134682912 0.14653608115146421\n",
      "2.4000970794027103 0.14361484026640386\n",
      "2.39923384434494 0.16618074983129558\n",
      "2.398351041242235 0.1582656778115638\n",
      "2.397543152367401 0.1663807970561854\n",
      "2.396593094926903 0.18752183451078958\n",
      "2.395865146905 0.18608941606139603\n",
      "2.3950069089202244 0.18393375317081004\n",
      "2.3937678389745574 0.19684334651033536\n",
      "2.3928976110592335 0.18629530665803212\n",
      "2.391890710768407 0.17581207205232877\n",
      "2.390666573716833 0.16039920562760515\n",
      "2.389263708553374 0.14684104451487165\n",
      "2.388025198783966 0.12973897187061248\n",
      "2.3869948407947557 0.12025673853770803\n",
      "norms 26.21463867132347 37.380755872495754 37.22512459556367\n",
      "gradients 0.12025673853770803 0.512489105369488 1.0353078311721795\n",
      "epoch 83 took 480.9s, optimizer loss=2.406, test mae=1.43\n",
      "2.3869948407947557 0.12025673853770803\n",
      "2.3859805451144793 0.10901957739704128\n",
      "2.385010243247206 0.10864039837797557\n",
      "2.384293004710236 0.10003418357864363\n",
      "2.383548692703896 0.11563717670438715\n",
      "2.3827993072585287 0.10549435145318428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3820660151621595 0.11767323875675477\n",
      "2.3814108588567553 0.11264451372693127\n",
      "2.380652869507844 0.11281539188882213\n",
      "2.3795977515267905 0.12258550049358163\n",
      "2.3785425414733177 0.11535059628659523\n",
      "2.377470856122356 0.11408024526670217\n",
      "2.376240863450831 0.11666797076786417\n",
      "2.3750925508906575 0.10628172945905091\n",
      "2.3740289281915423 0.10957872402622978\n",
      "2.3728555371561715 0.11334969206131858\n",
      "2.3719787185539825 0.11398926761168914\n",
      "2.370914227251 0.1325954713021348\n",
      "2.369720898849251 0.12423107723922996\n",
      "2.368615656245657 0.14656801207127118\n",
      "2.367500697389081 0.129186755303991\n",
      "norms 26.24118252893748 37.37486166943486 37.238959596204246\n",
      "gradients 0.129186755303991 0.47880206766109085 1.5623491908177323\n",
      "epoch 84 took 483.3s, optimizer loss=2.387, test mae=1.426\n",
      "2.367500697389081 0.129186755303991\n",
      "2.3665979959346504 0.1382212616768219\n",
      "2.3656902192737403 0.12680761097198995\n",
      "2.3648373439191843 0.13409713918694843\n",
      "2.3640180455966746 0.1276093890730262\n",
      "2.36337341931359 0.12880470316936077\n",
      "2.3627078687318006 0.129847183805806\n",
      "2.3618934675771683 0.14623446899178077\n",
      "2.3611262609808 0.14422598496802735\n",
      "2.360438153273311 0.1504590596904219\n",
      "2.359699890435065 0.1619593211567198\n",
      "2.3588766084576593 0.1560123101279685\n",
      "2.358280016978725 0.15972940692735907\n",
      "2.3575056009169857 0.1571229778849915\n",
      "2.3566031287677345 0.15553039077230693\n",
      "2.355396085259977 0.15550293694820724\n",
      "2.3538449587346144 0.143818596465114\n",
      "2.3521883980998655 0.15846075402059817\n",
      "2.350593134232211 0.14288992207322682\n",
      "2.349166470883972 0.14721254422008756\n",
      "2.3479277566632444 0.13713473586884087\n",
      "norms 26.234234524289356 37.36904979955185 37.22875026457048\n",
      "gradients 0.13713473586884087 0.4958848396741462 1.0940884992127884\n",
      "epoch 85 took 483.9s, optimizer loss=2.368, test mae=1.415\n",
      "2.3479277566632444 0.13713473586884087\n",
      "2.3468898503399673 0.132869503066213\n",
      "2.3458427774587762 0.12944186283657\n",
      "2.345056930572062 0.12856517475674592\n",
      "2.344337647101465 0.13225817784230381\n",
      "2.343675440752773 0.13491407253531765\n",
      "2.3430177085134845 0.13954521490849167\n",
      "2.3423090161786324 0.1445893762248459\n",
      "2.341533029342222 0.14778701114506004\n",
      "2.3406492997582347 0.15306546880925673\n",
      "2.339654808460715 0.1525254386048389\n",
      "2.3385372275270035 0.1543260496447666\n",
      "2.337278818721001 0.1492281363563013\n",
      "2.335975988615915 0.15025779940578057\n",
      "2.3343663985648004 0.1640349912407676\n",
      "2.3335243506887355 0.1512514894748442\n",
      "2.3325783358708305 0.13861422562993617\n",
      "2.331470580685285 0.12893631928656568\n",
      "2.3305561190724124 0.11424065774343334\n",
      "2.3297707391553635 0.10628584559382627\n",
      "2.3289211394943528 0.10389478012435836\n",
      "norms 26.265762927620443 37.36306308213269 37.245050428210604\n",
      "gradients 0.10389478012435836 0.5130210978684894 0.3659115579544474\n",
      "epoch 86 took 484.6s, optimizer loss=2.348, test mae=1.411\n",
      "2.3289211394943528 0.10389478012435836\n",
      "2.3280060642129863 0.09935969903715687\n",
      "2.327181696303247 0.11108036891463335\n",
      "2.326552742493573 0.11319920206959158\n",
      "2.3258883563816557 0.11605283212867451\n",
      "2.325146302000794 0.119082419449549\n",
      "2.3244594413094184 0.12272948100779914\n",
      "2.323697010727764 0.12691887195460158\n",
      "2.322947999463737 0.12605901042111178\n",
      "2.3221212398392237 0.1402712315319368\n",
      "2.321427256304187 0.13473992271623297\n",
      "2.320710506092434 0.13607586515511083\n",
      "2.3197087391342905 0.13055552253550204\n",
      "2.3188769096398825 0.12208930570093188\n",
      "2.3179832999078838 0.12278866324537029\n",
      "2.316862226133593 0.10925795159594416\n",
      "2.315459487600637 0.12316340446552564\n",
      "2.3143895872021534 0.1213873576967261\n",
      "2.313254193389845 0.13903955539632487\n",
      "2.3120757400998153 0.14072409090862847\n",
      "2.3109534624412467 0.14783129507636011\n",
      "norms 26.294206386745426 37.35715941031239 37.260561651020886\n",
      "gradients 0.14783129507636011 0.5085520072142296 0.4382575716741733\n",
      "epoch 87 took 483.6s, optimizer loss=2.329, test mae=1.409\n",
      "2.3109534624412467 0.14783129507636011\n",
      "2.3098303694805735 0.15109102918120387\n",
      "2.308875749213602 0.15031862275416027\n",
      "2.307793347583047 0.17404131935933884\n",
      "2.306771809216062 0.16514233611530205\n",
      "2.305830467938831 0.16464799590066376\n",
      "2.3048147777665173 0.1459994400818032\n",
      "2.30395354209916 0.1400415650250632\n",
      "2.3030860434084284 0.126540490893364\n",
      "2.3023345972821727 0.11752904496957034\n",
      "2.3015215182910573 0.10902171481652731\n",
      "2.3005632081424854 0.10330798955976832\n",
      "2.2995331293410852 0.10415955101818263\n",
      "2.2985871495805643 0.10325314598719516\n",
      "2.2976215278990386 0.11212138569136422\n",
      "2.296518168498746 0.1170566865550894\n",
      "2.2954194038457314 0.12672438815970324\n",
      "2.2942820757271387 0.17830828402067764\n",
      "2.2932122690203025 0.174862721709193\n",
      "2.2922254712865473 0.1780079146909624\n",
      "2.2909789770854947 0.17057229296451062\n",
      "norms 26.359952932020157 37.34938652234601 37.300732462550734\n",
      "gradients 0.17057229296451062 0.5144566491907143 1.013751492145724\n",
      "epoch 88 took 485.4s, optimizer loss=2.311, test mae=1.406\n",
      "2.2909789770854947 0.17057229296451062\n",
      "2.289973718153605 0.1695258407910344\n",
      "2.288750296316224 0.15940182428994343\n",
      "2.2872553168979803 0.17276045125808803\n",
      "2.2857958563662093 0.15206032196484373\n",
      "2.2847074667375193 0.14633368342451342\n",
      "2.2836365650111365 0.14254843411583315\n",
      "2.282312437338083 0.16124008275183144\n",
      "2.281305511123018 0.15289943937544623\n",
      "2.280269967064156 0.16711301060429995\n",
      "2.279045160685994 0.1581981385291874\n",
      "2.2779692606329895 0.17062373168440437\n",
      "2.2769957415910307 0.16848481641545837\n",
      "2.2759210713456186 0.18613515516736814\n",
      "2.2749513648362876 0.18138986457974518\n",
      "2.2739902556714746 0.179446105837843\n",
      "2.272856446480371 0.1744850695279728\n",
      "2.271987621281715 0.16522350275836553\n",
      "2.271257980503126 0.15840710698114735\n",
      "2.2704780055275906 0.15181215268340179\n",
      "2.2697752026156364 0.13979492565935467\n",
      "norms 26.422042099204035 37.34154831697727 37.33814207046541\n",
      "gradients 0.13979492565935467 0.5018423037474163 0.3426071981921084\n",
      "epoch 89 took 489.0s, optimizer loss=2.291, test mae=1.4\n",
      "2.2697752026156364 0.13979492565935467\n",
      "2.2690827549878043 0.144033057269267\n",
      "2.2684144101007586 0.12995277214012943\n",
      "2.2677824274498835 0.1326674899755109\n",
      "2.267081152146948 0.1260007949118343\n",
      "2.2664086627633524 0.11986580395269977\n",
      "2.265726776366521 0.13386118193648658\n",
      "2.2650491045516903 0.1300127514103594\n",
      "2.264382901612169 0.13872486448426344\n",
      "2.2636089509394517 0.14817347877193232\n",
      "2.2628821132679198 0.14843781025672229\n",
      "2.262066436605804 0.16482864657537374\n",
      "2.261298707186974 0.16492577632021002\n",
      "2.260516088899702 0.17857227926305733\n",
      "2.259550777108551 0.17093148957192722\n",
      "2.258906769417302 0.17118331983507054\n",
      "2.2581727423147275 0.17293339377462602\n",
      "2.2573126706353253 0.16237466618457455\n",
      "2.25647200593989 0.16471510712742674\n",
      "2.2557398463148375 0.15695255998706972\n",
      "2.2550703547989257 0.14874964693020915\n",
      "norms 26.407668961336405 37.33808831763587 37.32588747685449\n",
      "gradients 0.14874964693020915 0.5137140365865782 0.9580224340043699\n",
      "epoch 90 took 482.1s, optimizer loss=2.27, test mae=1.406\n",
      "2.2550703547989257 0.14874964693020915\n",
      "2.2542757711311445 0.15100265975969834\n",
      "2.2536056566915086 0.1453575186288266\n",
      "2.252910646874886 0.1390032196217284\n",
      "2.2520208894136924 0.17074902562689645\n",
      "2.2512709180723633 0.16149520127265687\n",
      "2.2504093411147785 0.1538415118007233\n",
      "2.249261397753702 0.15983938532560665\n",
      "2.2477137522982136 0.15083390259985463\n",
      "2.2466118080211555 0.14425450016311772\n",
      "2.2454050855632444 0.14463848146908675\n",
      "2.243921761051285 0.12639469957510183\n",
      "2.2424025753245624 0.12615576730708292\n",
      "2.240909386675813 0.12762880677057506\n",
      "2.2395595029249864 0.11957631830111035\n",
      "2.2382822772956583 0.12042502592500337\n",
      "2.236777618675308 0.19235628033272473\n",
      "2.235605624213915 0.18287255380089293\n",
      "2.2343351930116575 0.17434796212186876\n",
      "2.232896171230886 0.18162603956482762\n",
      "2.2312151325565805 0.16157377617141983\n",
      "norms 26.449496851991846 37.329314124349374 37.34655357108364\n",
      "gradients 0.16157377617141983 0.5358298975084347 1.1123823641423403\n",
      "epoch 91 took 486.8s, optimizer loss=2.255, test mae=1.392\n",
      "2.2312151325565805 0.16157377617141983\n",
      "2.2299765354482464 0.16248898808257708\n",
      "2.2288124947769936 0.16130426057005484\n",
      "2.2274947456833085 0.15830243631304633\n",
      "2.22635356455827 0.16002185152933307\n",
      "2.2251606859381092 0.15392158427748653\n",
      "2.2238390565461357 0.15480094433128985\n",
      "2.222535236801291 0.14378601786375136\n",
      "2.2211151354317753 0.1554657366678797\n",
      "2.219886263646716 0.1409005229291487\n",
      "2.21861570893506 0.16016395558860297\n",
      "2.217576713758054 0.14661169820383274\n",
      "2.2165111365174655 0.14487362909908053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.215243714072745 0.13296040548415888\n",
      "2.2140785624195303 0.1308367179416977\n",
      "2.2128617535687187 0.12980880685632495\n",
      "2.2113578342001468 0.13130232280637033\n",
      "2.209405203975395 0.13880643226774578\n",
      "2.2077826098397733 0.14551673263381767\n",
      "2.2062283558206013 0.14280334867546074\n",
      "2.2047759048467612 0.14564072005069376\n",
      "norms 26.48042348363259 37.32306961022704 37.36327525637849\n",
      "gradients 0.14564072005069376 0.5235316727092955 0.5341845229341435\n",
      "epoch 92 took 477.8s, optimizer loss=2.231, test mae=1.388\n",
      "2.2047759048467612 0.14564072005069376\n",
      "2.203466453368253 0.14269979385195708\n",
      "2.202107495140859 0.14298786967431726\n",
      "2.20088950716906 0.1372197331521122\n",
      "2.199855662598079 0.1769919297492101\n",
      "2.199048968778963 0.1598277094218728\n",
      "2.1982977025953283 0.14435466405918118\n",
      "2.1975414942474414 0.1295949895358128\n",
      "2.1967925337555885 0.1170133128473896\n",
      "2.196101959844727 0.10822851186438295\n",
      "2.195506622236475 0.10260769707607886\n",
      "2.19494104390109 0.10199207438869985\n",
      "2.1943280714864573 0.10570232792904516\n",
      "2.193691493098641 0.10660489241211735\n",
      "2.1931005168308784 0.11492849072787141\n",
      "2.192465017428464 0.11789034651122271\n",
      "2.191750828119129 0.12922832524371056\n",
      "2.190998384835385 0.1306555793659883\n",
      "2.1902008028167987 0.14170261389984234\n",
      "2.189294107193212 0.14095368482789516\n",
      "2.1885467946876545 0.1406252696422321\n",
      "norms 26.53272909967836 37.316691139004014 37.391273414799116\n",
      "gradients 0.1406252696422321 0.5498631660134857 0.709792939040794\n",
      "epoch 93 took 482.8s, optimizer loss=2.205, test mae=1.383\n",
      "2.1885467946876545 0.1406252696422321\n",
      "2.187746213455474 0.1419368668452996\n",
      "2.1867852372570065 0.1391238764105125\n",
      "2.1858148075796016 0.132141107293361\n",
      "2.1849138346828973 0.13031482369081004\n",
      "2.1839101447539506 0.12232009791381102\n",
      "2.1829369376096155 0.11879048680307758\n",
      "2.181911400088045 0.15927439454069958\n",
      "2.1811764266652456 0.14405626242346256\n",
      "2.180454269061527 0.1344533335336038\n",
      "2.1796508891612203 0.1263680824128669\n",
      "2.178788115867396 0.12324267262666927\n",
      "2.1778414374287958 0.1322900212071738\n",
      "2.177076059095378 0.12992206982539137\n",
      "2.1763252561976496 0.1337825346194778\n",
      "2.1753545883280596 0.13761908939564316\n",
      "2.174610451381992 0.1362771110327253\n",
      "2.173815617334518 0.136326124919653\n",
      "2.172739766942404 0.14355024516045056\n",
      "2.171776337463442 0.13388969757601163\n",
      "2.17087849897775 0.12951760099896856\n",
      "norms 26.56949215901258 37.3116646205159 37.41287874741362\n",
      "gradients 0.12951760099896856 0.5103085198551229 0.9184020074465234\n",
      "epoch 94 took 480.9s, optimizer loss=2.189, test mae=1.376\n",
      "2.17087849897775 0.12951760099896856\n",
      "2.169760253390482 0.13104163127956717\n",
      "2.1686692821417113 0.12169943404255273\n",
      "2.1675596087886344 0.11803344022843208\n",
      "2.1663485135204867 0.11199458689169867\n",
      "2.165071971072193 0.11490909126038154\n",
      "2.1638607009007775 0.12273555709291818\n",
      "2.1626453039905966 0.12648828128188536\n",
      "2.161532187388694 0.14264506644277875\n",
      "2.160593598286982 0.14939896964815236\n",
      "2.1597924854121304 0.15793909795197855\n",
      "2.15898820666036 0.16399348120784119\n",
      "2.1579574213616435 0.18017960695884128\n",
      "2.156932871928488 0.1810399768785701\n",
      "2.1558009015162596 0.19968515698347158\n",
      "2.1548983836996327 0.19497410379001312\n",
      "2.1538710016672304 0.1895941765873116\n",
      "2.152525345306994 0.17780412894683148\n",
      "2.151145830987183 0.16616400584491933\n",
      "2.149801854974169 0.15516158490245285\n",
      "2.1481669566498476 0.16104244127730588\n",
      "norms 26.64432650384223 37.30659104968107 37.46010374967671\n",
      "gradients 0.16104244127730588 0.540232991807648 0.7730126461729531\n",
      "epoch 95 took 483.7s, optimizer loss=2.171, test mae=1.372\n",
      "2.1481669566498476 0.16104244127730588\n",
      "2.146798228244374 0.15008902651693187\n",
      "2.145451213919222 0.14335722541087534\n",
      "2.1436577698697543 0.18602377948214346\n",
      "2.142617993183065 0.16958980259466838\n",
      "2.14158923223916 0.1658904337839061\n",
      "2.1403477788967074 0.15918235048253065\n",
      "2.1388579155666534 0.1504446505273053\n",
      "2.137468876073993 0.15801929938053114\n",
      "2.1362769807751443 0.14749660765076045\n",
      "2.1351383980506187 0.15075468859750696\n",
      "2.133956639673927 0.13891884129832605\n",
      "2.132835623250065 0.14831143016349807\n",
      "2.131825610572 0.13912930470888996\n",
      "2.130795543338006 0.1493533725430276\n",
      "2.1297231325804744 0.14349866643500406\n",
      "2.1287829284708613 0.1381976754531328\n",
      "2.127932740180762 0.14117706122025953\n",
      "2.1271346125868673 0.14013183255008\n",
      "2.1263352112955713 0.13983527508128213\n",
      "2.1256505603268616 0.14409232393849816\n",
      "norms 26.69307592273136 37.30129868377785 37.48863532070085\n",
      "gradients 0.14409232393849816 0.5335154533661727 0.5022191288731352\n",
      "epoch 96 took 483.4s, optimizer loss=2.148, test mae=1.362\n",
      "2.1256505603268616 0.14409232393849816\n",
      "2.1249404279441486 0.14650850697861567\n",
      "2.124329728310731 0.14450376521151773\n",
      "2.123740027899046 0.1512314264243143\n",
      "2.122960962711913 0.1483707497869424\n",
      "2.1224557535310953 0.14377364153662675\n",
      "2.121900308110742 0.1431910479937563\n",
      "2.1211657271818023 0.14047555878670123\n",
      "2.1202612267215315 0.13494326690899797\n",
      "2.1192710275910867 0.16893196430490623\n",
      "2.1184117672267173 0.15694328537976396\n",
      "2.1175603219824017 0.14977082510186204\n",
      "2.116480198707854 0.14830081364225103\n",
      "2.1156395263705554 0.14420642354854832\n",
      "2.114829578928004 0.13945221633391214\n",
      "2.1138982830754207 0.15343452593917634\n",
      "2.1131684852049206 0.15057567766180824\n",
      "2.1123016754381885 0.1403452341276072\n",
      "2.1111420478629515 0.16659434282673238\n",
      "2.11022380625613 0.15447568669423498\n",
      "2.109355010089267 0.1595973390569203\n",
      "norms 26.701237134078173 37.29790864032798 37.491001007865925\n",
      "gradients 0.1595973390569203 0.5037821521898889 1.4719082069842537\n",
      "epoch 97 took 484.1s, optimizer loss=2.126, test mae=1.356\n",
      "2.109355010089267 0.1595973390569203\n",
      "2.1083466619185316 0.15260092033123215\n",
      "2.1073905093339373 0.15547662484975083\n",
      "2.1063825305960697 0.16594991245965224\n",
      "2.1052397485377647 0.16424692249062448\n",
      "2.1040363587569506 0.19254146455500246\n",
      "2.102448472500621 0.1630583189498275\n",
      "2.101430817830624 0.17174550346445958\n",
      "2.1004223547513186 0.17368596123119084\n",
      "2.0991261972225677 0.1861646937637079\n",
      "2.0977992676026544 0.18347359390384232\n",
      "2.0965944652728 0.18302403808862197\n",
      "2.095254123400797 0.1704303598521779\n",
      "2.0941911489154017 0.16034401437461074\n",
      "2.0931451972942057 0.1544422734521321\n",
      "2.0919013626632794 0.14217846948178287\n",
      "2.0907254341749977 0.13793165570510996\n",
      "2.0895311966172345 0.1348243726972705\n",
      "2.0882225646456622 0.12517685898769498\n",
      "2.087061744453068 0.13148805699325541\n",
      "2.086036368493053 0.12638161443988777\n",
      "norms 26.730602489201324 37.29209592168192 37.502413871153664\n",
      "gradients 0.12638161443988777 0.5367746124573283 0.631529753152959\n",
      "epoch 98 took 483.4s, optimizer loss=2.109, test mae=1.354\n",
      "2.086036368493053 0.12638161443988777\n",
      "2.0851381758695346 0.12526091456278543\n",
      "2.084257040958237 0.12735852948358817\n",
      "2.083405264520156 0.12408626399381968\n",
      "2.0825590096118773 0.12905436399506404\n",
      "2.081700501467638 0.1250555744662786\n",
      "2.0807956779044865 0.13629028532781934\n",
      "2.07988162610436 0.1335102598805404\n",
      "2.0791671083769443 0.1316726416173671\n",
      "2.0783189644882865 0.13396097747361047\n",
      "2.0773436736808586 0.12905450978462693\n",
      "2.0764844060752297 0.12981335886122566\n",
      "2.0756278127550907 0.135037291981658\n",
      "2.074933282251195 0.1340023112517465\n",
      "2.074201883915066 0.13797577889476023\n",
      "2.0733508648896373 0.17881995750139082\n",
      "2.0726740452511176 0.17420607015341089\n",
      "2.0719791607792564 0.16967056972804456\n",
      "2.0711699977471496 0.16507652584460325\n",
      "2.0702514138799373 0.15981122207058396\n",
      "2.0692305899244694 0.16588397702534421\n",
      "norms 26.7508694977234 37.2892016971284 37.512577148190395\n",
      "gradients 0.16588397702534421 0.5304081614283649 0.9964971411374617\n",
      "epoch 99 took 483.9s, optimizer loss=2.086, test mae=1.349\n",
      "2.0692305899244694 0.16588397702534421\n",
      "2.0684213876573163 0.15978580857925226\n",
      "2.0675892388501023 0.15603820554692507\n",
      "2.0665516884166046 0.15645181770801708\n",
      "2.0656595318379245 0.14711298773411857\n",
      "2.0647402808293576 0.14634848470319403\n",
      "2.0636768020004284 0.12162002646815756\n",
      "2.0627530971628687 0.12349635847250309\n",
      "2.061858084984941 0.11405952013277268\n",
      "2.0609436879930447 0.11347368994701279\n",
      "2.060004648981123 0.10711510622431238\n",
      "2.0590083140513444 0.11002961866140666\n",
      "2.0581576598672315 0.106137863137202\n",
      "2.057396362985134 0.10782672539601802\n",
      "2.0566163612438397 0.12584294223966938\n",
      "2.0559233980321383 0.11882278949770154\n",
      "2.0553119135065785 0.12687782870378578\n",
      "2.054539934069491 0.1147236023915119\n",
      "2.0537676153971747 0.11415424483858318\n",
      "2.052878647545748 0.14039603154614855\n",
      "2.05223537004439 0.13243312545344416\n",
      "norms 26.78213819217858 37.287578042566224 37.52915280088726\n",
      "gradients 0.13243312545344416 0.4365203326169491 0.6480849763980929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 took 487.4s, optimizer loss=2.069, test mae=1.342\n",
      "2.05223537004439 0.13243312545344416\n",
      "2.051512281771075 0.13338871099249075\n",
      "2.0505138913601844 0.1405275109620903\n",
      "2.049477025153323 0.12902254978803332\n",
      "2.04850467825029 0.12670877761096308\n",
      "2.04743955008104 0.16945999565021133\n",
      "2.0465416975633652 0.15838285076774944\n",
      "2.0455233453995594 0.1445389173808285\n",
      "2.0444222152149467 0.13095023260206515\n",
      "2.043285769633535 0.1382349720450163\n",
      "2.042161419782703 0.1261287681346581\n",
      "2.0413409046085698 0.12226511055741633\n",
      "2.040557895483917 0.12296727667683623\n",
      "2.0398111731955133 0.11776494915007495\n",
      "2.039110752381793 0.12084133096251924\n",
      "2.03841583525158 0.11654038464001139\n",
      "2.0377397551361196 0.11960691062883534\n",
      "2.0370784851836583 0.12286988983094174\n",
      "2.0364312247421426 0.12026378663117848\n",
      "2.0358008179362996 0.14346675161416408\n",
      "2.035311478413704 0.13977499488483452\n",
      "norms 26.82348764940864 37.28411987162352 37.554207300259556\n",
      "gradients 0.13977499488483452 0.4808981372246712 0.40760056357922736\n",
      "epoch 101 took 485.4s, optimizer loss=2.052, test mae=1.333\n",
      "2.035311478413704 0.13977499488483452\n",
      "2.0348145431183946 0.13813140289081535\n",
      "2.034176306627484 0.1310419324715792\n",
      "2.0333678244562097 0.12887095478062208\n",
      "2.032431655830153 0.14066616509751778\n",
      "2.031892402467902 0.13155981702612196\n",
      "2.0312936152938597 0.12101364396926788\n",
      "2.030601347673492 0.10396997608281702\n",
      "2.0298573240146602 0.10099389439440025\n",
      "2.0291036710609296 0.08074346205731406\n",
      "2.028559497207463 0.07664322695348803\n",
      "2.028038812560852 0.0827567990846973\n",
      "2.0274827016457144 0.08453268892764351\n",
      "2.0270228054470074 0.09129142733501057\n",
      "2.026508475019752 0.10380170171723826\n",
      "2.02610845317157 0.10761953773098719\n",
      "2.0256980286774904 0.11325246483581473\n",
      "2.025159274026768 0.11880679058593513\n",
      "2.0244807546284123 0.12285388318518527\n",
      "2.0237246804206146 0.14929457014262618\n",
      "2.0230865089266006 0.14832706315453045\n",
      "norms 26.838707251069795 37.28248543195768 37.5608862380173\n",
      "gradients 0.14832706315453045 0.46775448987529644 1.0329197030072754\n",
      "epoch 102 took 483.0s, optimizer loss=2.035, test mae=1.325\n",
      "2.0230865089266006 0.14832706315453045\n",
      "2.0223956076933485 0.14523892942163583\n",
      "2.021519809290378 0.14808152279903192\n",
      "2.020546952902104 0.13732252468091233\n",
      "2.0197443348007 0.13512325496618038\n",
      "2.0189008080684108 0.12892742189162934\n",
      "2.018118173722455 0.1295004013297102\n",
      "2.0174198052844727 0.1244571895799095\n",
      "2.0167934030321306 0.1282270474331815\n",
      "2.0161713232757488 0.1311788129135272\n",
      "2.0154944378593904 0.13624930935455448\n",
      "2.0148534328339407 0.14380043230358602\n",
      "2.0142053026333286 0.14733199113365786\n",
      "2.013536698792254 0.15182461708913136\n",
      "2.012808055600138 0.15529993860970176\n",
      "2.0121530411404835 0.1542362100394117\n",
      "2.011420336950401 0.15272542543451242\n",
      "2.0105731608051918 0.14534637528074934\n",
      "2.0096992501433193 0.13778619395193956\n",
      "2.008740065951651 0.1252190384842454\n",
      "2.0078230711595397 0.11473893149967876\n",
      "norms 26.892633302032163 37.28028154398573 37.595430512550706\n",
      "gradients 0.11473893149967876 0.4641556185170828 0.380934601819002\n",
      "epoch 103 took 483.5s, optimizer loss=2.023, test mae=1.316\n",
      "2.0078230711595397 0.11473893149967876\n",
      "2.0068996157530683 0.10775309940812149\n",
      "2.0058334190287965 0.09560818112491527\n",
      "2.0049065177800465 0.09105611688398443\n",
      "2.003997441288016 0.11788777245712055\n",
      "2.0033584708120755 0.1045447901877027\n",
      "2.002768158737728 0.102501516736891\n",
      "2.0021515269550503 0.10021517398521342\n",
      "2.0015128107610964 0.100200034480224\n",
      "2.0008733228589777 0.11200886416270146\n",
      "2.0000948016399676 0.11212510302652205\n",
      "1.9996465827935523 0.11194423254687809\n",
      "1.999215967226407 0.11588053964673944\n",
      "1.9986959296022564 0.11405910568221438\n",
      "1.9981953473880265 0.1193406403556261\n",
      "1.9977519997437239 0.12425799541974085\n",
      "1.997214409591135 0.13105515592087638\n",
      "1.9967728081811598 0.13355953497933024\n",
      "1.996319858162419 0.13682211337830058\n",
      "1.9957016994005974 0.14108922337493932\n",
      "1.9949890193224207 0.14486637198592883\n",
      "norms 26.902010345270675 37.27723842345605 37.59833880323519\n",
      "gradients 0.14486637198592883 0.47671053278723197 0.5207053968460852\n",
      "epoch 104 took 489.9s, optimizer loss=2.008, test mae=1.311\n",
      "1.9949890193224207 0.14486637198592883\n",
      "1.994265596778064 0.14984252249701718\n",
      "1.9934970841729542 0.15061890222826022\n",
      "1.9927974750622535 0.15216319732914838\n",
      "1.9920152643642148 0.15158157744917491\n",
      "1.9911289553955598 0.14645514470541068\n",
      "1.9901614148634856 0.15557193818736553\n",
      "1.9893446639702923 0.14623296817199116\n",
      "1.9885686552948396 0.14555987474360874\n",
      "1.987641256655142 0.14698654479197854\n",
      "1.9869423361788527 0.138162299399772\n",
      "1.9862505465655929 0.13609155933238548\n",
      "1.9853443714653911 0.13834129416851615\n",
      "1.9844460542402607 0.13709258479403885\n",
      "1.9833496016226757 0.13232105731536903\n",
      "1.9823090587750105 0.1332624614597435\n",
      "1.9813697916812887 0.13160552126005057\n",
      "1.9804163960709538 0.12772518904034408\n",
      "1.9794153312397387 0.13180856503920563\n",
      "1.9786511106034477 0.12434311132758455\n",
      "1.977852955827514 0.1179174638519078\n",
      "norms 26.949043963663332 37.275837240555894 37.62648534102901\n",
      "gradients 0.1179174638519078 0.4229953833058606 1.4505154471179245\n",
      "epoch 105 took 485.4s, optimizer loss=1.995, test mae=1.301\n",
      "1.977852955827514 0.1179174638519078\n",
      "1.9769598321982202 0.11282595355655581\n",
      "1.9762122804325533 0.10261956681559686\n",
      "1.9755698371769146 0.10514428197635384\n",
      "1.9749051348308204 0.08892438953971422\n",
      "1.974227870775222 0.10694502742032423\n",
      "1.9736129258801125 0.09863615126579021\n",
      "1.973068837053293 0.10561961818255641\n",
      "1.972536939696562 0.10530671045386703\n",
      "1.9718683510320354 0.11594164662152709\n",
      "1.9710498962967542 0.11423203562212102\n",
      "1.9701779201749592 0.15503328951014722\n",
      "1.9694793110731865 0.15104222845872486\n",
      "1.9688377120564156 0.14576424775284624\n",
      "1.968039518814737 0.14005693252438198\n",
      "1.9672109704464198 0.13060467601568496\n",
      "1.9663096272020197 0.14023716610673023\n",
      "1.9654563459939998 0.12276534115990978\n",
      "1.964703830502215 0.11224843444654678\n",
      "1.9638578948043557 0.10003414615357333\n",
      "1.9630519800316184 0.08751108307919528\n",
      "norms 26.981390981859427 37.27469892513177 37.64536375381415\n",
      "gradients 0.08751108307919528 0.3978179256198942 0.9972399911568238\n",
      "epoch 106 took 481.7s, optimizer loss=1.978, test mae=1.294\n",
      "1.9630519800316184 0.08751108307919528\n",
      "1.962364601333063 0.08472293838327907\n",
      "1.9615412284396112 0.11247097021881687\n",
      "1.9608839458742042 0.11587917041257846\n",
      "1.9601537116809935 0.12647875166834524\n",
      "1.9595151105378683 0.12658147762582744\n",
      "1.9587875152479652 0.14908677157087596\n",
      "1.9580071663605163 0.14507728245462573\n",
      "1.9572238516035665 0.15220380129415176\n",
      "1.9561882690931591 0.1356294962167773\n",
      "1.955437024420922 0.1327285844876601\n",
      "1.9546223156744247 0.12282388457775693\n",
      "1.9537079362373604 0.11758961394897095\n",
      "1.9528270210852103 0.1082280308751828\n",
      "1.9520691829360108 0.10584850555413497\n",
      "1.9514156712463044 0.12027571175304246\n",
      "1.9509127023354806 0.1089514453666265\n",
      "1.950440382037926 0.10010235979744547\n",
      "1.9499610558604872 0.09455437758480144\n",
      "1.949482775625071 0.09063484504176206\n",
      "1.9490051505884993 0.0889974487725744\n",
      "norms 27.031692174853074 37.273018050733 37.67358413659178\n",
      "gradients 0.0889974487725744 0.434223011218642 1.1888050930701988\n",
      "epoch 107 took 482.6s, optimizer loss=1.963, test mae=1.287\n",
      "1.9490051505884993 0.0889974487725744\n",
      "1.948519205830554 0.08833705294662444\n",
      "1.9480124381569042 0.08852702213704838\n",
      "1.9474856227509323 0.09042267000669243\n",
      "1.947029803718248 0.08971982957177982\n",
      "1.946565835736328 0.09228035189139921\n",
      "1.9460286530999507 0.10154544382631762\n",
      "1.945557811974384 0.09781798180282543\n",
      "1.9450706595895741 0.10540574295811818\n",
      "1.944510257958968 0.09774179977265879\n",
      "1.9439492141646717 0.10258917589032102\n",
      "1.943445413683046 0.10980382840813847\n",
      "1.9430171586385265 0.11171728631244157\n",
      "1.9425269574306576 0.12041048525850874\n",
      "1.941919870801412 0.12420854138133681\n",
      "1.941252052453276 0.13115092170541526\n",
      "1.9408046585879821 0.13644182067082009\n",
      "1.940330949751612 0.137346652886048\n",
      "1.9397311346586856 0.13921946807872093\n",
      "1.938987837458043 0.1481412138007283\n",
      "1.9382682914155713 0.1426718947883644\n",
      "norms 27.075790701781496 37.27270392182979 37.700360317961106\n",
      "gradients 0.1426718947883644 0.46028471202003257 0.9026781512469989\n",
      "epoch 108 took 482.3s, optimizer loss=1.949, test mae=1.281\n",
      "1.9382682914155713 0.1426718947883644\n",
      "1.9375731861507952 0.1399351894852862\n",
      "1.936705869940373 0.14288185907803552\n",
      "1.936037773728312 0.13349103290728886\n",
      "1.9353093448949443 0.13131430720448575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.934451467310172 0.12311956382724339\n",
      "1.9336250936710226 0.12069280007213642\n",
      "1.9329129053847973 0.11729535518845452\n",
      "1.9319541685987314 0.1129877724363083\n",
      "1.9308765181207959 0.11014701610827114\n",
      "1.9309915207557038 0.3268939952410865\n",
      "1.930417068858284 0.17441794623364612\n",
      "1.929495884280226 0.16027544616388462\n",
      "1.9285538942499054 0.14209518486382872\n",
      "1.9276020176023945 0.13440778777526127\n",
      "1.9266610871122225 0.12133498732196392\n",
      "1.9259858218754609 0.11183978969644205\n",
      "1.925403865921533 0.11236833034340032\n",
      "1.9247815310331264 0.10582042384520504\n",
      "1.9243031944281428 0.10581385046021495\n",
      "1.9238313877854611 0.12045571430753482\n",
      "1.923389639840087 0.1207536454520985\n",
      "norms 27.110822054045624 37.27036310983312 37.719439390145766\n",
      "gradients 0.1207536454520985 0.41797080514334056 0.4944433173188953\n",
      "epoch 109 took 507.1s, optimizer loss=1.938, test mae=1.27\n",
      "1.923389639840087 0.1207536454520985\n",
      "1.9228904087249314 0.13003992976485856\n",
      "1.9222969801564223 0.12557968717217488\n",
      "1.921683233372145 0.14578889511478177\n",
      "1.921152551718586 0.1432756602678556\n",
      "1.9206361020789902 0.13839494674232614\n",
      "1.9200121821913496 0.1391856845453294\n",
      "1.9194530317116816 0.12942149774618325\n",
      "1.9189157792948843 0.12606049423332585\n",
      "1.918337339276777 0.11638844951108017\n",
      "1.9178294296141405 0.10916466048493166\n",
      "1.9173562360722576 0.10727869597712934\n",
      "1.9168549009290075 0.09389829072891313\n",
      "1.9164386672892781 0.09142599361209887\n",
      "1.9160113777462007 0.12936101177379336\n",
      "1.9156365459415587 0.11360285170524613\n",
      "1.9152658426371985 0.10990001283745991\n",
      "1.914797180876863 0.10069866713950965\n",
      "1.914236449019484 0.10001496735063745\n",
      "1.9136130330575396 0.10011119399500813\n",
      "1.912920865725955 0.10494300758273929\n",
      "norms 27.13220947785375 37.27025796267164 37.73096193202548\n",
      "gradients 0.10494300758273929 0.38746665613192915 1.085227276658543\n",
      "epoch 110 took 484.4s, optimizer loss=1.923, test mae=1.271\n",
      "1.912920865725955 0.10494300758273929\n",
      "1.9122030167835917 0.10556753636383547\n",
      "1.9113835928496525 0.12074095583328198\n",
      "1.910876835134727 0.12129034913994526\n",
      "1.9102659592436935 0.12270120643495719\n",
      "1.9095172554057211 0.11520561123078032\n",
      "1.9086921025839851 0.12363323066151195\n",
      "1.908098761396079 0.10999568016797716\n",
      "1.907545573304043 0.10411812617559646\n",
      "1.9069134005474384 0.11132057635269434\n",
      "1.9064368816183608 0.10164717921407111\n",
      "1.9059579262997028 0.0952497215729954\n",
      "1.9054091617923683 0.09556141259147817\n",
      "1.9049449245454668 0.09564649651952868\n",
      "1.9044726085746846 0.10154337378836355\n",
      "1.9039467957251577 0.10646677866687973\n",
      "1.9033892333763434 0.11420183407386648\n",
      "1.902761449016405 0.12883373456462532\n",
      "1.902213092472554 0.12883786277732073\n",
      "1.9015742050844824 0.17801029843087257\n",
      "1.9010515810894562 0.17357924099961194\n",
      "norms 27.177996206537102 37.270406123931465 37.75611407191091\n",
      "gradients 0.17357924099961194 0.40453792055506 2.8551416263752105\n",
      "epoch 111 took 484.2s, optimizer loss=1.913, test mae=1.269\n",
      "1.9010515810894562 0.17357924099961194\n",
      "1.90029283734219 0.17162995423442096\n",
      "1.8995183861571991 0.16273244903542747\n",
      "1.8986550354064335 0.14829710241367092\n",
      "1.8975618429015935 0.19191610106067636\n",
      "1.8967586437066424 0.16151295514573044\n",
      "1.895928884281343 0.1527475872130629\n",
      "1.8950482140373959 0.1161562316429097\n",
      "1.8942040323959795 0.11290023354205679\n",
      "1.8934581657705787 0.09466590915835606\n",
      "1.892800931480318 0.09427445762395732\n",
      "1.892150890212933 0.09288448162714308\n",
      "1.8914305480123568 0.10490742982739291\n",
      "1.8908150752693202 0.10653207416231482\n",
      "1.8903059202122718 0.11727112516545268\n",
      "1.88985735713787 0.11925261103741586\n",
      "1.889421225943552 0.12503757383443484\n",
      "1.8889733468472796 0.12088470203401208\n",
      "1.888544177624063 0.12449078528422108\n",
      "1.888109313664686 0.11211994705951908\n",
      "1.8876272185863663 0.1242830644085756\n",
      "norms 27.189650022185983 37.26768912991312 37.7605167801363\n",
      "gradients 0.1242830644085756 0.4547704525151371 2.2725379972109665\n",
      "epoch 112 took 485.3s, optimizer loss=1.901, test mae=1.266\n",
      "1.8876272185863663 0.1242830644085756\n",
      "1.8871785040632816 0.11094352844628436\n",
      "1.8867111490610875 0.1028838159149804\n",
      "1.8862340383462362 0.09767028994416\n",
      "1.885700399819104 0.09055133933112931\n",
      "1.885160543582194 0.09141490322741624\n",
      "1.8845302561209396 0.09438859870747997\n",
      "1.8838261019999882 0.09990060426009097\n",
      "1.8831705585719052 0.11372976899092498\n",
      "1.8824385776516972 0.12547681610427924\n",
      "1.8817292541233648 0.13652409217821113\n",
      "1.8809927573504615 0.1597267789370264\n",
      "1.8803194797269511 0.15981633103969514\n",
      "1.8796517422900565 0.17604441879063143\n",
      "1.8790621153007623 0.16980150329146784\n",
      "1.8784677274148842 0.1688009124461009\n",
      "1.8778083651768416 0.16208991502116699\n",
      "1.8772175688528911 0.15109797579348502\n",
      "1.87660371176606 0.15393134908202002\n",
      "1.875938304050217 0.1314683316204203\n",
      "1.87543124340669 0.12334752659769338\n",
      "norms 27.2355528502422 37.26794729046207 37.78466872031548\n",
      "gradients 0.12334752659769338 0.43488405453817225 1.5550785131613738\n",
      "epoch 113 took 486.9s, optimizer loss=1.888, test mae=1.258\n",
      "1.87543124340669 0.12334752659769338\n",
      "1.8748253148735121 0.11859134611203147\n",
      "1.8742465998304243 0.10973161404824658\n",
      "1.8735534155441038 0.10720199960370166\n",
      "1.8726843281159962 0.10265639768523299\n",
      "1.8717693713374044 0.1480695797304888\n",
      "1.871205840893798 0.14299342421051153\n",
      "1.8705389692101142 0.14069120754548947\n",
      "1.8697051731391972 0.1423495246737451\n",
      "1.869028673491674 0.139876609408092\n",
      "1.8683022244350145 0.14384414220891703\n",
      "1.8674402629733722 0.13429005916952066\n",
      "1.8665537460320385 0.14134610864533204\n",
      "1.8656843804161176 0.13171992586136108\n",
      "1.8648643613640434 0.13146488320515912\n",
      "1.8640665321793153 0.13733907087595335\n",
      "1.863268750396839 0.11743213707254872\n",
      "1.8625973491664818 0.12214259945533848\n",
      "1.8618863047809229 0.10535785425163761\n",
      "1.861304740371948 0.10267355102448675\n",
      "1.860704731119214 0.10629917611966992\n",
      "norms 27.25265617598846 37.26413832759088 37.79120216488922\n",
      "gradients 0.10629917611966992 0.4022993692893195 0.36308681148519495\n",
      "epoch 114 took 485.2s, optimizer loss=1.875, test mae=1.254\n",
      "1.860704731119214 0.10629917611966992\n",
      "1.8598874334207531 0.10293561582396246\n",
      "1.8593750406774578 0.10103651481467944\n",
      "1.8588029489230036 0.10244204358914794\n",
      "1.8581407634913123 0.1019079923201876\n",
      "1.8574381158794118 0.10061477279746185\n",
      "1.8569342224945027 0.10128429306140875\n",
      "1.8564487520163373 0.10169108942476848\n",
      "1.8559142826018258 0.10188785394232903\n",
      "1.8553887211124658 0.10269935598131684\n",
      "1.8549131182531775 0.09816532225959997\n",
      "1.8545042020120397 0.09979893914688374\n",
      "1.8540898493202302 0.09783653760850823\n",
      "1.8537550094912596 0.09543241959877635\n",
      "1.8534364475295284 0.0943054026433106\n",
      "1.8529649756244364 0.10726296970132082\n",
      "1.8525349521317123 0.10396506709963577\n",
      "1.8518825710223 0.11942499548989038\n",
      "1.8513524608112557 0.11483035235479876\n",
      "1.8506892160543824 0.11219355389038294\n",
      "1.8498522422094097 0.10725593045096866\n",
      "norms 27.284311554986502 37.26284964803175 37.80803429258196\n",
      "gradients 0.10725593045096866 0.3904847288809072 0.38456091665530406\n",
      "epoch 115 took 486.5s, optimizer loss=1.861, test mae=1.253\n",
      "1.8498522422094097 0.10725593045096866\n",
      "1.8489558476649741 0.11454204436412734\n",
      "1.8481791796011504 0.10648243576745223\n",
      "1.8473741605494978 0.11818284141468265\n",
      "1.8467122622179615 0.11681737740824534\n",
      "1.8460554210718814 0.11906358056715874\n",
      "1.8455230394360487 0.11775637776368275\n",
      "1.845009591150204 0.11970846252379289\n",
      "1.8443772980082758 0.11829425878615461\n",
      "1.843865043063312 0.11352983134078243\n",
      "1.8433967837294667 0.1134777798906739\n",
      "1.8428621003421595 0.10297185326987457\n",
      "1.8423486941713885 0.1007172933246469\n",
      "1.841893656776758 0.09687969057554797\n",
      "1.8414500092749349 0.09411724267138145\n",
      "1.8410570097816517 0.0961927773310558\n",
      "1.8406486673921232 0.09469034774085068\n",
      "1.840255303189651 0.10154345657377245\n",
      "1.8398645304650985 0.10301550555383063\n",
      "1.839401011278356 0.12276247639950559\n",
      "1.8388879382435643 0.12311353934363796\n",
      "norms 27.349794995534204 37.26502418094713 37.84941917365895\n",
      "gradients 0.12311353934363796 0.3998541498186304 0.47560515400786396\n",
      "epoch 116 took 484.4s, optimizer loss=1.85, test mae=1.251\n",
      "1.8388879382435643 0.12311353934363796\n",
      "1.8383729529144537 0.14182296680375342\n",
      "1.8378131547562866 0.1478389757201296\n",
      "1.837210075394946 0.1565550877690521\n",
      "1.8364506731394714 0.18557728526880174\n",
      "1.8358960707572531 0.18617919500429653\n",
      "1.8352201646644504 0.1838127831658349\n",
      "1.834395827257333 0.17766801176873642\n",
      "1.833526716508523 0.16793573373970583\n",
      "1.8326853539419108 0.15340738946132076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8318981519224218 0.13802684250663036\n",
      "1.8311641487408297 0.12051232012480391\n",
      "1.8304798370933595 0.10301560436789611\n",
      "1.829939793894795 0.08488552772852066\n",
      "1.8294845030746696 0.07270037487549502\n",
      "1.8289567894952206 0.059925321804821775\n",
      "1.8285421042323662 0.05082522746518922\n",
      "1.8280566040933344 0.07672715944854705\n",
      "1.8276121986637393 0.07731024977435218\n",
      "1.8270620032343738 0.07490963281128887\n",
      "1.8263357738744113 0.08106365219297122\n",
      "norms 27.351555832690924 37.260782914243855 37.846734798443684\n",
      "gradients 0.08106365219297122 0.39827541526214805 0.3694315930853178\n",
      "epoch 117 took 485.8s, optimizer loss=1.839, test mae=1.243\n",
      "1.8263357738744113 0.08106365219297122\n",
      "1.825506972556887 0.09392557198250555\n",
      "1.8248444030722617 0.09415402378713116\n",
      "1.8241815373464927 0.10065308891062844\n",
      "1.8234319955229448 0.09885605767370173\n",
      "1.8227821697855522 0.09989785429725159\n",
      "1.8221850696438464 0.09351545388273577\n",
      "1.8214635917138082 0.10304158970497793\n",
      "1.8208162330819806 0.09327031260570892\n",
      "1.820184288526463 0.14760074311997837\n",
      "1.819712489883804 0.13163913651606138\n",
      "1.8192691011261002 0.1218696885310889\n",
      "1.8187620380173406 0.1118189741697181\n",
      "1.8181990318747392 0.10809469034078473\n",
      "1.8175718359863797 0.11901763074728941\n",
      "1.8170765193931595 0.12033863102829702\n",
      "1.8165102926004324 0.12512619015614004\n",
      "1.8158451995851659 0.13206892389358835\n",
      "1.8150900141044344 0.18590839533482717\n",
      "1.814461072571705 0.1864227854896232\n",
      "1.813790403644748 0.185196331371361\n",
      "norms 27.40596849082561 37.26444061031806 37.88066194278036\n",
      "gradients 0.185196331371361 0.35573423575695895 3.7239299798775964\n",
      "epoch 118 took 488.8s, optimizer loss=1.826, test mae=1.24\n",
      "1.813790403644748 0.185196331371361\n",
      "1.8129849622674121 0.18263353814848152\n",
      "1.812084371241937 0.18109128367423208\n",
      "1.811087342878792 0.16623914162382822\n",
      "1.8103512324770956 0.15686525991791025\n",
      "1.8096384858633106 0.14636111045510095\n",
      "1.8088528937214319 0.1358434713481753\n",
      "1.808193718387626 0.12144628840511922\n",
      "1.8075795262480372 0.11137711831893284\n",
      "1.8068977686963053 0.10116669400936663\n",
      "1.8064329151333607 0.09056491979764816\n",
      "1.806006497678222 0.08588347954553596\n",
      "1.805521808259138 0.10512192020372785\n",
      "1.8051732138740086 0.10121900406116104\n",
      "1.804809905248615 0.10200459423044039\n",
      "1.804372661531527 0.10255615342790653\n",
      "1.8038457313599556 0.10772653268444192\n",
      "1.8033516826034668 0.1071512409979356\n",
      "1.802897854422602 0.11366113853547616\n",
      "1.8024357717394444 0.11450468667549892\n",
      "1.8020592229450145 0.11457960286893867\n",
      "norms 27.434691773925493 37.265420623336134 37.89592552924867\n",
      "gradients 0.11457960286893867 0.3883497925551328 0.6177241720720463\n",
      "epoch 119 took 484.4s, optimizer loss=1.814, test mae=1.236\n",
      "1.8020592229450145 0.11457960286893867\n",
      "1.8016570222170394 0.12019227905938425\n",
      "1.8012470863704126 0.11836738237177914\n",
      "1.8008623807953614 0.11783410501944838\n",
      "1.8004229296500691 0.11662376203359522\n",
      "1.7999768361771726 0.11470492539034456\n",
      "1.799579128261359 0.11079161877602647\n",
      "1.799149420331928 0.11432883311547477\n",
      "1.798625955352863 0.1037161501384609\n",
      "1.7981752192979712 0.10648003949214092\n",
      "1.797724527265608 0.10617866604475282\n",
      "1.7971518783931986 0.12333717829784034\n",
      "1.7967334762814968 0.1258949947163291\n",
      "1.7961816686253178 0.13138341158491168\n",
      "1.7954389770850145 0.1376755049684311\n",
      "1.7946377783963696 0.12453729034857965\n",
      "1.793878465577382 0.12731044920838594\n",
      "1.793333665200501 0.12312465921701166\n",
      "1.792728852573871 0.1274230709756738\n",
      "1.7921600101825788 0.11752874544742407\n",
      "1.7916668313032695 0.10973606095944667\n",
      "norms 27.45658703963336 37.26000457285341 37.90689898112198\n",
      "gradients 0.10973606095944667 0.3713612327839866 0.8446629324898802\n",
      "epoch 120 took 487.3s, optimizer loss=1.802, test mae=1.231\n",
      "1.7916668313032695 0.10973606095944667\n",
      "1.7910449996971596 0.10662973699750301\n",
      "1.790471015219644 0.100975099338284\n",
      "1.78981312219247 0.09449291142941649\n",
      "1.788964391653615 0.10042203397683241\n",
      "1.7882620230537905 0.09318642448794175\n",
      "1.787652461262034 0.09493939878131005\n",
      "1.7870389767275443 0.09601181803651601\n",
      "1.7864304403703226 0.09845940428670646\n",
      "1.7859245038097493 0.10105889228264145\n",
      "1.7853739271045626 0.11083689985476311\n",
      "1.7848229876778647 0.11536038246410019\n",
      "1.784333204650189 0.11427936680481808\n",
      "1.7838304640387952 0.11740018898067915\n",
      "1.783374758297351 0.1228095575589752\n",
      "1.7830036713904867 0.11667287001012829\n",
      "1.7826303028489325 0.12211592423970218\n",
      "1.7822617808372603 0.11555112469626391\n",
      "1.7818925588029506 0.11707659256071112\n",
      "1.7814951393825 0.11137306907228245\n",
      "1.7811089988888413 0.1079405537704129\n",
      "norms 27.513690161462055 37.26363418257612 37.94174219807182\n",
      "gradients 0.1079405537704129 0.3583789472140577 0.7774683578892491\n",
      "epoch 121 took 485.5s, optimizer loss=1.792, test mae=1.229\n",
      "1.7811089988888413 0.1079405537704129\n",
      "1.7807632410390044 0.10957761456080102\n",
      "1.780377417209366 0.09993031814867576\n",
      "1.780057645464671 0.09846091507015069\n",
      "1.7797108083257396 0.09597141951680789\n",
      "1.7792790127980094 0.09079415756610383\n",
      "1.7789355341281465 0.089294101868464\n",
      "1.778520646457002 0.10332036549326919\n",
      "1.7782069228075517 0.09800975626154934\n",
      "1.7778430323297327 0.09212860835189685\n",
      "1.7773767716729514 0.0848315193311053\n",
      "1.7768112534316787 0.08065753427309058\n",
      "1.7761373916748875 0.08193916335295724\n",
      "1.7756536310152558 0.07241487669202878\n",
      "1.7752262406109458 0.06854142894274745\n",
      "1.774736409496112 0.08270723868217097\n",
      "1.7743241084082002 0.0763044049018144\n",
      "1.773922219781165 0.0759192964705328\n",
      "1.7734369147120517 0.07354678043391397\n",
      "1.772935611262835 0.08176413452405339\n",
      "1.7724731151286228 0.08929639997442997\n",
      "norms 27.555170009398385 37.266308580265914 37.96995369822694\n",
      "gradients 0.08929639997442997 0.3862824511477425 0.5335894139813221\n",
      "epoch 122 took 484.3s, optimizer loss=1.781, test mae=1.226\n",
      "1.7724731151286228 0.08929639997442997\n",
      "1.7719465753277213 0.11246597244440984\n",
      "1.7714302161021103 0.12230501507223807\n",
      "1.7709324458885822 0.13829483847427682\n",
      "1.7703722480541173 0.15329633857954536\n",
      "1.7699056605367913 0.16117045331120394\n",
      "1.76936843849988 0.1773549620084752\n",
      "1.768736554503661 0.18031558559957642\n",
      "1.7681681703612262 0.182177961848832\n",
      "1.7675136892581145 0.18855219844518323\n",
      "1.7668188854112354 0.18077771728806177\n",
      "1.766129939634294 0.18471859911700242\n",
      "1.7653570658503586 0.16813928486006052\n",
      "1.7647505137042963 0.15180530074703402\n",
      "1.7642048836081707 0.1486253459775191\n",
      "1.7637068427359597 0.1320674368227412\n",
      "1.7632648518675529 0.12355750401604802\n",
      "1.7628239796023286 0.11692271325029031\n",
      "1.7624067937685335 0.10536504064200962\n",
      "1.7619871988173184 0.10375586706777934\n",
      "1.7614768370887368 0.09232155390468433\n",
      "norms 27.57420964519028 37.26452481230234 37.97722201459344\n",
      "gradients 0.09232155390468433 0.3844838615970108 0.3848051589947782\n",
      "epoch 123 took 485.8s, optimizer loss=1.772, test mae=1.223\n",
      "1.7614768370887368 0.09232155390468433\n",
      "1.7610181589329543 0.08923463951583872\n",
      "1.760591145901466 0.08901183757308637\n",
      "1.760079756296781 0.10016637127825999\n",
      "1.7597620557351379 0.0976845557351059\n",
      "1.7593403103947634 0.10357950910112383\n",
      "1.7586887613106348 0.09567498775569099\n",
      "1.7580578209768902 0.09505020938834977\n",
      "1.7573690918404752 0.0991936272429237\n",
      "1.7565985464510674 0.09960863716357335\n",
      "1.755886303652711 0.10033043069500526\n",
      "1.7551449652660223 0.10113928329828845\n",
      "1.7543671289949276 0.10883963704927958\n",
      "1.7535236909155836 0.10592551046039315\n",
      "1.7529248430385744 0.10176558225213943\n",
      "1.7523933566981111 0.10283674831750558\n",
      "1.7518614622937194 0.09396942876397191\n",
      "1.7513753110487125 0.09745608350414796\n",
      "1.750930981446065 0.08767322804848118\n",
      "1.7505693523999515 0.08565935229842046\n",
      "1.7502741925718523 0.0853223565436286\n",
      "norms 27.612280178096498 37.26535547620924 37.997806520814756\n",
      "gradients 0.0853223565436286 0.34801433453229336 0.6870416628568877\n",
      "epoch 124 took 485.8s, optimizer loss=1.761, test mae=1.218\n",
      "1.7502741925718523 0.0853223565436286\n",
      "1.7500377600400068 0.08542314916171542\n",
      "1.7498233880424328 0.08247053795841953\n",
      "1.7495809836225744 0.08552517312869236\n",
      "1.749312657320508 0.08147738818646816\n",
      "1.7490504339226314 0.08308642270103457\n",
      "1.7488029835725125 0.08155090002345314\n",
      "1.7485736496816635 0.07904387916753808\n",
      "1.748333207414508 0.09346170629072187\n",
      "1.74808808995421 0.08213652400736061\n",
      "1.7478742485048162 0.08363958801682728\n",
      "1.747524145613523 0.0935781872777592\n",
      "1.7472499324118163 0.09672890398924466\n",
      "1.746667825099967 0.11057439555226314\n",
      "1.7462247058688647 0.11193520670025393\n",
      "1.745572261489173 0.13925409968042532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7449234990310847 0.13220947090906973\n",
      "1.7441844380287617 0.1250455273909072\n",
      "1.7433526808555768 0.13927099051572303\n",
      "1.7426651600380663 0.12963747397448008\n",
      "1.7419603560010315 0.11624388020057648\n",
      "norms 27.616995755188466 37.25994884337105 37.99473459042443\n",
      "gradients 0.11624388020057648 0.34337252386914136 0.5599708071817051\n",
      "epoch 125 took 487.9s, optimizer loss=1.75, test mae=1.216\n",
      "1.7419603560010315 0.11624388020057648\n",
      "1.7412422292034515 0.1147078332712039\n",
      "1.7405211347740281 0.09677325017403715\n",
      "1.740034634690569 0.09281200097257307\n",
      "1.7396388037472492 0.0903268356595997\n",
      "1.739254159244234 0.08919353625532418\n",
      "1.7388614958236959 0.08820844989225733\n",
      "1.7384970490825205 0.08911138996659279\n",
      "1.7381289393165187 0.08792365881369628\n",
      "1.7377779036518302 0.08868905048915791\n",
      "1.737453570143346 0.09481754790288588\n",
      "1.7370705463031306 0.09466882477591039\n",
      "1.736759067340416 0.09611440207449054\n",
      "1.7364206586456266 0.10074041355815708\n",
      "1.736024616559863 0.10403348137434526\n",
      "1.735626365136707 0.10795986443561505\n",
      "1.735186346847463 0.12814532212112814\n",
      "1.7348299365328328 0.12739501786573035\n",
      "1.7344510200161884 0.12660397735929438\n",
      "1.7339511796315825 0.1304686198422904\n",
      "1.733505051400086 0.12863408257030967\n",
      "norms 27.64612088337261 37.2611693359989 38.01134913227158\n",
      "gradients 0.12863408257030967 0.3501144611640469 0.40829556046757987\n",
      "epoch 126 took 486.8s, optimizer loss=1.742, test mae=1.212\n",
      "1.733505051400086 0.12863408257030967\n",
      "1.7330473444303152 0.1255420117050715\n",
      "1.7325539115050446 0.12443622993004194\n",
      "1.7320224948231813 0.12102133685277786\n",
      "1.731447928784382 0.11787141064684073\n",
      "1.7308627304118767 0.13997472643639938\n",
      "1.7303718260845338 0.13033038390531132\n",
      "1.729869536237327 0.12446753769020738\n",
      "1.7292875348361385 0.11470940449862477\n",
      "1.7286573368058502 0.1065245795946875\n",
      "1.7279765076380473 0.1179546884776868\n",
      "1.727440555137429 0.10637960910327023\n",
      "1.7270139380482947 0.10579614576169469\n",
      "1.7265068708764817 0.10312196245619988\n",
      "1.7259417332282554 0.10246437280097856\n",
      "1.7253486409333991 0.11397574304772644\n",
      "1.7247110616624655 0.10758357503995945\n",
      "1.7241992488114108 0.11114057784712725\n",
      "1.7236122197227286 0.09905131646428002\n",
      "1.7231124771448665 0.09793665635339154\n",
      "1.7225742746776846 0.10301057432982898\n",
      "norms 27.693463302469645 37.260681558651925 38.03861163803735\n",
      "gradients 0.10301057432982898 0.36768466740260697 0.6099870966976293\n",
      "epoch 127 took 482.8s, optimizer loss=1.734, test mae=1.212\n",
      "1.7225742746776846 0.10301057432982898\n",
      "1.722064650075304 0.09941838613206896\n",
      "1.7215913580479758 0.09674663478947691\n",
      "1.721042377369281 0.09130618674189664\n",
      "1.7206176901953911 0.08962097786141006\n",
      "1.7201671522563793 0.08667390251370882\n",
      "1.719658167096498 0.07795666936071419\n",
      "1.7191487313616156 0.0916606453801885\n",
      "1.718683589191567 0.07153857519416242\n",
      "1.7182743815020933 0.08311523471807888\n",
      "1.7179150680604343 0.0758708126009442\n",
      "1.7175757851316111 0.07835521974218587\n",
      "1.7171743271618471 0.08733609811659578\n",
      "1.7167997901823036 0.08485365515811537\n",
      "1.7164124316943377 0.10902856628851713\n",
      "1.7160739799251723 0.10232705402145922\n",
      "1.7156912806725797 0.09270887794029262\n",
      "1.715245574845705 0.08657050940395683\n",
      "1.7147372503280665 0.08611390206452077\n",
      "1.714424706678543 0.07725415467782987\n",
      "1.7141139473676426 0.07070714174286481\n",
      "norms 27.714350887157046 37.26116848320843 38.049433462057664\n",
      "gradients 0.07070714174286481 0.3302990542365486 0.445287184410498\n",
      "epoch 128 took 485.4s, optimizer loss=1.723, test mae=1.21\n",
      "1.7141139473676426 0.07070714174286481\n",
      "1.7137849620484562 0.06775818652920239\n",
      "1.7134330449007737 0.0881899018931249\n",
      "1.7131943725246046 0.08562603319285507\n",
      "1.712972339601429 0.08537307499502111\n",
      "1.7127176589383026 0.09081559729774602\n",
      "1.7124640395480166 0.09043485586679209\n",
      "1.712238686837626 0.09516520372673848\n",
      "1.7119860461482272 0.09562968899559197\n",
      "1.711704373365718 0.10000412647410117\n",
      "1.7113707102348084 0.101087484913977\n",
      "1.7110515965601434 0.10116244135731221\n",
      "1.710710180526745 0.10671378251029172\n",
      "1.7102824750399137 0.099716442275184\n",
      "1.7099110865428087 0.10253623418385642\n",
      "1.7094891567978057 0.10151775658216575\n",
      "1.709068502207332 0.1028436743465142\n",
      "1.708608995036006 0.10732806299978752\n",
      "1.7081738691900616 0.10859230179729099\n",
      "1.7076729621556208 0.1100807372832221\n",
      "1.7071094107299734 0.11190341022196235\n",
      "norms 27.73208012486213 37.257905502451834 38.055347576062395\n",
      "gradients 0.11190341022196235 0.34283042955585946 0.9088562719951531\n",
      "epoch 129 took 484.5s, optimizer loss=1.714, test mae=1.208\n",
      "1.7071094107299734 0.11190341022196235\n",
      "1.7065491363969167 0.1253715648835445\n",
      "1.706103477664823 0.12364430257900594\n",
      "1.705644911121135 0.12112315013312432\n",
      "1.7051402617775853 0.11682948557790078\n",
      "1.7046747692875972 0.11002922454679721\n",
      "1.7042691020554341 0.10451079678617577\n",
      "1.703884836561212 0.09873639699353574\n",
      "1.703548150004076 0.09222831329929297\n",
      "1.7032277635094286 0.09048135645836\n",
      "1.7029346358593187 0.0854917955468202\n",
      "1.702634996052108 0.08308027522002098\n",
      "1.702263407712875 0.08025624747375823\n",
      "1.7019343070137025 0.07892001843919902\n",
      "1.7016153266843985 0.0850557195212882\n",
      "1.7013294545938817 0.08496239624760385\n",
      "1.7010437547194217 0.09215547455553527\n",
      "1.7006783902170755 0.09138136288777712\n",
      "1.7003418052864927 0.09721269585823578\n",
      "1.6999735292383809 0.09831959562330454\n",
      "1.699541350001727 0.10480295280632475\n",
      "norms 27.77212726132641 37.2618696145089 38.07909752697986\n",
      "gradients 0.10480295280632475 0.3234722990900554 0.7054602813157136\n",
      "epoch 130 took 485.9s, optimizer loss=1.707, test mae=1.207\n",
      "1.699541350001727 0.10480295280632475\n",
      "1.699118953258711 0.10452232356700787\n",
      "1.69866264355154 0.1078698907348531\n",
      "1.6982160108723865 0.10524043034497257\n",
      "1.6977963240327942 0.1033012032199599\n",
      "1.6973230240132964 0.09701286744604136\n",
      "1.6969523130351851 0.09214292492775114\n",
      "1.6965526930572172 0.08685438013839764\n",
      "1.6961833540202726 0.07910693022350893\n",
      "1.6958181980084888 0.07282162646643466\n",
      "1.6953862205391295 0.07324998994481559\n",
      "1.6950395241865794 0.06519935235294595\n",
      "1.694736307842598 0.060168606142040365\n",
      "1.6944297187664377 0.05895661711943058\n",
      "1.6941252752563591 0.05796367370295738\n",
      "1.6938181964035581 0.05659056011377743\n",
      "1.693557616256934 0.06127429934376999\n",
      "1.6933199660018692 0.060839018666439613\n",
      "1.6930284803733708 0.07514307685221035\n",
      "1.6927069381447997 0.07406574928438775\n",
      "1.6923677159316832 0.10569522280655892\n",
      "norms 27.803005123146065 37.26340451568876 38.09531918791525\n",
      "gradients 0.10569522280655892 0.3124985005441006 1.0834135935443885\n",
      "epoch 131 took 484.5s, optimizer loss=1.7, test mae=1.205\n",
      "1.6923677159316832 0.10569522280655892\n",
      "1.692057406459629 0.10133460088076075\n",
      "1.6917458014548752 0.10945215864881849\n",
      "1.6913793653644327 0.10655653401191054\n",
      "1.6909518915038155 0.11785863910383146\n",
      "1.6904480174059955 0.10438740631385134\n",
      "1.6900274892674303 0.10761897868947959\n",
      "1.6895655723909337 0.1130599498951765\n",
      "1.6890431609767502 0.10826722080320869\n",
      "1.6885421903860676 0.10962565467811035\n",
      "1.688072306372012 0.10720747425248611\n",
      "1.6875855123325123 0.1030752701414654\n",
      "1.6871217537282315 0.09771099034545563\n",
      "1.6866477996177816 0.09368431484378537\n",
      "1.6861194418743022 0.08470700172422162\n",
      "1.6857117821053005 0.07880557541928657\n",
      "1.6853998123523197 0.07489034476251727\n",
      "1.6850695046881032 0.07187532698797264\n",
      "1.684757892875016 0.07010131120104167\n",
      "1.6844505879313938 0.06752912000066706\n",
      "1.6841366473140253 0.06898282361738811\n",
      "norms 27.810398417089115 37.255738141835444 38.08997596059449\n",
      "gradients 0.06898282361738811 0.29203549157969927 0.44045282927975976\n",
      "epoch 132 took 484.7s, optimizer loss=1.692, test mae=1.202\n",
      "1.6841366473140253 0.06898282361738811\n",
      "1.6838585537593485 0.06767903730680414\n",
      "1.6835312100629112 0.07475795047460237\n",
      "1.683223033334477 0.07263426604966391\n",
      "1.6828867897180564 0.07497694807058343\n",
      "1.6825342731490947 0.07462200991627539\n",
      "1.682205306087331 0.06724369725525102\n",
      "1.6819165587462372 0.06597895655827747\n",
      "1.6816430129571662 0.06224951218857741\n",
      "1.6813452767378825 0.05957306317402954\n",
      "1.6810325397331638 0.058132781260922355\n",
      "1.680783677372473 0.05740256064184499\n",
      "1.6804777131478472 0.06001717264788006\n",
      "1.680143651715155 0.06216809150932457\n",
      "1.6797027047336857 0.07135188499584337\n",
      "1.6791838218093786 0.07602362045655615\n",
      "1.6786763868477959 0.0814663219143789\n",
      "1.6781766227495847 0.08103498316055754\n",
      "1.6777091527330843 0.0827201368066022\n",
      "1.6772110805239326 0.08305599238185335\n",
      "1.6767320795603866 0.07541847381205925\n",
      "norms 27.84301917714963 37.257666331966455 38.108043583753805\n",
      "gradients 0.07541847381205925 0.30645448930213637 0.5215648632828113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 133 took 483.7s, optimizer loss=1.684, test mae=1.204\n",
      "1.6767320795603866 0.07541847381205925\n",
      "1.6762819558902815 0.09971324585130945\n",
      "1.6759603685277815 0.088676734252305\n",
      "1.675699039354095 0.08186286448416077\n",
      "1.675411330419548 0.07417933956870376\n",
      "1.6750953376523852 0.06731361602004313\n",
      "1.6747833844312572 0.06483607363862959\n",
      "1.6744802133615189 0.061810937902794544\n",
      "1.6741872786772514 0.0641184012444459\n",
      "1.673924501954043 0.06741224247811838\n",
      "1.6737024120953923 0.06818124677705273\n",
      "1.6734534599485333 0.07764219764325271\n",
      "1.6731890583493885 0.07696257979797136\n",
      "1.6729458244613067 0.08377463647357229\n",
      "1.6726799798832204 0.0827837267070134\n",
      "1.6724216851824574 0.08521985886343758\n",
      "1.6721602669480324 0.08485452763564444\n",
      "1.6718434532135094 0.08432733123051893\n",
      "1.6715142316349043 0.08723988125806739\n",
      "1.6711923761710836 0.08331692994139758\n",
      "1.670879063342712 0.08880109488022515\n",
      "norms 27.847424070090018 37.25396363094311 38.10712580055482\n",
      "gradients 0.08880109488022515 0.2998933525622531 0.37731758223738304\n",
      "epoch 134 took 484.4s, optimizer loss=1.677, test mae=1.2\n",
      "1.670879063342712 0.08880109488022515\n",
      "1.6705897166110577 0.08615259341545678\n",
      "1.6702821378767105 0.10081935455736474\n",
      "1.6699968339978124 0.10044432618376205\n",
      "1.6696731119115862 0.1091339593858815\n",
      "1.6692327237174889 0.10965945858185104\n",
      "1.668866558345608 0.11293508768766208\n",
      "1.6684653589506515 0.11908995155030387\n",
      "1.6680163626905349 0.11522609436032268\n",
      "1.6675447322189778 0.11972802990331628\n",
      "1.6671123419472942 0.11742290009974433\n",
      "1.666751437700956 0.11086750450237214\n",
      "1.6663833393417757 0.11761712126233251\n",
      "1.6660156373813324 0.10273670706068001\n",
      "1.6657266189001605 0.09738906646370245\n",
      "1.6654782634249874 0.08836525662502757\n",
      "1.665245072061841 0.08081582819651036\n",
      "1.6650125803593017 0.07444746216723504\n",
      "1.664758505051281 0.0721015752700097\n",
      "1.6645786972935768 0.06841042040578385\n",
      "1.6643972009639265 0.06828987572082348\n",
      "norms 27.87613696683372 37.25204229676075 38.11904047724831\n",
      "gradients 0.06828987572082348 0.2871322763238565 0.22668410885335177\n",
      "epoch 135 took 486.7s, optimizer loss=1.671, test mae=1.199\n",
      "1.6643972009639265 0.06828987572082348\n",
      "1.6641609934084907 0.07232499595941926\n",
      "1.6639413046558977 0.07389660387752961\n",
      "1.6636969791057887 0.0804328260149582\n",
      "1.6634247060732865 0.08241824781204109\n",
      "1.663170851733792 0.08506413495510988\n",
      "1.662901959760794 0.08725695430726536\n",
      "1.662635679525604 0.08771932245019858\n",
      "1.6623701739566779 0.0867443115017933\n",
      "1.662091675761195 0.08499383286308498\n",
      "1.661802075165985 0.0823313670658129\n",
      "1.6615155243816992 0.07642027209271797\n",
      "1.661186365421092 0.08219125484741838\n",
      "1.6609856837397703 0.07725302888002718\n",
      "1.6607889363401502 0.07360258861447919\n",
      "1.6605191476101824 0.08558894943162158\n",
      "1.6602966771983896 0.08098046328703033\n",
      "1.6600407286468575 0.08011384561021208\n",
      "1.6597264569906063 0.08548967568107799\n",
      "1.6593516847066732 0.07917751854337783\n",
      "1.659040297878127 0.08192692933181053\n",
      "norms 27.918945419010875 37.25526914791179 38.145477131057135\n",
      "gradients 0.08192692933181053 0.289273387855329 0.71102955775986\n",
      "epoch 136 took 484.3s, optimizer loss=1.664, test mae=1.203\n",
      "1.659040297878127 0.08192692933181053\n",
      "1.6586966505545138 0.08005958737058116\n",
      "1.6583302891428322 0.08204237361001428\n",
      "1.657954846069589 0.0789865177398732\n",
      "1.6576031918481737 0.0786875735103487\n",
      "1.657276734940955 0.07702816697753219\n",
      "1.656879971573296 0.084396210133825\n",
      "1.6565691596744543 0.08038736971691046\n",
      "1.6562473686616281 0.0834376344750378\n",
      "1.6558810270272972 0.0777841478951984\n",
      "1.6555455286310106 0.0771622323548993\n",
      "1.6552276387009794 0.07629311540967089\n",
      "1.654886062209095 0.07904374828637907\n",
      "1.6546029033044443 0.07695037009468714\n",
      "1.6543426204809561 0.08379709220256369\n",
      "1.6540393746261972 0.07871975181473266\n",
      "1.653787466077825 0.08212947126200307\n",
      "1.6535335051587616 0.08757725576954793\n",
      "1.6532590523453603 0.08833864475303907\n",
      "1.6529653116147038 0.09585259335814533\n",
      "1.6526510487205954 0.10210069372968267\n",
      "norms 27.944262752927095 37.25421091603416 38.15703792664228\n",
      "gradients 0.10210069372968267 0.31373263377029514 0.28574573014712584\n",
      "epoch 137 took 485.2s, optimizer loss=1.659, test mae=1.2\n",
      "1.6526510487205954 0.10210069372968267\n",
      "1.652357632499831 0.1025134422139762\n",
      "1.6520188851597681 0.11412807717414981\n",
      "1.6516509415135427 0.11336243922271291\n",
      "1.6512528241682125 0.1200542911975743\n",
      "1.6507703119589572 0.1182358582982479\n",
      "1.6503855996587342 0.11521631206777115\n",
      "1.649930106823515 0.11800666214966075\n",
      "1.6494786052903494 0.11029604897637776\n",
      "1.6489915614675141 0.1167526993556625\n",
      "1.6483932247399706 0.09033258017775335\n",
      "1.6479564491503789 0.08853925525397324\n",
      "1.6475459210445538 0.07975676227722005\n",
      "1.6471523568117277 0.07504118928976185\n",
      "1.64677728220996 0.06989891751367815\n",
      "1.6463936752212052 0.0669028347292524\n",
      "1.6459049847174911 0.06977164640713397\n",
      "1.6456317785681749 0.06733389081005554\n",
      "1.6453595585470517 0.06999070285370919\n",
      "1.645088726901562 0.06949852203029781\n",
      "1.6448447280833305 0.07266862247148714\n",
      "norms 27.993443555614984 37.25615142212381 38.18533507439971\n",
      "gradients 0.07266862247148714 0.3222387463241266 0.2769318274690703\n",
      "epoch 138 took 486.5s, optimizer loss=1.653, test mae=1.2\n",
      "1.6448447280833305 0.07266862247148714\n",
      "1.6445580870059036 0.06827311907171128\n",
      "1.6443438649304178 0.07041227430518378\n",
      "1.6441320017012937 0.06877785820543433\n",
      "1.643888890034426 0.09546286540648542\n",
      "1.643700716274588 0.09177113141927636\n",
      "1.6434838798877274 0.08470630286517436\n",
      "1.643214045609047 0.07771493987638381\n",
      "1.642904755210777 0.070358412721949\n",
      "1.6425534885480306 0.0606780307343773\n",
      "1.642235922023661 0.05819119889893177\n",
      "1.641935722718254 0.05534365288443336\n",
      "1.6416080250711587 0.06281592167391446\n",
      "1.6413026710514471 0.06791561768839209\n",
      "1.6409765126558138 0.07636674987562898\n",
      "1.640584097453831 0.08504025307835493\n",
      "1.6401800779707671 0.09847085393929074\n",
      "1.639753953276453 0.10840630105796026\n",
      "1.6394147935705805 0.11305023350195177\n",
      "1.63905666640018 0.12961267438723886\n",
      "1.6387296876278417 0.12754524307399906\n",
      "norms 28.023098369155548 37.25552265088337 38.20108846008126\n",
      "gradients 0.12754524307399906 0.33849569130218576 0.4943622788708682\n",
      "epoch 139 took 484.3s, optimizer loss=1.645, test mae=1.201\n",
      "1.6387296876278417 0.12754524307399906\n",
      "1.6384001780486528 0.12672884196652492\n",
      "1.6379710701638375 0.12044187440196322\n",
      "1.6376247966004156 0.11183287315664599\n",
      "1.6373024922086212 0.10839853915230628\n",
      "1.636935618402783 0.09004705410380301\n",
      "1.6366582626267117 0.08338420361522773\n",
      "1.6364169570570826 0.07919223354142126\n",
      "1.6361334836712764 0.07630513154161353\n",
      "1.6358081088771226 0.07415411345237408\n",
      "1.635451208626785 0.08223604807822822\n",
      "1.635065432659889 0.08206243005403727\n",
      "1.6346969138761833 0.09227790957324988\n",
      "1.6343191863363418 0.09484463911042101\n",
      "1.6339236952924552 0.1033384837951295\n",
      "1.6334749681533405 0.10573126225685202\n",
      "1.6330038372342908 0.11095607924719085\n",
      "1.6324898062736972 0.11602456650765687\n",
      "1.6319419999370846 0.11214589807039875\n",
      "1.6314660887339083 0.11293826787474842\n",
      "1.6309857383899138 0.10787755335943908\n",
      "norms 28.04438321587691 37.25169155761177 38.20781252305198\n",
      "gradients 0.10787755335943908 0.2962563259136234 0.8454673003986535\n",
      "epoch 140 took 485.9s, optimizer loss=1.639, test mae=1.199\n",
      "1.6309857383899138 0.10787755335943908\n",
      "1.6305218305224831 0.10323278916892834\n",
      "1.6300734017970928 0.09856170058701197\n",
      "1.629655210809356 0.0927214386832108\n",
      "1.6293011943844904 0.09200644646450315\n",
      "1.6289789097219092 0.08417635111744698\n",
      "1.628679033503441 0.09274549154932474\n",
      "1.6284483780590007 0.09090183427580996\n",
      "1.6282047886598194 0.10003463402785402\n",
      "1.627947473351202 0.10070185487223683\n",
      "1.627675220437279 0.10582791986960216\n",
      "1.6274023609087918 0.10807063021632896\n",
      "1.6271173095516687 0.11543393603524743\n",
      "1.6269036847957323 0.11396398879209226\n",
      "1.6266727676079533 0.11463637000154381\n",
      "1.6263837818308817 0.11234137411331446\n",
      "1.6260659089966094 0.10481795546983276\n",
      "1.6257981924933844 0.10414238613876958\n",
      "1.6255049779330628 0.09460999458865896\n",
      "1.625232645044864 0.08874609131340769\n",
      "1.6249618070068177 0.08633053573920041\n",
      "norms 28.074489205442482 37.25020001439378 38.22261101504906\n",
      "gradients 0.08633053573920041 0.3112102712508423 0.389957351549799\n",
      "epoch 141 took 485.0s, optimizer loss=1.631, test mae=1.2\n",
      "1.6249618070068177 0.08633053573920041\n",
      "1.6246864747032959 0.07797026063773947\n",
      "1.624446666900047 0.0769943472472485\n",
      "1.624181622144045 0.07453494333350813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6239575380198294 0.07390233992145458\n",
      "1.6237360174488071 0.07333661885305048\n",
      "1.6234392087021388 0.08781471652438703\n",
      "1.623159027630255 0.08725444563329506\n",
      "1.6228643118608996 0.08936989802032091\n",
      "1.6225061845667124 0.08738317255921502\n",
      "1.6222392560530754 0.08531424269535523\n",
      "1.6219480964182313 0.08179727512879202\n",
      "1.621652407304615 0.07853547564877886\n",
      "1.6213391891565907 0.07055272570559046\n",
      "1.6211037269841229 0.06387911621094079\n",
      "1.6208703734659737 0.058576521361346875\n",
      "1.6205577291643654 0.06232845282646262\n",
      "1.6202536490430708 0.06282975439322393\n",
      "1.6199189730650059 0.06316730540294035\n",
      "1.6195173224261175 0.07582214628213411\n",
      "1.6191072811948013 0.08230603812624075\n",
      "norms 28.095897136639046 37.24652411185118 38.22987268606879\n",
      "gradients 0.08230603812624075 0.2893878915103131 0.5362866584975586\n",
      "epoch 142 took 490.5s, optimizer loss=1.625, test mae=1.198\n",
      "1.6191072811948013 0.08230603812624075\n",
      "1.6187169326214292 0.08046384969952178\n",
      "1.6183605815608466 0.09344194017617724\n",
      "1.618057751915816 0.09917181309965652\n",
      "1.6176764330957658 0.11034728531702512\n",
      "1.6171993856072142 0.11799563572345667\n",
      "1.6167090788887886 0.11942158696022655\n",
      "1.616246962247545 0.13161050385876208\n",
      "1.6157922285441129 0.1276357141884805\n",
      "1.6153417516516875 0.13303884065722013\n",
      "1.6149005415103472 0.12873655258315664\n",
      "1.6144604539535168 0.12691749697202415\n",
      "1.6140136387187842 0.13835908068890587\n",
      "1.6135320215251001 0.12286822992516118\n",
      "1.6131931618459554 0.12100526860880338\n",
      "1.6128157153795104 0.1077144847545052\n",
      "1.6124793248531288 0.09949561576731204\n",
      "1.6121530608000636 0.10262713338841507\n",
      "1.6118671620024063 0.09243711104012195\n",
      "1.6116207100723734 0.0886786835457774\n",
      "1.6113406359513256 0.08514611898459931\n",
      "norms 28.1552261399542 37.25167333291421 38.26649403639917\n",
      "gradients 0.08514611898459931 0.2675247367198437 0.2930221745092336\n",
      "epoch 143 took 485.4s, optimizer loss=1.619, test mae=1.194\n",
      "1.6113406359513256 0.08514611898459931\n",
      "1.6110609774692757 0.07871188742746868\n",
      "1.6108103545979606 0.07916716525300867\n",
      "1.6105383832723905 0.07595699378073077\n",
      "1.6102908506245288 0.07376420105240009\n",
      "1.6100367130014177 0.0775307101930676\n",
      "1.6097669135703498 0.07349119750283781\n",
      "1.6095043614210587 0.07661495923769214\n",
      "1.609197074517348 0.07264895515502863\n",
      "1.6089062055032786 0.07132927998753363\n",
      "1.6086342046428133 0.07166738669280648\n",
      "1.6083339010120765 0.06668491781412819\n",
      "1.60803451491083 0.06563438018148919\n",
      "1.607765111736468 0.06074513524065683\n",
      "1.6074967507390177 0.059733472455103395\n",
      "1.6072583474503777 0.056607043471799776\n",
      "1.6070362019440185 0.05671504466852789\n",
      "1.6067990570141732 0.05350464895079623\n",
      "1.6065970980490054 0.05549886106143166\n",
      "1.6063867223810757 0.05638802832203937\n",
      "1.606118705772898 0.06959787331823827\n",
      "norms 28.170588221524145 37.25298439686326 38.2780537842563\n",
      "gradients 0.06959787331823827 0.28958277145642636 0.9639034753403845\n",
      "epoch 144 took 487.6s, optimizer loss=1.611, test mae=1.192\n",
      "1.606118705772898 0.06959787331823827\n",
      "1.605933745186086 0.06736886521421036\n",
      "1.6057318172607407 0.06927164107219502\n",
      "1.6054560481700133 0.06961953662523729\n",
      "1.6050999093048344 0.07151522467403998\n",
      "1.6046457553473499 0.07228455863515347\n",
      "1.6041072668763947 0.0716902767171094\n",
      "1.6035599001191143 0.0716368388939897\n",
      "1.6029808425344116 0.07035223363943871\n",
      "1.602493420347788 0.06899816278735847\n",
      "1.6020919061262018 0.07164022050632725\n",
      "1.601771597755299 0.06590437094644451\n",
      "1.6015201463080229 0.0644081904774104\n",
      "1.6012412953492527 0.06273813470358082\n",
      "1.6009417164069468 0.061949624400322256\n",
      "1.600624544063239 0.06440428236810215\n",
      "1.6002871897484137 0.06589270673312923\n",
      "1.5999535873473554 0.07181901343967298\n",
      "1.599596874622917 0.07292197821820286\n",
      "1.5992638211221566 0.08032127222488314\n",
      "1.5989810865962064 0.08041120013886338\n",
      "norms 28.213574286270678 37.250849319892424 38.298161963996726\n",
      "gradients 0.08041120013886338 0.2647615303001538 0.24744888259443057\n",
      "epoch 145 took 485.6s, optimizer loss=1.606, test mae=1.191\n",
      "1.5989810865962064 0.08041120013886338\n",
      "1.5986800556779592 0.09061162484206581\n",
      "1.5983907131376363 0.08878831979589816\n",
      "1.5981079164918388 0.08866641752905038\n",
      "1.5978145274710889 0.08900178097380566\n",
      "1.5974979786732721 0.08113392602007617\n",
      "1.5972128719594776 0.0833803035688248\n",
      "1.5969593406697822 0.07506682996049427\n",
      "1.5967196555437935 0.07872825067095797\n",
      "1.59652149227967 0.0735442468804542\n",
      "1.5963305643202186 0.07854571077991639\n",
      "1.5961301166536006 0.07507057703249319\n",
      "1.595929613672128 0.07903491801117374\n",
      "1.5956954322666375 0.08059730307960629\n",
      "1.5954614266267417 0.08388873633992773\n",
      "1.5952120676235324 0.08731776349566485\n",
      "1.594918026085976 0.0934884600216697\n",
      "1.5946366031121113 0.09704492875614125\n",
      "1.5943444977279873 0.10030492733280853\n",
      "1.5940293186152181 0.10505544307510377\n",
      "1.5937220794190252 0.10452367770687847\n",
      "norms 28.243717780161486 37.25116010017028 38.31410310207049\n",
      "gradients 0.10452367770687847 0.2786504268012873 0.5198326190427863\n",
      "epoch 146 took 486.4s, optimizer loss=1.599, test mae=1.187\n",
      "1.5937220794190252 0.10452367770687847\n",
      "1.5934007185249979 0.10576730494827986\n",
      "1.593059701426536 0.10285280100597373\n",
      "1.5927191381449537 0.09923788386039835\n",
      "1.5924108771956067 0.09492623815664666\n",
      "1.5921200383356957 0.0888990380674034\n",
      "1.5918412175320338 0.08602441303871386\n",
      "1.5916086255669615 0.08021883923449148\n",
      "1.591371182492022 0.0766604385465663\n",
      "1.5911072459913282 0.07982469227896746\n",
      "1.5908763444935243 0.07266826026109943\n",
      "1.590634867334766 0.07915212975226478\n",
      "1.5903822262528382 0.07305175392513101\n",
      "1.5901129038987802 0.07507665702380827\n",
      "1.589806894947711 0.06958854775642985\n",
      "1.5894852793888112 0.06909426880739519\n",
      "1.5891912703285258 0.07148740468691302\n",
      "1.5889296217264806 0.06759283498129835\n",
      "1.5886992392588961 0.06850560696126945\n",
      "1.588463722346175 0.0702037432202534\n",
      "1.5882133754201486 0.06667394637858427\n",
      "norms 28.27113364834965 37.250818278036576 38.32705204496625\n",
      "gradients 0.06667394637858427 0.3112801830933929 0.20982979681314085\n",
      "epoch 147 took 482.9s, optimizer loss=1.594, test mae=1.19\n",
      "1.5882133754201486 0.06667394637858427\n",
      "1.58798078294001 0.06859311695907698\n",
      "1.5877709448198838 0.0679754648219112\n",
      "1.587583035823586 0.06809239044414005\n",
      "1.587380023650161 0.06895696347599037\n",
      "1.5871162204669234 0.0755358204773919\n",
      "1.5869302027467642 0.07708048989542096\n",
      "1.586676246296576 0.07809311159672763\n",
      "1.5863101114793114 0.08514257385350557\n",
      "1.5859927903263564 0.08178664534711812\n",
      "1.5856742669219164 0.08817006169092353\n",
      "1.585317665585587 0.08510058796248551\n",
      "1.5849582987097932 0.0861775109461946\n",
      "1.5846168150863422 0.09944926831201703\n",
      "1.58436593788516 0.0946245067287588\n",
      "1.5841276856198643 0.09798374858132794\n",
      "1.5838589300137056 0.09271768525543456\n",
      "1.5835493389134645 0.09407818782102702\n",
      "1.5832181001898529 0.09388193030300479\n",
      "1.5829660833759778 0.08875423563564844\n",
      "1.5827173475407765 0.08844636358863281\n",
      "norms 28.313200085750424 37.25415818018666 38.3518461832686\n",
      "gradients 0.08844636358863281 0.29912394659530567 0.45219479533023194\n",
      "epoch 148 took 489.1s, optimizer loss=1.588, test mae=1.183\n",
      "1.5827173475407765 0.08844636358863281\n",
      "1.5824255481881717 0.08575022757558777\n",
      "1.5820752505838915 0.08305325469198827\n",
      "1.581656583816055 0.09558806028406247\n",
      "1.5813980665865297 0.0900492507079507\n",
      "1.5811146782068084 0.08643643892322998\n",
      "1.580753229176083 0.0832973326430403\n",
      "1.5802660214587274 0.08835109491225791\n",
      "1.579840232474661 0.08525516831130364\n",
      "1.5794913801450685 0.08822310852379187\n",
      "1.5791061820766898 0.08778662225612323\n",
      "1.5786644514761425 0.09261399396301484\n",
      "1.5781871805807364 0.09497147435859837\n",
      "1.5777767712062283 0.09304315697491482\n",
      "1.5774047261130681 0.09516016434787837\n",
      "1.5770531566840094 0.09117203489832401\n",
      "1.5767196996938373 0.08930666367263991\n",
      "1.5763380737879065 0.08407355511707468\n",
      "1.575947748412153 0.07940278840905272\n",
      "1.5755944524041856 0.07231841638141345\n",
      "1.575190759447818 0.08648534299337204\n",
      "norms 28.34573866060575 37.25110464975661 38.36545136531901\n",
      "gradients 0.08648534299337204 0.29231785670163646 0.7665468150604139\n",
      "epoch 149 took 484.1s, optimizer loss=1.583, test mae=1.183\n",
      "1.575190759447818 0.08648534299337204\n",
      "1.5748534305333985 0.07796003473831933\n",
      "1.57453172293573 0.08148930313594803\n",
      "1.574233473834381 0.06984384508942548\n",
      "1.5739749502901466 0.07452006985702367\n",
      "1.5737381713670249 0.07117517491791431\n",
      "1.573527766603276 0.06888783888228424\n",
      "1.5733565407161898 0.07182019216564524\n",
      "1.5731757169635403 0.07187877715742845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5729938638914753 0.07179884408857615\n",
      "1.5727917912612759 0.07347396923158889\n",
      "1.5725489167374778 0.07830199428428856\n",
      "1.5723078758904576 0.07193027106340595\n",
      "1.5720958138152272 0.07480855602343023\n",
      "1.5718525535601091 0.07529993140447135\n",
      "1.5716474694559914 0.07153090566796748\n",
      "1.5714115598287248 0.08318167769767379\n",
      "1.5711563966678672 0.07360667314426896\n",
      "1.5709311295021449 0.07113163050196875\n",
      "1.5706610194963413 0.07954211852007166\n",
      "1.5704618859827673 0.07034354985338347\n",
      "norms 28.330094914719098 37.24509862719607 38.3531824021704\n",
      "gradients 0.07034354985338347 0.28189490996628214 0.7249327234380022\n",
      "epoch 150 took 486.4s, optimizer loss=1.575, test mae=1.178\n",
      "1.5704618859827673 0.07034354985338347\n",
      "1.5702461027999541 0.06386442132353247\n",
      "1.5699880746184067 0.054071456877311\n",
      "1.5697065594730477 0.05007639966058811\n",
      "1.5694590937865502 0.048364154657736344\n",
      "1.5692224846093465 0.0512783417344188\n",
      "1.568971403779995 0.05516628991694884\n",
      "1.5687497049783146 0.05907725285899471\n",
      "1.5685169310332003 0.06439175365566771\n",
      "1.56821658233879 0.07008335255944821\n",
      "1.5679098253140897 0.07275707249802092\n",
      "1.5676528257264994 0.07262694057207501\n",
      "1.5673026933896614 0.0833681278208077\n",
      "1.5669985786846004 0.08091896441378263\n",
      "1.566578072773429 0.08262669905741107\n",
      "1.5660640927766358 0.07724386293126903\n",
      "1.5655001618629354 0.07787096084449784\n",
      "1.564915359121653 0.07040506164693196\n",
      "1.5643373934704736 0.07050601696909381\n",
      "1.5637864848289176 0.07468600143354764\n",
      "1.5632231964580887 0.06872526700687294\n",
      "norms 28.404826581466683 37.24699211145077 38.39310588848773\n",
      "gradients 0.06872526700687294 0.2785651782485257 0.3479334274024662\n",
      "epoch 151 took 479.9s, optimizer loss=1.57, test mae=1.178\n",
      "1.5632231964580887 0.06872526700687294\n",
      "1.5628688583271861 0.06894584507483677\n",
      "1.5625481692105734 0.06844661802337601\n",
      "1.5622286130427636 0.07219037426147407\n",
      "1.5619209543608694 0.07174406885591762\n",
      "1.5616532225623778 0.07430491217717917\n",
      "1.5614137575152978 0.07568937703339718\n",
      "1.5611826780637656 0.07698817343692278\n",
      "1.5608911071919946 0.08631609649355095\n",
      "1.560654157677079 0.08751730114748549\n",
      "1.5602503573896436 0.0967548778292095\n",
      "1.5599554307926755 0.09688021184308351\n",
      "1.559505385044597 0.10101181993913326\n",
      "1.5590846339894862 0.09541303378856433\n",
      "1.5585957232155045 0.10012185211973756\n",
      "1.5581344928207124 0.09387746324342476\n",
      "1.5577099161963448 0.08184919062823807\n",
      "1.55732561509798 0.08134419179263287\n",
      "1.5570222925486543 0.07781281544138582\n",
      "1.5567586643423563 0.07220031418863965\n",
      "1.5565287622175492 0.07509103972393279\n",
      "norms 28.420152949676883 37.24404468958791 38.39913395461069\n",
      "gradients 0.07509103972393279 0.2840091506389807 0.2379772120466139\n",
      "epoch 152 took 485.0s, optimizer loss=1.563, test mae=1.175\n",
      "1.5565287622175492 0.07509103972393279\n",
      "1.5563267502961515 0.06963104230482949\n",
      "1.556132927129966 0.07068920075902152\n",
      "1.5559204473335202 0.06505839008079521\n",
      "1.5557112057605964 0.06370957469215945\n",
      "1.555519964240019 0.059952594931752055\n",
      "1.5553119277543461 0.06096505935518038\n",
      "1.5551511418902746 0.0582195617535502\n",
      "1.5549822735317533 0.057698708783049585\n",
      "1.5547636666530664 0.057989025064258584\n",
      "1.5545821503943174 0.05991816587832679\n",
      "1.554397318570895 0.06146379483225324\n",
      "1.554194327936756 0.06712939765297576\n",
      "1.5539820822816146 0.06967932029155445\n",
      "1.5537670674153463 0.0760787027329609\n",
      "1.5535446332975058 0.08043741765468773\n",
      "1.5533051237407147 0.08512956848573124\n",
      "1.5530243924701743 0.1041969190072516\n",
      "1.552768214629285 0.10727635171968936\n",
      "1.552474458370031 0.10397934945393437\n",
      "1.552088554688057 0.12649978406335732\n",
      "norms 28.440516425063116 37.24173347973284 38.408207951700426\n",
      "gradients 0.12649978406335732 0.2867201772487645 0.6490769106307589\n",
      "epoch 153 took 483.7s, optimizer loss=1.557, test mae=1.176\n",
      "1.552088554688057 0.12649978406335732\n",
      "1.5517478458971994 0.12217153390883706\n",
      "1.5513293845095273 0.11427460430496496\n",
      "1.5508454096081863 0.10352035552955682\n",
      "1.550360759046429 0.09144286618316586\n",
      "1.5498933026932464 0.08380870120447953\n",
      "1.5495380774654517 0.0706016037550211\n",
      "1.5492052875904383 0.06850948115010796\n",
      "1.5488721515556423 0.06475627709352293\n",
      "1.5485982363762538 0.06159956159223573\n",
      "1.5483796407673096 0.06685351045924282\n",
      "1.5481747664045473 0.06917625194306008\n",
      "1.5479902111098243 0.07496270678333156\n",
      "1.5478052293666 0.07951670880263315\n",
      "1.5475969497827093 0.08675325650462322\n",
      "1.5474042309239833 0.09105729339558981\n",
      "1.5471945361917354 0.09848422638421515\n",
      "1.5469891109440574 0.10154384478997139\n",
      "1.54677431779651 0.10325755457686228\n",
      "1.5465066482563763 0.10527259227711487\n",
      "1.5462594708442858 0.10511123415315893\n",
      "norms 28.477274606125164 37.246643504298 38.43258928914618\n",
      "gradients 0.10511123415315893 0.26728626473432593 0.6751438024420848\n",
      "epoch 154 took 482.2s, optimizer loss=1.552, test mae=1.174\n",
      "1.5462594708442858 0.10511123415315893\n",
      "1.5460029310858752 0.10147734906626464\n",
      "1.5456989824000271 0.09611353737250197\n",
      "1.5454115037633465 0.09436583960062198\n",
      "1.5451533422953507 0.08290587368763028\n",
      "1.5448935112573385 0.08236730079731515\n",
      "1.544656790517 0.07158324588231885\n",
      "1.5444497667298869 0.06989723118798234\n",
      "1.5442469134201902 0.0649967092609107\n",
      "1.5440446689622769 0.06434106288236496\n",
      "1.5438360346101536 0.07112729193255521\n",
      "1.5436665048658365 0.070814188171347\n",
      "1.5435094489462209 0.07186273111994576\n",
      "1.543367325645749 0.10470486330168861\n",
      "1.543223755452548 0.09866197265063903\n",
      "1.5430505248005275 0.09389918500238024\n",
      "1.5428410309570744 0.08748981249815813\n",
      "1.5426080239629423 0.08648392843704727\n",
      "1.542366460883269 0.08113448226066888\n",
      "1.5421543034540006 0.0819548523187399\n",
      "1.5419447033906493 0.08219606437896125\n",
      "norms 28.476751628897073 37.243321776969424 38.430252156968756\n",
      "gradients 0.08219606437896125 0.25874499034652265 0.42781959906741046\n",
      "epoch 155 took 486.8s, optimizer loss=1.546, test mae=1.171\n",
      "1.5419447033906493 0.08219606437896125\n",
      "1.541728458656793 0.08317099869927475\n",
      "1.541508924964789 0.08848543388357326\n",
      "1.541280222480618 0.08597066148551995\n",
      "1.5410852249696876 0.0912738083739527\n",
      "1.5409080542859714 0.09040410902980273\n",
      "1.5407232956056032 0.09652994212744977\n",
      "1.5405589787779805 0.09513855680513227\n",
      "1.5403519442613853 0.09942303800603626\n",
      "1.5400838897417153 0.09071999690350986\n",
      "1.5397992382158496 0.09492865264519856\n",
      "1.5395605311344556 0.0892215545134877\n",
      "1.5393329437219487 0.08463903851344338\n",
      "1.5390985696595734 0.07835958235625022\n",
      "1.5388654168259102 0.07494506349104742\n",
      "1.5386461999042926 0.06915647431035978\n",
      "1.5384524214038247 0.06850180296258017\n",
      "1.5382782785171594 0.06803556875725048\n",
      "1.5381146490153896 0.06940747224960649\n",
      "1.5379532341903714 0.07345489854467936\n",
      "1.537774683353413 0.07606801779002945\n",
      "norms 28.51458547095108 37.246349331126126 38.45338654556311\n",
      "gradients 0.07606801779002945 0.26417760568333587 0.19618505313706605\n",
      "epoch 156 took 483.7s, optimizer loss=1.542, test mae=1.169\n",
      "1.537774683353413 0.07606801779002945\n",
      "1.5375865627527456 0.08412148640236687\n",
      "1.5373969512868 0.08709111580423082\n",
      "1.537224064447448 0.09198244896317002\n",
      "1.5370096686691672 0.1044046403573832\n",
      "1.53684259152959 0.10499648152786391\n",
      "1.5366385259392454 0.101379294022388\n",
      "1.536342043698596 0.11142775625404082\n",
      "1.536134535889675 0.10690623714134198\n",
      "1.5359052165433724 0.10212722847426965\n",
      "1.5356325217665645 0.09792647941661288\n",
      "1.5353703856568495 0.08954661699851749\n",
      "1.5351425348969083 0.08185036452711063\n",
      "1.534870245340213 0.08592565266027782\n",
      "1.5346369692754467 0.07718569053374535\n",
      "1.534429790081018 0.07281448947516893\n",
      "1.5341942259511858 0.06866854083402417\n",
      "1.533991157809648 0.06621420842555809\n",
      "1.533796042902208 0.0633913466348127\n",
      "1.5335814525972342 0.06429033055138501\n",
      "1.5333670932020198 0.06323139077448413\n",
      "norms 28.52626696542519 37.243651913149066 38.45755795702055\n",
      "gradients 0.06323139077448413 0.2397000873354759 0.21414589425269892\n",
      "epoch 157 took 483.6s, optimizer loss=1.538, test mae=1.171\n",
      "1.5333670932020198 0.06323139077448413\n",
      "1.5331728495283852 0.06285923166707671\n",
      "1.5330041912791827 0.06377648845723156\n",
      "1.5328296904090146 0.06058063452308829\n",
      "1.5326341946795219 0.0644575622055761\n",
      "1.5324279282070457 0.0583899770334667\n",
      "1.5322341675485753 0.060269858953728044\n",
      "1.5320397043820948 0.05873549929136183\n",
      "1.5318256797185215 0.05967932850031246\n",
      "1.5315894562227894 0.06006077599338759\n",
      "1.5313337297097183 0.061820675790772286\n",
      "1.5310743681458758 0.06403726707887913\n",
      "1.530868430814252 0.06532499174598251\n",
      "1.5306156915324534 0.07361521493937208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5303918523719517 0.07342505545552416\n",
      "1.5301581446820811 0.07684209866495299\n",
      "1.529903330788961 0.07556161693416592\n",
      "1.529650746357413 0.07760263188007463\n",
      "1.5294111296304487 0.07407836623495294\n",
      "1.529195417217129 0.0750290390436404\n",
      "1.5289924614362476 0.07154619716301024\n",
      "norms 28.568529927396078 37.2501553021911 38.485593814544394\n",
      "gradients 0.07154619716301024 0.2768041079869594 0.3320744229421498\n",
      "epoch 158 took 489.2s, optimizer loss=1.533, test mae=1.17\n",
      "1.5289924614362476 0.07154619716301024\n",
      "1.5288217484837516 0.07219389842485087\n",
      "1.5286783790692402 0.07025294160057885\n",
      "1.5285146463939314 0.07502404951966808\n",
      "1.5283928248487704 0.07388593642785195\n",
      "1.528260467557024 0.07445658372188832\n",
      "1.5280792937777412 0.07542323947653246\n",
      "1.5279097166255202 0.0709970980110039\n",
      "1.5277431589726544 0.07237563787318395\n",
      "1.5275217423144076 0.06755915244704822\n",
      "1.5273663675125544 0.06454438899952104\n",
      "1.5271735598793195 0.06215804504899566\n",
      "1.526919547830173 0.059508792550196546\n",
      "1.5265991671188188 0.06344272252986662\n",
      "1.5262970147355257 0.06205360412088247\n",
      "1.5260190350935392 0.06593479952689607\n",
      "1.525716556731694 0.06693563735059876\n",
      "1.5253726033491315 0.07357729519238096\n",
      "1.5250763755230548 0.0801402137107959\n",
      "1.524804977289265 0.08370226375233525\n",
      "1.5245408093464772 0.0904769681556914\n",
      "norms 28.58631550039604 37.24570285831863 38.49004895853078\n",
      "gradients 0.0904769681556914 0.289331593100663 0.4051492976158369\n",
      "epoch 159 took 482.9s, optimizer loss=1.529, test mae=1.168\n",
      "1.5245408093464772 0.0904769681556914\n",
      "1.5243010118464901 0.09224656911526263\n",
      "1.5240554234985715 0.09551966295126714\n",
      "1.5237680451088331 0.09522769933648587\n",
      "1.523488435420497 0.0950058067772921\n",
      "1.523198082492893 0.09410929620855861\n",
      "1.522908927221536 0.08942389468856002\n",
      "1.5226188815275155 0.10082171123190833\n",
      "1.5223998130772303 0.0947589333472776\n",
      "1.5221890967922376 0.08912438494671338\n",
      "1.521968610258223 0.08429174379202968\n",
      "1.5217451942164286 0.07670676431611263\n",
      "1.5215587577436116 0.07407183703499867\n",
      "1.5213495768214473 0.07180937211160945\n",
      "1.5211514962890853 0.07127726691533691\n",
      "1.520934931556358 0.07354362025637833\n",
      "1.520676847444409 0.07429381919896827\n",
      "1.5204285027765985 0.07764275916036328\n",
      "1.5201938889308653 0.08059443558539196\n",
      "1.5199777462534347 0.08088174941943681\n",
      "1.519721099796249 0.08284622532867648\n",
      "norms 28.60601492158146 37.24367045223864 38.500292771745265\n",
      "gradients 0.08284622532867648 0.295863813310238 0.49031705373141504\n",
      "epoch 160 took 485.4s, optimizer loss=1.525, test mae=1.171\n",
      "1.519721099796249 0.08284622532867648\n",
      "1.519416112392151 0.07966514733652215\n",
      "1.519082411839151 0.08348881283823303\n",
      "1.51879008072408 0.07788387007876524\n",
      "1.5185275079220488 0.07373952472073489\n",
      "1.518229671717754 0.07519071019162969\n",
      "1.5179868956396891 0.07059316750864507\n",
      "1.5177401884031085 0.06564344476932253\n",
      "1.5174482614989524 0.06474320330940321\n",
      "1.5172186623313089 0.06045046261114504\n",
      "1.5169998155154303 0.05850141082708475\n",
      "1.5167376430426187 0.06787832934461858\n",
      "1.5165024885120215 0.06837196390585057\n",
      "1.5162615290187489 0.07920997055023854\n",
      "1.5159749008729688 0.07801878808243577\n",
      "1.5157481947898526 0.08269681272898699\n",
      "1.5155387709108255 0.08200721657630791\n",
      "1.5153126097223637 0.08350323847680521\n",
      "1.5150734925527627 0.08108733442021014\n",
      "1.5148022691629957 0.08223380734599514\n",
      "1.5144872179731719 0.07763344930178954\n",
      "norms 28.637155397052123 37.24497146094851 38.51846345937615\n",
      "gradients 0.07763344930178954 0.2885002035580782 0.17350957055540506\n",
      "epoch 161 took 484.9s, optimizer loss=1.52, test mae=1.17\n",
      "1.5144872179731719 0.07763344930178954\n",
      "1.5142013773112737 0.07674967152858432\n",
      "1.513955894659708 0.07194516910605946\n",
      "1.513699015977717 0.0785543821498013\n",
      "1.5134344097537238 0.0721230093758358\n",
      "1.5132007101502412 0.0754715969421451\n",
      "1.5129415764147536 0.08010860181471495\n",
      "1.5127081228945318 0.07564481995479548\n",
      "1.5124807526839965 0.08215711338151495\n",
      "1.5122565018216718 0.08290657912840432\n",
      "1.5120159657601466 0.08585264735785504\n",
      "1.5117133588772784 0.09242172083251385\n",
      "1.5113277533814227 0.08922994355231209\n",
      "1.5109414114005986 0.09330734485541022\n",
      "1.5106185002100339 0.09168303729666809\n",
      "1.5102880478777427 0.08201796920344663\n",
      "1.5099947857614577 0.07659188427561599\n",
      "1.509705170988564 0.0730508284324094\n",
      "1.5093526035897502 0.06303908452544543\n",
      "1.509056773570516 0.055798848482213025\n",
      "1.508782822634848 0.05267384144289623\n",
      "norms 28.664841800647906 37.24132300463003 38.5302124366178\n",
      "gradients 0.05267384144289623 0.2958920215073571 0.21596640586734306\n",
      "epoch 162 took 487.7s, optimizer loss=1.514, test mae=1.172\n",
      "1.508782822634848 0.05267384144289623\n",
      "1.50848125461674 0.049332471503047384\n",
      "1.5081388610380166 0.05211562687295641\n",
      "1.5078130286182079 0.05713505705055861\n",
      "1.5075600388148485 0.06111477068963159\n",
      "1.5073294406120061 0.06642301514570506\n",
      "1.5070571610046584 0.07533164867327555\n",
      "1.506787071101688 0.07871628835110664\n",
      "1.5064694286895555 0.08572923660240436\n",
      "1.5060512338003114 0.08239926091530048\n",
      "1.5056113990376416 0.0920483053357168\n",
      "1.5052076550619962 0.08677299325045935\n",
      "1.504817205184951 0.08942995238494017\n",
      "1.5044227444152782 0.08533527221922829\n",
      "1.5039998290723287 0.08273036224397053\n",
      "1.5037121857760554 0.07858387364216932\n",
      "1.5034237476013403 0.07274933482188295\n",
      "1.50306808316266 0.10338685712569927\n",
      "1.5028454294414146 0.09433176140656198\n",
      "1.5026139069844433 0.08551779516249798\n",
      "1.5023581065658307 0.07727906347336887\n",
      "norms 28.689533125355524 37.24118299342474 38.54503206918961\n",
      "gradients 0.07727906347336887 0.29627408334345473 0.3121677289461745\n",
      "epoch 163 took 486.3s, optimizer loss=1.509, test mae=1.17\n",
      "1.5023581065658307 0.07727906347336887\n",
      "1.5020779002282605 0.07102846167492213\n",
      "1.5017685982459696 0.06358015438704617\n",
      "1.501564586061315 0.060691990422760946\n",
      "1.501357688862415 0.05877550454022109\n",
      "1.5011026701162251 0.0642029550960117\n",
      "1.500816340575872 0.06161368914122844\n",
      "1.500554456781293 0.0660883812244571\n",
      "1.5003096000052643 0.06866300190877599\n",
      "1.500065353633379 0.07024986645350247\n",
      "1.4998304917759424 0.07361519819988585\n",
      "1.4996161730340063 0.0753289194800853\n",
      "1.4994164866402464 0.07799669602862705\n",
      "1.4992080259919631 0.07987537315447524\n",
      "1.4989652938405664 0.0867166654258848\n",
      "1.4987267296484443 0.08631957239034874\n",
      "1.4985146069845587 0.09011420693213597\n",
      "1.4982681284949868 0.08948582666465399\n",
      "1.4979962109549998 0.0886407091810084\n",
      "1.4977180818907554 0.08933202173884455\n",
      "1.4974557949290237 0.08595281705156219\n",
      "norms 28.71797188709431 37.24282400333288 38.561926187425435\n",
      "gradients 0.08595281705156219 0.2805216308709873 0.27320994211953287\n",
      "epoch 164 took 487.5s, optimizer loss=1.502, test mae=1.171\n",
      "1.4974557949290237 0.08595281705156219\n",
      "1.4971959088502111 0.0857295499399373\n",
      "1.4969405459353982 0.08023328253526239\n",
      "1.4967035063179979 0.07815019055520367\n",
      "1.4965208395212266 0.07375901233957774\n",
      "1.496337408902414 0.07097709104686899\n",
      "1.496116288132713 0.06552683664491624\n",
      "1.49588305616891 0.06245991006933565\n",
      "1.495625284824232 0.06647372583559738\n",
      "1.495385692161836 0.06565939546798753\n",
      "1.4951794819316557 0.06607529128708929\n",
      "1.4949765977120613 0.06796947728712319\n",
      "1.4947338158286958 0.07517730356031074\n",
      "1.494500789362031 0.07612505621299219\n",
      "1.4942523737963074 0.08097103452467683\n",
      "1.4939387148186933 0.08046618239484608\n",
      "1.4936315824909105 0.08041148283691928\n",
      "1.493296232040592 0.08726520183383726\n",
      "1.4930053866939963 0.08335818702428284\n",
      "1.4927258840329356 0.11471325147986255\n",
      "1.492488394395952 0.10584739018564462\n",
      "norms 28.726635807609604 37.240202345784326 38.56571826470019\n",
      "gradients 0.10584739018564462 0.26429992361295723 0.6688063093507514\n",
      "epoch 165 took 484.0s, optimizer loss=1.497, test mae=1.171\n",
      "1.492488394395952 0.10584739018564462\n",
      "1.4922451198687254 0.09627829656529448\n",
      "1.4919830122507314 0.08708731129925337\n",
      "1.4917171514715037 0.07742693363377737\n",
      "1.4914580317422903 0.07100274332052985\n",
      "1.491208283099064 0.0639811504778771\n",
      "1.490965330620949 0.06703200969147415\n",
      "1.4907426051563408 0.06300086168067077\n",
      "1.4905614160640381 0.06749216605040592\n",
      "1.490384867100529 0.06934282063967838\n",
      "1.490199404507034 0.07665066937539289\n",
      "1.4900054060748278 0.08009526156581108\n",
      "1.4898367340188203 0.08614998545839579\n",
      "1.4896798250902716 0.08988306256096981\n",
      "1.4895078898898046 0.09408640886644384\n",
      "1.4893082990883846 0.09797661745560823\n",
      "1.4891113435752152 0.10008657763072323\n",
      "1.4888913171770894 0.10312776680246558\n",
      "1.4886278972104399 0.10379698689173203\n",
      "1.4884090737500228 0.10262357746076786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.488151253640777 0.10510990431255347\n",
      "norms 28.773414327537466 37.245093222151205 38.594267075579545\n",
      "gradients 0.10510990431255347 0.2697028826497412 0.3753637736017834\n",
      "epoch 166 took 491.3s, optimizer loss=1.492, test mae=1.175\n",
      "1.488151253640777 0.10510990431255347\n",
      "1.4878557544591857 0.10337074410757566\n",
      "1.487612212798887 0.09732842434128797\n",
      "1.4873785263790644 0.09604494585799861\n",
      "1.4871572645321445 0.08996429261934874\n",
      "1.4869418964486725 0.0871159992957641\n",
      "1.4867189535011422 0.07982762860872421\n",
      "1.4864887717128943 0.0758862262832729\n",
      "1.4862795547630774 0.07268030561727651\n",
      "1.4860674600141768 0.06689145762428869\n",
      "1.4859012726909122 0.06421538713786124\n",
      "1.4857108757576258 0.06286284163808482\n",
      "1.4854784835394799 0.061354962076514113\n",
      "1.485217286220446 0.06073994217319336\n",
      "1.4849899740922514 0.05678021872394988\n",
      "1.4847517094819769 0.06123455967817702\n",
      "1.484511354689047 0.057572965280142455\n",
      "1.4842777368546902 0.06280647538357581\n",
      "1.484018761905702 0.0609665023999007\n",
      "1.4838000233152304 0.060561566844262435\n",
      "1.4835999600802299 0.06465507006900459\n",
      "norms 28.78516029999982 37.24340201649992 38.5994961154682\n",
      "gradients 0.06465507006900459 0.25939465637459164 0.47880971069811645\n",
      "epoch 167 took 488.5s, optimizer loss=1.488, test mae=1.172\n",
      "1.4835999600802299 0.06465507006900459\n",
      "1.4833397977676075 0.06379212193969666\n",
      "1.4831416045115375 0.06642779707480502\n",
      "1.4829111441572158 0.06840725544599856\n",
      "1.4826293827098624 0.06962976341354167\n",
      "1.4823773435428782 0.07521231053056257\n",
      "1.4821509894896305 0.0745021413792802\n",
      "1.4819414413124878 0.07990488366412787\n",
      "1.4817363066344134 0.07842783625821664\n",
      "1.4814991783452656 0.08833317900369397\n",
      "1.4812777507829462 0.08690851851414089\n",
      "1.4810597723480743 0.08565557648259986\n",
      "1.4808092111544287 0.08660208413643798\n",
      "1.4805762764519381 0.08255733697873228\n",
      "1.4803270544071638 0.08363883264436227\n",
      "1.480114791459946 0.08015168490077552\n",
      "1.479915759646431 0.07599089452553591\n",
      "1.4796981658006592 0.07468339808371192\n",
      "1.4794724679669602 0.06853593123072863\n",
      "1.4792586944536072 0.06844024396113198\n",
      "1.4790546084558807 0.06351576836121958\n",
      "norms 28.80910117275593 37.24136435702992 38.61103962118673\n",
      "gradients 0.06351576836121958 0.2662945274537242 0.2893959590091473\n",
      "epoch 168 took 491.6s, optimizer loss=1.484, test mae=1.169\n",
      "1.4790546084558807 0.06351576836121958\n",
      "1.4788615512676162 0.06364956178656363\n",
      "1.4786753290488357 0.0637680151783033\n",
      "1.4784792741780348 0.06100626045588451\n",
      "1.478324361968914 0.0636296389124878\n",
      "1.4781684362806298 0.06103422957348862\n",
      "1.4780003367390833 0.06497684846085917\n",
      "1.477820271735252 0.06201927072586028\n",
      "1.4776732374361048 0.06264502410170839\n",
      "1.4774991729664213 0.06253964837787242\n",
      "1.477253268794101 0.06359498074850403\n",
      "1.477025895335042 0.06209137578360807\n",
      "1.4767495624627633 0.06278914561427817\n",
      "1.476407516502145 0.05911101663114549\n",
      "1.476024605702926 0.06149987571266434\n",
      "1.4757141587262748 0.05555040171292536\n",
      "1.4754427136817174 0.055677914040112804\n",
      "1.4751511271306816 0.057181311420747206\n",
      "1.4748999389086603 0.055966422925019244\n",
      "1.4746547553546714 0.05469548035318136\n",
      "1.4743554318647532 0.0671584905652218\n",
      "norms 28.836641440981364 37.24361981049422 38.62822563434337\n",
      "gradients 0.0671584905652218 0.26523549399020285 0.35870947259436536\n",
      "epoch 169 took 486.1s, optimizer loss=1.479, test mae=1.171\n",
      "1.4743554318647532 0.0671584905652218\n",
      "1.4741428434159658 0.06450418343211323\n",
      "1.4739286320196507 0.06894877018042153\n",
      "1.4736566656442436 0.06809784591866831\n",
      "1.4734637067544332 0.06519132400899386\n",
      "1.4732732785717209 0.06563650350605414\n",
      "1.4730778804751155 0.09597336181909324\n",
      "1.4729047094031522 0.08800773032132106\n",
      "1.4727051174380361 0.07809380062615774\n",
      "1.472478516083433 0.06961099629544241\n",
      "1.4722395137673403 0.059628367122016\n",
      "1.4720153289324172 0.053399160320890496\n",
      "1.4718156822879729 0.04937195456360776\n",
      "1.471638344882122 0.04857164515610962\n",
      "1.4714752926405459 0.050698970536327034\n",
      "1.4713156614249037 0.05547710928673462\n",
      "1.4711271051994237 0.06254522808395291\n",
      "1.470995461143945 0.06611618726592998\n",
      "1.4708609691274752 0.06970722482165372\n",
      "1.4706452824174128 0.07723198466635799\n",
      "1.4704265614473213 0.08148385963653489\n",
      "norms 28.872136214821165 37.24720308888608 38.65115843739205\n",
      "gradients 0.08148385963653489 0.24356168377972817 0.19553937480580427\n",
      "epoch 170 took 490.9s, optimizer loss=1.474, test mae=1.171\n",
      "1.4704265614473213 0.08148385963653489\n",
      "1.4701651463783074 0.08188366831056268\n",
      "1.4698065530309714 0.08894013509583919\n",
      "1.469514834148222 0.08206037886375783\n",
      "1.4691950980407078 0.08679885814370454\n",
      "1.4688787828683456 0.0761034427399717\n",
      "1.4685798902178098 0.07710094908195321\n",
      "1.4682949061537935 0.07573523630348666\n",
      "1.468038479691631 0.06720997505286509\n",
      "1.4678461176228068 0.07156675114482446\n",
      "1.467648933814175 0.06956549605725777\n",
      "1.4674589749453135 0.07166959118570514\n",
      "1.4672542247708549 0.09096369210042209\n",
      "1.4670905521193256 0.09407329741301862\n",
      "1.4669075231708066 0.09419567299810777\n",
      "1.4666737439635211 0.09521143663768028\n",
      "1.4664123057157539 0.09616010732963762\n",
      "1.4661673070299392 0.09318512308838171\n",
      "1.4659264539206192 0.09203172221928023\n",
      "1.465676926467816 0.08541718409270761\n",
      "1.4654802963621054 0.08028940091152423\n",
      "norms 28.87352469647255 37.24241840035141 38.64914396869442\n",
      "gradients 0.08028940091152423 0.23174403122964465 0.383048698598465\n",
      "epoch 171 took 485.1s, optimizer loss=1.47, test mae=1.17\n",
      "1.4654802963621054 0.08028940091152423\n",
      "1.4652890378769714 0.07980083958452597\n",
      "1.4651227638150022 0.07266432950887047\n",
      "1.4649662868132762 0.07161211576650589\n",
      "1.4647752505328981 0.06144760725325118\n",
      "1.4646233718576391 0.05636525882071358\n",
      "1.4644777750548512 0.05479213129639736\n",
      "1.4643132576591757 0.04855366644056998\n",
      "1.4641665639601016 0.049119259210828096\n",
      "1.4640305937331104 0.048948985013446834\n",
      "1.4638645620598738 0.053275352287555804\n",
      "1.4637613355916346 0.053976865620678785\n",
      "1.4636451115830642 0.055566977733012675\n",
      "1.4634866491065477 0.05571586549874407\n",
      "1.4632958792755346 0.05714259637237278\n",
      "1.4630690083163684 0.056375474850089154\n",
      "1.4628181556204471 0.05673060316950545\n",
      "1.4625346147703064 0.059722320981815485\n",
      "1.4622335541146907 0.057204053036539936\n",
      "1.4619552987541748 0.058074984072461144\n",
      "1.4616541405819168 0.05363535225814845\n",
      "norms 28.886993416884863 37.241157853022216 38.65539014348937\n",
      "gradients 0.05363535225814845 0.2523966660254892 0.4263500032584609\n",
      "epoch 172 took 488.7s, optimizer loss=1.465, test mae=1.167\n",
      "1.4616541405819168 0.05363535225814845\n",
      "1.4614082244471072 0.052315385398134864\n",
      "1.461159992726766 0.04992863558399581\n",
      "1.4609185079538873 0.04982706718706229\n",
      "1.4606908479651262 0.04874775267406538\n",
      "1.4604386283979105 0.05564083336163005\n",
      "1.4601630797619585 0.05782974891945328\n",
      "1.4599292605454668 0.06309575538194581\n",
      "1.4596882073705761 0.0721671760140717\n",
      "1.4594272876266605 0.07539452081864834\n",
      "1.4591894620844101 0.08184701931879891\n",
      "1.4589245601858931 0.08208976369415673\n",
      "1.4586772551699536 0.0845092048716842\n",
      "1.458410430900471 0.07882103456598438\n",
      "1.4581447573801374 0.0802255351706268\n",
      "1.4578835265222634 0.07656403885330543\n",
      "1.4576211614924939 0.07565949825353317\n",
      "1.4573722465738814 0.0687167095347028\n",
      "1.4571437633257827 0.06542821176658212\n",
      "1.4569193619020615 0.058787881331060686\n",
      "1.4567677615411136 0.052725918718716056\n",
      "norms 28.92332310424551 37.24256619982264 38.67643298580079\n",
      "gradients 0.052725918718716056 0.23620212108394145 0.12889273976445845\n",
      "epoch 173 took 484.3s, optimizer loss=1.462, test mae=1.163\n",
      "1.4567677615411136 0.052725918718716056\n",
      "1.4566374460682485 0.04839854114724815\n",
      "1.4564863193673323 0.057283817220589756\n",
      "1.456357804165156 0.05388973915414917\n",
      "1.4562293479187556 0.05172075621448185\n",
      "1.4560749331406073 0.056832439324569384\n",
      "1.4559459985062662 0.05504753813758186\n",
      "1.4558000587966542 0.06029962670916762\n",
      "1.4556309959210902 0.05689372229881749\n",
      "1.4554726661690043 0.060459194150267376\n",
      "1.4552949939429691 0.060282277380757816\n",
      "1.4551614298022821 0.05789207440597596\n",
      "1.4550254422101336 0.05719389018523493\n",
      "1.4548627957652787 0.04891739964359528\n",
      "1.4546983559100732 0.047789664029709605\n",
      "1.4545307316000766 0.06063025997339112\n",
      "1.4544117634706077 0.055711528561994954\n",
      "1.4542784914649256 0.05097402590924408\n",
      "1.4541115021725817 0.04722871758563584\n",
      "1.4539188610555653 0.044070789730751674\n",
      "1.4537295487869566 0.04579263453511624\n",
      "norms 28.934529643856862 37.240402254951576 38.68007022979688\n",
      "gradients 0.04579263453511624 0.23262681856206635 0.3780677214825217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 174 took 488.9s, optimizer loss=1.457, test mae=1.162\n",
      "1.4537295487869566 0.04579263453511624\n",
      "1.4535544510469383 0.046977526602945914\n",
      "1.4533675059457327 0.05409580863426153\n",
      "1.4531517050596303 0.05840087655116704\n",
      "1.452964132359936 0.06259773028125931\n",
      "1.4527738452173262 0.06776063031612481\n",
      "1.4525767843610458 0.0713094800132986\n",
      "1.4523915700318808 0.07197814683243915\n",
      "1.4522162593791226 0.07240259927422237\n",
      "1.4520142171889487 0.06975002333094887\n",
      "1.4518035997389123 0.06671725953594208\n",
      "1.45154213806178 0.060968555469049036\n",
      "1.4512141440496693 0.0580688075866757\n",
      "1.4508948054894577 0.04662647704858747\n",
      "1.4506564149808596 0.04292625207259306\n",
      "1.4504411795799221 0.035970263907930934\n",
      "1.4502441352368611 0.036959169834763404\n",
      "1.4500604501718621 0.03284595756517397\n",
      "1.4499226723242535 0.03418145792939562\n",
      "1.449804622242658 0.03561458307678161\n",
      "1.449662327318779 0.045042950007247846\n",
      "norms 28.94346568691264 37.234611979782535 38.68105099115168\n",
      "gradients 0.045042950007247846 0.23306996729501023 0.20079328977989971\n",
      "epoch 175 took 482.1s, optimizer loss=1.454, test mae=1.16\n",
      "1.449662327318779 0.045042950007247846\n",
      "1.4495619080427289 0.0464464053802641\n",
      "1.4494553137209647 0.04789777219734676\n",
      "1.4493316803597063 0.04746972818961405\n",
      "1.4492187012442403 0.04797981502033461\n",
      "1.4491020354049555 0.041942135240540725\n",
      "1.449000886023336 0.042024916542833246\n",
      "1.4489057775472245 0.037494538622422566\n",
      "1.448805537186203 0.03846375032140552\n",
      "1.448713795143476 0.034977924231218385\n",
      "1.448615204395503 0.039328529685093816\n",
      "1.4485328512588045 0.0369604368967498\n",
      "1.4484484485500082 0.03632266562181499\n",
      "1.4483359062499248 0.05837992587753626\n",
      "1.4482583574930874 0.05556939442563284\n",
      "1.4481697114593035 0.05490125082935598\n",
      "1.448061803126473 0.057343570708816495\n",
      "1.4479354565608735 0.05708328266742677\n",
      "1.4478343228802493 0.059285055459549714\n",
      "1.4477005113351038 0.06232577746217194\n",
      "1.4475363569807869 0.06548350707820536\n",
      "norms 28.946803197813356 37.23103116564819 38.680326826672655\n",
      "gradients 0.06548350707820536 0.22693460379402697 0.14347009464882451\n",
      "epoch 176 took 487.6s, optimizer loss=1.45, test mae=1.159\n",
      "1.4475363569807869 0.06548350707820536\n",
      "1.447336440790801 0.07205641379854687\n",
      "1.4470962344882985 0.0744807022738424\n",
      "1.4468335829144119 0.0936028277544134\n",
      "1.4465971944138722 0.0946470264966352\n",
      "1.4463199635752186 0.09582953920478009\n",
      "1.4459564840785757 0.0975303563354629\n",
      "1.4456542227533054 0.09437785542400016\n",
      "1.4453669703975658 0.0879671304712836\n",
      "1.4450640165369588 0.08920385757553054\n",
      "1.4447406604546207 0.06767258926207785\n",
      "1.4444919278355062 0.06325412059288625\n",
      "1.444212137204457 0.047671537799356796\n",
      "1.4439742397021087 0.044355216351378\n",
      "1.4436728993771286 0.06828113683625167\n",
      "1.443406561706692 0.06216139915089655\n",
      "1.4431334742869588 0.060374894798559765\n",
      "1.4428339491007516 0.054131712557693834\n",
      "1.4425161424194843 0.061136044911295304\n",
      "1.4422088207179766 0.08232034406888708\n",
      "1.4419929364611295 0.07777498942696397\n",
      "norms 28.960588372026326 37.22584599725832 38.68447718404797\n",
      "gradients 0.07777498942696397 0.2600160181559049 0.4576683029343525\n",
      "epoch 177 took 487.0s, optimizer loss=1.448, test mae=1.154\n",
      "1.4419929364611295 0.07777498942696397\n",
      "1.441778968244137 0.07495754445322572\n",
      "1.441541712348241 0.07997224628652816\n",
      "1.4413769949724495 0.07471458517674141\n",
      "1.4412138772863414 0.07125345042123502\n",
      "1.4410347900326639 0.06595647542261757\n",
      "1.4408536994675862 0.06303670120529045\n",
      "1.4406819762605878 0.0601069867037616\n",
      "1.4405182134538737 0.056876602227629774\n",
      "1.4403646654311846 0.05815256578255802\n",
      "1.440208362803261 0.054288303819952484\n",
      "1.4400564774508726 0.059485131451887226\n",
      "1.4399218376714862 0.057883633451451796\n",
      "1.439779024511782 0.06475589420804131\n",
      "1.43963237421333 0.06218071744991482\n",
      "1.4394687216255795 0.07283453321093718\n",
      "1.4393195307303026 0.07082059308509661\n",
      "1.4391385587445278 0.07626734510715977\n",
      "1.4389290413296676 0.07540262371077368\n",
      "1.4387152210660694 0.07749544576513522\n",
      "1.4384510734720883 0.07865375690735874\n",
      "norms 29.010570883438973 37.23269197716707 38.71737444701668\n",
      "gradients 0.07865375690735874 0.2601476467931095 0.6010319933578657\n",
      "epoch 178 took 487.0s, optimizer loss=1.442, test mae=1.155\n",
      "1.4384510734720883 0.07865375690735874\n",
      "1.4382347692606292 0.07647461528206619\n",
      "1.4380281426301842 0.07614611994310987\n",
      "1.437775076489195 0.07235786756990864\n",
      "1.4375372567658515 0.0695963342106984\n",
      "1.4373106380035015 0.06796665898383103\n",
      "1.437060399248435 0.06471075017413323\n",
      "1.4368186683567123 0.061379717492682014\n",
      "1.436593158177238 0.05914729229591335\n",
      "1.4363739988218907 0.0551243137711101\n",
      "1.4361442142336223 0.06318758387805505\n",
      "1.4359834436069905 0.058755463382380836\n",
      "1.4358235653740083 0.05654762925174094\n",
      "1.4356326980934442 0.05509879063189906\n",
      "1.4354712737406832 0.05531129577811852\n",
      "1.435300103411096 0.05319766855499695\n",
      "1.4351201800242452 0.05821702325255577\n",
      "1.4349491788964654 0.056833270474609315\n",
      "1.4347635383113577 0.06429675655557084\n",
      "1.4345860310845964 0.06414254222877612\n",
      "1.434397611371584 0.07076089961920333\n",
      "norms 29.03226259143619 37.23274427269102 38.72984784008418\n",
      "gradients 0.07076089961920333 0.25857447792371313 0.2823701279929619\n",
      "epoch 179 took 481.1s, optimizer loss=1.438, test mae=1.157\n",
      "1.434397611371584 0.07076089961920333\n",
      "1.4341852413788403 0.06910369249829137\n",
      "1.4340002870329684 0.07036715935213388\n",
      "1.4337948180559719 0.07042181607739621\n",
      "1.43360523013015 0.06806998273546676\n",
      "1.4333769771225902 0.0680720393276788\n",
      "1.4330844615603489 0.0633235131292419\n",
      "1.432817215588066 0.060628353946410425\n",
      "1.4325662007546633 0.07384258282851977\n",
      "1.4323965147663291 0.06580656456617792\n",
      "1.4322315432962496 0.05982777013223116\n",
      "1.432049539742169 0.05516305106843693\n",
      "1.431854180860712 0.052489608423674325\n",
      "1.4316450758656019 0.05523058457231764\n",
      "1.4314259289241469 0.057140930749249585\n",
      "1.4312434218248833 0.06253385394234078\n",
      "1.4310576668332498 0.0681575355708993\n",
      "1.4308502470614477 0.07268684315620182\n",
      "1.4306723884961574 0.07697233561209828\n",
      "1.430490022713739 0.07948951239854343\n",
      "1.4302551672825512 0.09557009196925456\n",
      "norms 29.036389523961414 37.230976826295304 38.732134372273315\n",
      "gradients 0.09557009196925456 0.25630649267914307 0.3035423571571604\n",
      "epoch 180 took 487.4s, optimizer loss=1.434, test mae=1.155\n",
      "1.4302551672825512 0.09557009196925456\n",
      "1.4300843423565286 0.09360216850860703\n",
      "1.429895359850906 0.09030732722797506\n",
      "1.4296468509013238 0.09090343558549517\n",
      "1.4294021947592879 0.08620060516543493\n",
      "1.4291720638301686 0.10409927834343545\n",
      "1.4289802259412707 0.09813803671196526\n",
      "1.4287766993696474 0.09243204145629601\n",
      "1.4285494383399948 0.08640037999552012\n",
      "1.4283069927872847 0.08090389238055666\n",
      "1.4280540282302085 0.07907808280263534\n",
      "1.4277840170741989 0.07107320757169724\n",
      "1.4275792647747434 0.07151609543172847\n",
      "1.427385504516679 0.06884219532222668\n",
      "1.427187528784041 0.06921510022504349\n",
      "1.4269761472184326 0.06748752976043845\n",
      "1.4267505146373185 0.06776927251424412\n",
      "1.4265156790709739 0.07253186205949097\n",
      "1.4263336495927261 0.06995966059156931\n",
      "1.4261479874041956 0.07072919498879603\n",
      "1.4258960861784034 0.07114917537110756\n",
      "norms 29.064105276211336 37.23267932340648 38.749922283512106\n",
      "gradients 0.07114917537110756 0.2330634488006044 1.0587807437588064\n",
      "epoch 181 took 486.1s, optimizer loss=1.43, test mae=1.155\n",
      "1.4258960861784034 0.07114917537110756\n",
      "1.4257318941181567 0.06910841953809328\n",
      "1.4255453737920902 0.06567933426574186\n",
      "1.4253087636745005 0.06771904732714079\n",
      "1.425017531156087 0.05601655692887348\n",
      "1.424800116588531 0.055463193234087604\n",
      "1.4245831606825443 0.05740278987290211\n",
      "1.4243232466069413 0.060503537956237315\n",
      "1.424114461397278 0.06230334353824085\n",
      "1.4239303537875718 0.06498489958403832\n",
      "1.4237140679459142 0.06993065719642554\n",
      "1.423567786568683 0.07051473851309512\n",
      "1.4233923854838566 0.07271654998201464\n",
      "1.4231669296156149 0.07365450711601682\n",
      "1.4229190793375788 0.07171885255166281\n",
      "1.4227482875389006 0.06881902961499091\n",
      "1.422574426520009 0.0677441648818778\n",
      "1.4223641415948627 0.060271869804758786\n",
      "1.42216993392748 0.0564438528281008\n",
      "1.4220086137006012 0.05347038424295651\n",
      "1.4218550709405233 0.050164711425121765\n",
      "norms 29.12249836161206 37.24369313424122 38.792280853455196\n",
      "gradients 0.050164711425121765 0.2430888201599461 0.5559641286320873\n",
      "epoch 182 took 485.3s, optimizer loss=1.426, test mae=1.156\n",
      "1.4218550709405233 0.050164711425121765\n",
      "1.4217211083183279 0.04657930753255599\n",
      "1.4215991354444437 0.045214364401050595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4214778827224597 0.042576617705204906\n",
      "1.4213385310265694 0.06454051611225023\n",
      "1.4212382138486197 0.061097389531118214\n",
      "1.421124625106334 0.05698506810659728\n",
      "1.42098341802059 0.055228596919286686\n",
      "1.4208399550370865 0.0537414859191728\n",
      "1.4206919072492679 0.05423798182591407\n",
      "1.4205301819316718 0.05685482903540912\n",
      "1.420355582447275 0.05875265319476413\n",
      "1.4201744363025302 0.0641958192863071\n",
      "1.4200127371088425 0.06592072996999118\n",
      "1.4198429754291408 0.0700675112336588\n",
      "1.4196715567296674 0.07149173472634808\n",
      "1.4194940023298195 0.07544469195378152\n",
      "1.4193146511399624 0.07542250782929771\n",
      "1.4191568770608625 0.07548150643673196\n",
      "1.4189708258224267 0.08072657013614568\n",
      "1.4188028388109226 0.07899502613730489\n",
      "norms 29.1070831699533 37.23535575075941 38.77864230118457\n",
      "gradients 0.07899502613730489 0.23141128639059186 0.5049779863045043\n",
      "epoch 183 took 484.1s, optimizer loss=1.422, test mae=1.153\n",
      "1.4188028388109226 0.07899502613730489\n",
      "1.4186285014316404 0.07761119853053643\n",
      "1.4184301448586396 0.07651643277457204\n",
      "1.4182636107287798 0.10470669595120276\n",
      "1.4181108687649766 0.09481114549769261\n",
      "1.4179581512965336 0.08780509868674413\n",
      "1.4178067612520504 0.08087243998462167\n",
      "1.41765826057265 0.07511555396914604\n",
      "1.4175153602695079 0.07038534611865764\n",
      "1.4173773018877496 0.06720680282991429\n",
      "1.417238824519868 0.06766577695394554\n",
      "1.4171082332420788 0.06701626890401342\n",
      "1.4170039085449735 0.06733846655493908\n",
      "1.4169090044573411 0.07017527427808927\n",
      "1.4168230337643286 0.0700068604085792\n",
      "1.4167298629936929 0.07477146932804195\n",
      "1.416632895836568 0.07382562295291092\n",
      "1.4165117192899221 0.07989329562748455\n",
      "1.416381237735142 0.07975008677168581\n",
      "1.416235572371188 0.07508072277983102\n",
      "1.416083824720014 0.07534409903423327\n",
      "norms 29.12705479086783 37.23837331675961 38.79363146166477\n",
      "gradients 0.07534409903423327 0.23051709645661125 0.6446393834571058\n",
      "epoch 184 took 483.8s, optimizer loss=1.419, test mae=1.154\n",
      "1.416083824720014 0.07534409903423327\n",
      "1.4159263251204903 0.07504225437887253\n",
      "1.4157386637870335 0.07035264212835395\n",
      "1.4155643278401044 0.07260693442585334\n",
      "1.4153738076110873 0.06746564048457446\n",
      "1.4151877204296774 0.06807875773973135\n",
      "1.4149922100098944 0.06708782936827097\n",
      "1.4147762920770857 0.06986615230704615\n",
      "1.414583602390898 0.07101315371519258\n",
      "1.4143844826228615 0.08000920805462254\n",
      "1.4142044876563344 0.08051467024339083\n",
      "1.4140298549331072 0.08647361968890778\n",
      "1.413844884022025 0.0853391715910326\n",
      "1.4136881160615293 0.0863512250797845\n",
      "1.4135238517368347 0.08512784832003908\n",
      "1.4133585638234454 0.08267681877698073\n",
      "1.4131846002764252 0.07909377161339888\n",
      "1.4130032430558626 0.07356171819825574\n",
      "1.4128344929442263 0.06884909364574747\n",
      "1.4126739422553074 0.06442997329538419\n",
      "1.4125291503935908 0.05875833449775235\n",
      "norms 29.150885351143447 37.24275868368314 38.81274750626134\n",
      "gradients 0.05875833449775235 0.21412590325578543 0.13141325367613998\n",
      "epoch 185 took 488.7s, optimizer loss=1.416, test mae=1.15\n",
      "1.4125291503935908 0.05875833449775235\n",
      "1.4124021585167497 0.057142729547357884\n",
      "1.4122611276170223 0.0531825363725613\n",
      "1.412143546316693 0.05222417660352334\n",
      "1.4120353809953767 0.05421027606807957\n",
      "1.4119095024203137 0.05714997062824721\n",
      "1.4118215940612868 0.05949924274599658\n",
      "1.4117188417846338 0.06387290816700328\n",
      "1.4115873517178883 0.06715478179143344\n",
      "1.4114573152702268 0.07123744679604833\n",
      "1.411311438146024 0.0752331937796543\n",
      "1.4111660499945413 0.07727817889783739\n",
      "1.4109999510845739 0.08105395802989285\n",
      "1.4108079857247569 0.08036675466342129\n",
      "1.410617526241271 0.08229209646946362\n",
      "1.4104241951099847 0.07991348282767145\n",
      "1.4101991371582239 0.0812908509730538\n",
      "1.4099540756101967 0.07698940501254482\n",
      "1.4097142257363064 0.07989842703055665\n",
      "1.409437558492739 0.069350007741886\n",
      "1.4092604483922113 0.06712107295882377\n",
      "norms 29.156726384026978 37.242748794147225 38.81795852980608\n",
      "gradients 0.06712107295882377 0.22891306803390382 0.1948911804941668\n",
      "epoch 186 took 487.2s, optimizer loss=1.413, test mae=1.146\n",
      "1.4092604483922113 0.06712107295882377\n",
      "1.4090941829648311 0.06274795355042413\n",
      "1.4089196991529978 0.060764620453446626\n",
      "1.4087535064260746 0.05606942385622367\n",
      "1.4085992989040048 0.054281367078913116\n",
      "1.4084427952920997 0.05354485066301333\n",
      "1.4083237619738114 0.05172029547473829\n",
      "1.4082073911430772 0.055803012741332855\n",
      "1.4080967315954669 0.05558394158791783\n",
      "1.4079946183825747 0.05531755947024108\n",
      "1.4078645010439963 0.059988956663774885\n",
      "1.407773226982735 0.058947960654358464\n",
      "1.4076764660444765 0.06048829754209645\n",
      "1.4075492552069961 0.05946223720101011\n",
      "1.4074016283401378 0.06090097540238931\n",
      "1.4072474922883278 0.06906168019670653\n",
      "1.4070990753746468 0.06678591834463413\n",
      "1.406971347957357 0.07001300819399969\n",
      "1.4068167705183516 0.0705484844050463\n",
      "1.4066801288193063 0.07155335839118047\n",
      "1.4065237576088714 0.077285050024494\n",
      "norms 29.180893394557067 37.2494849455262 38.839190331487636\n",
      "gradients 0.077285050024494 0.21353329160139803 0.3094873765452021\n",
      "epoch 187 took 486.5s, optimizer loss=1.409, test mae=1.144\n",
      "1.4065237576088714 0.077285050024494\n",
      "1.4063362972556235 0.07513656804034902\n",
      "1.4061461759433005 0.08028143253465146\n",
      "1.4059324508020952 0.0810257756217063\n",
      "1.4057790597455022 0.07474896359681897\n",
      "1.4056384908748896 0.07556501407581956\n",
      "1.4054919120108638 0.07063597248657875\n",
      "1.405341474115612 0.06954955722844\n",
      "1.4051801524113363 0.0645389819536012\n",
      "1.4050262698890998 0.062332303626160115\n",
      "1.4048691163998137 0.06222877996477145\n",
      "1.4046603132964934 0.0727155418670311\n",
      "1.4045161656954022 0.06792512999120019\n",
      "1.404365434237541 0.06477548226524185\n",
      "1.4041848259790546 0.061964227421576035\n",
      "1.40397488538788 0.06201693956687635\n",
      "1.4037726016818988 0.06100601846276237\n",
      "1.4036013808643504 0.061904999740531194\n",
      "1.4034331906475248 0.06544386151686149\n",
      "1.403267142002587 0.06440089881376962\n",
      "1.4031286531243217 0.06770007759343101\n",
      "norms 29.196416526475215 37.24601350702736 38.846577403870285\n",
      "gradients 0.06770007759343101 0.21104483753162082 0.1578595932139622\n",
      "epoch 188 took 486.0s, optimizer loss=1.407, test mae=1.145\n",
      "1.4031286531243217 0.06770007759343101\n",
      "1.4030023893779113 0.06770483218902909\n",
      "1.4028563940965506 0.07102813574455608\n",
      "1.402711082707172 0.06799562624768687\n",
      "1.402548504231266 0.07423469032929343\n",
      "1.4024207591642108 0.0718130357870572\n",
      "1.4022919117150314 0.07078341934025285\n",
      "1.4021126048786223 0.07078889401719878\n",
      "1.401955508881357 0.06728282805195641\n",
      "1.401797646781855 0.0684767140452861\n",
      "1.401634986094881 0.06694141537815744\n",
      "1.401486427278952 0.06818562728863178\n",
      "1.4013511023570653 0.07003140110007182\n",
      "1.4012204659297398 0.07178140678264514\n",
      "1.4010916644380584 0.07334387917960135\n",
      "1.4009575259813887 0.07675445041124897\n",
      "1.4008190892655443 0.07656678036482033\n",
      "1.4007023106336378 0.07966424097531555\n",
      "1.4005932910500982 0.08002363997878756\n",
      "1.4004695268436498 0.08199137161111221\n",
      "1.4003477572316945 0.08171215127091687\n",
      "norms 29.213257632536646 37.24742225954568 38.8582005138759\n",
      "gradients 0.08171215127091687 0.22556152319477119 0.15915018183985988\n",
      "epoch 189 took 488.6s, optimizer loss=1.403, test mae=1.143\n",
      "1.4003477572316945 0.08171215127091687\n",
      "1.4002160370284202 0.08292224126999134\n",
      "1.4000678951996255 0.08013754175230844\n",
      "1.3998956383360142 0.08672061074389596\n",
      "1.3997438857533604 0.08347453248368053\n",
      "1.3995883882260698 0.08101225154280503\n",
      "1.3994181889743686 0.07809034695222064\n",
      "1.3992629082612085 0.07218830633409314\n",
      "1.3991090322010278 0.07758272798886338\n",
      "1.3989804436381552 0.07032883262307388\n",
      "1.3988666291325573 0.06806898723940417\n",
      "1.3987315723329583 0.060957702663609566\n",
      "1.3986006199896226 0.05475299675444338\n",
      "1.3984129283409117 0.07861809295081575\n",
      "1.398333085795377 0.07196046738715234\n",
      "1.3982415483666208 0.06631598145391569\n",
      "1.3981279023672422 0.06092506574937574\n",
      "1.3979983702450298 0.05704307648751999\n",
      "1.3978577851086698 0.05514297420772263\n",
      "1.3977056549279634 0.05480050465667818\n",
      "1.3975370494913817 0.05885464484471282\n",
      "norms 29.242632891290683 37.252795427726504 38.880189254736386\n",
      "gradients 0.05885464484471282 0.23965273486921637 0.21664468276726032\n",
      "epoch 190 took 485.2s, optimizer loss=1.4, test mae=1.143\n",
      "1.3975370494913817 0.05885464484471282\n",
      "1.3973370123969482 0.06071030020277502\n",
      "1.3971801548138 0.06137015704928145\n",
      "1.3970359258066343 0.06591017977188177\n",
      "1.396854152559421 0.05896099500693423\n",
      "1.3967097975433886 0.06089958781716985\n",
      "1.396530672972839 0.06290793708680484\n",
      "1.3962805123821203 0.06237940327258847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3959661239124308 0.06981148804196628\n",
      "1.3957465261857618 0.06714041681559121\n",
      "1.3955209554846506 0.060461585389272376\n",
      "1.3952652486776387 0.06292332379980055\n",
      "1.3950185638139634 0.05116589396179426\n",
      "1.3947997259711715 0.05216313620807979\n",
      "1.3946040022701929 0.04707501154808879\n",
      "1.3944152842081001 0.052430664125008304\n",
      "1.3942026663629208 0.0506626800509568\n",
      "1.3940667042464776 0.05236170795645455\n",
      "1.3939416354978742 0.055257587066889095\n",
      "1.3937674564165248 0.060327917650562814\n",
      "1.3935548512019622 0.06015000846274578\n",
      "norms 29.269671774969154 37.25376135910214 38.89859246365046\n",
      "gradients 0.06015000846274578 0.22915190688060497 0.3079707101111541\n",
      "epoch 191 took 487.8s, optimizer loss=1.398, test mae=1.14\n",
      "1.3935548512019622 0.06015000846274578\n",
      "1.3933308889455198 0.0769797775177402\n",
      "1.393162699382926 0.07563775925314602\n",
      "1.3930009482437151 0.075888016639012\n",
      "1.3927930157541064 0.0797566048541671\n",
      "1.3926582632316709 0.07756646445072292\n",
      "1.3925007532687654 0.07510374820222945\n",
      "1.392309146176531 0.06924748348844019\n",
      "1.3920906851930241 0.07827712454869726\n",
      "1.3919068736305154 0.07171148029745436\n",
      "1.391723765151437 0.07180655201273481\n",
      "1.3914967303660835 0.06336429065854983\n",
      "1.3913473721807197 0.060315628055298165\n",
      "1.3912139863119284 0.05839044920217678\n",
      "1.3910656276516875 0.06902888145937164\n",
      "1.3909669477441415 0.06305725627412258\n",
      "1.3908657835239369 0.059410700197065\n",
      "1.3907607858589404 0.05521270855364922\n",
      "1.3906556188039882 0.05370412156613347\n",
      "1.3905503843896987 0.05320738939780108\n",
      "1.3904437410526338 0.05253781090514536\n",
      "norms 29.28556198337788 37.255392251500055 38.911127446877245\n",
      "gradients 0.05253781090514536 0.22308557734596562 0.15530033026163506\n",
      "epoch 192 took 486.3s, optimizer loss=1.394, test mae=1.137\n",
      "1.3904437410526338 0.05253781090514536\n",
      "1.39034497058735 0.05609984148130836\n",
      "1.390256456331498 0.05746019824628149\n",
      "1.3901736775725368 0.05801088047415606\n",
      "1.3900673741091525 0.06819839975358719\n",
      "1.3899853502320376 0.06854029390760558\n",
      "1.3898844355922384 0.06765372572021464\n",
      "1.3897364779982142 0.071699863186313\n",
      "1.3896124330419135 0.06694592172780621\n",
      "1.389466611613022 0.06742722021218786\n",
      "1.3892983545103599 0.0719248692257336\n",
      "1.389138722520978 0.06346733616566094\n",
      "1.3889928828797906 0.06209006689513403\n",
      "1.388814861284883 0.052908409227779396\n",
      "1.388660688999084 0.05088439164418958\n",
      "1.3884843903904236 0.04741730569248094\n",
      "1.388285543802525 0.04711274377211003\n",
      "1.3880613523834278 0.049527588857345324\n",
      "1.3878165512219953 0.05120884779511182\n",
      "1.3875711827089565 0.06843111291615683\n",
      "1.3873565174897167 0.06726987101147315\n",
      "norms 29.274002604267853 37.24944191392951 38.90227887691207\n",
      "gradients 0.06726987101147315 0.246151136446859 0.6274646895575242\n",
      "epoch 193 took 490.2s, optimizer loss=1.39, test mae=1.133\n",
      "1.3873565174897167 0.06726987101147315\n",
      "1.387158959815392 0.07180768550394642\n",
      "1.3869461966767682 0.0674203877981546\n",
      "1.3867754362482712 0.06738151184882259\n",
      "1.3865933105385577 0.06461753880413802\n",
      "1.3863753480646224 0.06547522120288214\n",
      "1.3862124316956215 0.06213981366968885\n",
      "1.3860337483852574 0.05970700734958887\n",
      "1.385822152284343 0.05525907991487029\n",
      "1.3855966967045796 0.05586693991805111\n",
      "1.3854080832777422 0.051058372139232\n",
      "1.385253641743049 0.05118835257279973\n",
      "1.3850890109742444 0.0515124110464518\n",
      "1.3849178113806717 0.05115828371980557\n",
      "1.3847448133122364 0.05463796361714467\n",
      "1.3845768009193535 0.05381283265535897\n",
      "1.3844119757669564 0.05770660634171636\n",
      "1.3842422008344577 0.05724433156257839\n",
      "1.3840602601417438 0.06238974682904036\n",
      "1.3839086338422562 0.06271285506086101\n",
      "1.3837728344151785 0.06312559369667936\n",
      "norms 29.318607548710446 37.25545794806917 38.93442123605743\n",
      "gradients 0.06312559369667936 0.2579079097256153 0.28485793517006536\n",
      "epoch 194 took 485.9s, optimizer loss=1.387, test mae=1.132\n",
      "1.3837728344151785 0.06312559369667936\n",
      "1.383615462002585 0.06516687931322618\n",
      "1.3834853812546468 0.06629724314222749\n",
      "1.3833567367156734 0.0622541777801877\n",
      "1.3832224141450407 0.0642013553960049\n",
      "1.3830960219243646 0.059863207133252334\n",
      "1.3829806038371357 0.05967457454502871\n",
      "1.3828700481526193 0.05758907191923743\n",
      "1.3827557967074855 0.0562944197319651\n",
      "1.3826256499213034 0.05813534908665538\n",
      "1.3824732637956476 0.05588403130965723\n",
      "1.3823500603496264 0.056872993149511865\n",
      "1.382219893312254 0.060538445176521356\n",
      "1.3820845474739683 0.06328143010010302\n",
      "1.381957196199405 0.06219480364185497\n",
      "1.3818257395561169 0.06709502936218517\n",
      "1.381690440343057 0.06830279944453288\n",
      "1.3815393498894852 0.06865566506811559\n",
      "1.381399613926848 0.07166037240164998\n",
      "1.3812465994416199 0.0701538699582013\n",
      "1.381080826698395 0.07223441901818234\n",
      "norms 29.314149638518682 37.24903193360122 38.92823429084194\n",
      "gradients 0.07223441901818234 0.23401949129074054 0.42144146342794986\n",
      "epoch 195 took 488.3s, optimizer loss=1.384, test mae=1.129\n",
      "1.381080826698395 0.07223441901818234\n",
      "1.3809019986962192 0.07184494743417365\n",
      "1.3807014694026587 0.07265305508239338\n",
      "1.3805362299837425 0.07393520158171692\n",
      "1.3803582515790271 0.07359046338487246\n",
      "1.3801327922570654 0.09480203262664297\n",
      "1.3799667757455873 0.09269067524293302\n",
      "1.3797825416283471 0.09204131282777642\n",
      "1.3795516907367513 0.0900224394428855\n",
      "1.379288607169746 0.09256186366929713\n",
      "1.379012603067081 0.0876830859297701\n",
      "1.3788128341218806 0.08629328366962245\n",
      "1.3786101282294652 0.08568179345449109\n",
      "1.3783907976156562 0.08230341707734028\n",
      "1.3782067385285122 0.07890632293857239\n",
      "1.3780342725674761 0.07746889930260903\n",
      "1.3778629146780228 0.0692902475988887\n",
      "1.377695961266157 0.06976382317163934\n",
      "1.377568258234543 0.06466396911081634\n",
      "1.377444298814095 0.06117228523141573\n",
      "1.3773228433132911 0.05924189368719517\n",
      "norms 29.312601570205523 37.2421443740807 38.924787856090404\n",
      "gradients 0.05924189368719517 0.21246339535765754 0.5284739074379814\n",
      "epoch 196 took 487.5s, optimizer loss=1.381, test mae=1.128\n",
      "1.3773228433132911 0.05924189368719517\n",
      "1.3772189331939089 0.05528028449058428\n",
      "1.3771137077301425 0.053967828881415034\n",
      "1.3769681732698826 0.05192884206292408\n",
      "1.3767698040885956 0.05152585485088706\n",
      "1.3765156179494131 0.055331015317610775\n",
      "1.3762298006299967 0.054912423651862365\n",
      "1.3759161583710802 0.06180290761656595\n",
      "1.375574957721678 0.06297082184550735\n",
      "1.3752408900758077 0.06886798903120683\n",
      "1.3749764656945613 0.0704279593042394\n",
      "1.3747488699406287 0.066808904842538\n",
      "1.3745256521097964 0.06992817692102403\n",
      "1.374312861837081 0.06980034730973395\n",
      "1.3741513632832416 0.05775238958695809\n",
      "1.3740197125515228 0.056000059375845554\n",
      "1.373909363577693 0.04955469162171428\n",
      "1.3738052365745008 0.046577976002890056\n",
      "1.3737127142828567 0.043985405162511475\n",
      "1.3736294590284999 0.041542823223266016\n",
      "1.3735552603226784 0.03997525724488439\n",
      "norms 29.308922329833017 37.236745497221065 38.9208194323771\n",
      "gradients 0.03997525724488439 0.19878268542819053 0.13960530680112587\n",
      "epoch 197 took 489.5s, optimizer loss=1.377, test mae=1.127\n",
      "1.3735552603226784 0.03997525724488439\n",
      "1.3734842638921028 0.0397776826254433\n",
      "1.373400940772439 0.04088387680590879\n",
      "1.3733166456266737 0.04138104672230831\n",
      "1.37320840120734 0.05121295475290828\n",
      "1.3731306255019198 0.05070327839697681\n",
      "1.3729949042330098 0.056935096900706854\n",
      "1.3728602979241875 0.0553246340317069\n",
      "1.3727360291036124 0.09609577329060787\n",
      "1.3726173593990607 0.08612739751631246\n",
      "1.3724884319674382 0.07582228936209848\n",
      "1.37236081513736 0.06419302755858376\n",
      "1.3722436563929294 0.05465887085299056\n",
      "1.372139419387408 0.04641466685049958\n",
      "1.3720462906725037 0.04100573862672733\n",
      "1.3719591681186916 0.0404736915603195\n",
      "1.371881934269165 0.04041073685298232\n",
      "1.3718217430204172 0.04298553787796517\n",
      "1.3717544810000786 0.04692111457786648\n",
      "1.3716760176936873 0.05063549810169174\n",
      "1.371581422176758 0.05827305514092571\n",
      "norms 29.303664073870166 37.23233565314738 38.91580734602876\n",
      "gradients 0.05827305514092571 0.20275330335514885 0.5778500032793304\n",
      "epoch 198 took 485.9s, optimizer loss=1.374, test mae=1.126\n",
      "1.371581422176758 0.05827305514092571\n",
      "1.3714767792018103 0.06349380172716834\n",
      "1.3713636737986532 0.06740483767379747\n",
      "1.3712416785910655 0.07280370839751427\n",
      "1.3711173440236213 0.07722023263461722\n",
      "1.3709714304032752 0.07574825907799553\n",
      "1.370856202127597 0.0779282541501625\n",
      "1.3707207477747725 0.07564075375882424\n",
      "1.3705716652619069 0.07324778229330647\n",
      "1.3704027419638318 0.0777134068857491\n",
      "1.370242901104497 0.0720426666789545\n",
      "1.370082915581934 0.07270898480504077\n",
      "1.3699273155925447 0.06614643839495929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3697913082833566 0.06404730364577235\n",
      "1.3696277748146768 0.061479342111179835\n",
      "1.369521331941917 0.059201363350891094\n",
      "1.3693987374642593 0.05859209942935003\n",
      "1.3692401515072508 0.05733901169010798\n",
      "1.3690672550880436 0.05833728451959215\n",
      "1.3689101101027261 0.06006355081703784\n",
      "1.3687461270723589 0.06566625448651693\n",
      "norms 29.334831115258208 37.23885849577859 38.94166474794243\n",
      "gradients 0.06566625448651693 0.2096672961093449 0.7191418064115552\n",
      "epoch 199 took 483.3s, optimizer loss=1.372, test mae=1.124\n",
      "1.3687461270723589 0.06566625448651693\n",
      "1.3685967569626674 0.06267961234814362\n",
      "1.3684533965452421 0.07293671959898095\n",
      "1.36832398222988 0.06886340008904106\n",
      "1.3682111505771353 0.07322919089786026\n",
      "1.3680963796925343 0.07272918725237\n",
      "1.367974534824231 0.0736029978918353\n",
      "1.3678607155944755 0.07469582527715818\n",
      "1.3677431573031207 0.07269546966850886\n",
      "1.3676260966872895 0.07356765437003991\n",
      "1.3674865333315083 0.07326390595706408\n",
      "1.3673888970473338 0.07109008034270858\n",
      "1.3672900033167632 0.06854751217678003\n",
      "1.3671603790805553 0.07121254113633427\n",
      "1.367057727505524 0.06686186906106807\n",
      "1.3669553722886065 0.06542447482954855\n",
      "1.3668207676730069 0.061282448420882894\n",
      "1.3667309231585643 0.058604105875426915\n",
      "1.3666191308039737 0.057005050000999334\n",
      "1.3664617113412263 0.05419356096155489\n",
      "1.366344014770984 0.050903054413750294\n",
      "norms 29.347875968250484 37.24040971247373 38.95168431361444\n",
      "gradients 0.050903054413750294 0.20958699788251486 0.42147298906562525\n",
      "epoch 200 took 485.8s, optimizer loss=1.369, test mae=1.124\n",
      "1.366344014770984 0.050903054413750294\n",
      "1.3662240780733557 0.049032586985620415\n",
      "1.366082029688067 0.048453954557246444\n",
      "1.3659246340233537 0.04722763130655131\n",
      "1.3657759379123178 0.04882963332277449\n",
      "1.365623950996754 0.05048892755742349\n",
      "1.365481677670534 0.052399803498293526\n",
      "1.3653334352092226 0.056598730766351815\n",
      "1.365180971538712 0.05805241324309005\n",
      "1.3650489131032666 0.05896666727099613\n",
      "1.3648956487514403 0.06312752822432653\n",
      "1.3647550127645531 0.06339926356899256\n",
      "1.3646189334321834 0.07431018516056713\n",
      "1.3645074412476355 0.06994782556184707\n",
      "1.3643830311953222 0.06387015888413598\n",
      "1.3642479702432015 0.06185693759313424\n",
      "1.3641136957349784 0.05486568473715354\n",
      "1.3639916305549273 0.05345771333116377\n",
      "1.3638792240300526 0.05015346716937583\n",
      "1.363772868075821 0.04917244649862993\n",
      "1.3636606034807086 0.04870607522883694\n",
      "norms 29.361324306951712 37.24019473973233 38.961138072559166\n",
      "gradients 0.04870607522883694 0.21369060278404747 0.3176486602701884\n",
      "epoch 201 took 486.6s, optimizer loss=1.366, test mae=1.12\n",
      "1.3636606034807086 0.04870607522883694\n",
      "1.3635385812641607 0.04923019623872354\n",
      "1.363410560612282 0.0579027045446199\n",
      "1.3632880357119863 0.060511447679923024\n",
      "1.363172353907014 0.06328401127935301\n",
      "1.363029311392546 0.07331707367007766\n",
      "1.3629115928731146 0.07448407572462272\n",
      "1.3627893167864942 0.07653478685267974\n",
      "1.3626242438337617 0.08226416282416646\n",
      "1.3625043938046144 0.08182624906936634\n",
      "1.362371359512319 0.08190042848037186\n",
      "1.3621937674874764 0.07713977178296465\n",
      "1.3620656816624783 0.07614447624077271\n",
      "1.3618996374748313 0.07274821890206377\n",
      "1.3616963870201029 0.07432296668379057\n",
      "1.3614966850746486 0.07199726738925041\n",
      "1.3613309342872415 0.0709131002667665\n",
      "1.3611808690538427 0.07173128200287131\n",
      "1.3610386138944153 0.06895005546325209\n",
      "1.3609162735612406 0.06937486485996142\n",
      "1.3608023994126646 0.06847349039825319\n",
      "norms 29.375536695495423 37.24244161360121 38.973417861771225\n",
      "gradients 0.06847349039825319 0.21776803904846573 0.14535494016019723\n",
      "epoch 202 took 483.7s, optimizer loss=1.364, test mae=1.117\n",
      "1.3608023994126646 0.06847349039825319\n",
      "1.360690314473364 0.06768861183234635\n",
      "1.3605750377403774 0.06923321450101742\n",
      "1.3604752962447095 0.06869270725871239\n",
      "1.3603726416617026 0.06882568690686236\n",
      "1.3602474132675513 0.06975811176858529\n",
      "1.3601160962226924 0.06740104031253703\n",
      "1.3599937789032754 0.07228464122344884\n",
      "1.359861127400574 0.06608435359312374\n",
      "1.3597392111062618 0.06919515814407541\n",
      "1.3596164178798937 0.06745443932617202\n",
      "1.3594566781628066 0.06796527084085296\n",
      "1.3593118934728292 0.07081115513531555\n",
      "1.3591580308881441 0.06281800973465018\n",
      "1.3589979988495573 0.06644639700049135\n",
      "1.3588483028004663 0.0594164831433088\n",
      "1.3587190787797976 0.05909963597918567\n",
      "1.3585835447898744 0.05636152649803385\n",
      "1.358435562063063 0.05516554144135186\n",
      "1.3582766680232938 0.05480146068038829\n",
      "1.3580941303824745 0.05669014483277115\n",
      "norms 29.375506739151877 37.23731472640914 38.97228783963722\n",
      "gradients 0.05669014483277115 0.2124768009547825 0.4634994055735309\n",
      "epoch 203 took 483.4s, optimizer loss=1.361, test mae=1.116\n",
      "1.3580941303824745 0.05669014483277115\n",
      "1.3579044133220637 0.056471040693092445\n",
      "1.3577365061634303 0.059067544503442725\n",
      "1.3575844907090242 0.06078432014079477\n",
      "1.3574510046482158 0.06278800727939718\n",
      "1.3573219483917505 0.06327928406903367\n",
      "1.3571939136657172 0.06442029232091258\n",
      "1.3570629799803196 0.06507968680145582\n",
      "1.3569258356635014 0.06350047395842733\n",
      "1.3567962554478827 0.06372351799903803\n",
      "1.356654854410654 0.06215684969638976\n",
      "1.356526346435482 0.060622571147617335\n",
      "1.3563774318846358 0.05922823682137173\n",
      "1.3561815650793683 0.05911776489967024\n",
      "1.3559606625826985 0.05484503449753396\n",
      "1.3557893579838314 0.053325305707531655\n",
      "1.3556114460692472 0.05374399518890933\n",
      "1.3554317297613576 0.05404093408623431\n",
      "1.3552391113510536 0.049320079887032076\n",
      "1.3551011430085282 0.05208819336218897\n",
      "1.3549715894524021 0.051583601208843644\n",
      "norms 29.394758125650714 37.240794524365356 38.98898153379789\n",
      "gradients 0.051583601208843644 0.2307785602553814 0.523136987928838\n",
      "epoch 204 took 481.8s, optimizer loss=1.358, test mae=1.11\n",
      "1.3549715894524021 0.051583601208843644\n",
      "1.3547993225791681 0.05625138918862366\n",
      "1.354600463376996 0.05549219343512149\n",
      "1.3543657592237852 0.057106232565656376\n",
      "1.3540889534999623 0.06094180559331099\n",
      "1.3538072289847454 0.052250679199507426\n",
      "1.3535782236044156 0.05161018705159136\n",
      "1.3533523399599043 0.047199944635547685\n",
      "1.3531031223459404 0.04776767095517314\n",
      "1.3528968400400654 0.04377426161019421\n",
      "1.35270633880191 0.04588516105574574\n",
      "1.352526199566902 0.04549736221978868\n",
      "1.3523532300505718 0.05648832342211662\n",
      "1.3522381467053166 0.05833632994932339\n",
      "1.352135533144653 0.05923927514651136\n",
      "1.3520214040658973 0.06139375256844341\n",
      "1.351879140930096 0.06512668700663816\n",
      "1.351755866012686 0.06786933927028052\n",
      "1.351646074480895 0.07057975006540533\n",
      "1.3515187894529235 0.0768503738379837\n",
      "1.3513704734275171 0.07511604954541178\n",
      "norms 29.398792971012256 37.23773102838662 38.99142413033305\n",
      "gradients 0.07511604954541178 0.23010163100034473 0.19180642258409825\n",
      "epoch 205 took 482.7s, optimizer loss=1.355, test mae=1.11\n",
      "1.3513704734275171 0.07511604954541178\n",
      "1.3512324383893926 0.07815828220949393\n",
      "1.3510725591776225 0.07812509296315602\n",
      "1.3509185962778756 0.07471421542854866\n",
      "1.3507489206173504 0.07716324213890174\n",
      "1.3506217349765501 0.0726010239434354\n",
      "1.350472552676605 0.06926110901627534\n",
      "1.3502848611477039 0.06189409797416028\n",
      "1.3500934876709485 0.05612993258379538\n",
      "1.349924174574055 0.05066008900436753\n",
      "1.3497670562862039 0.04685060016088469\n",
      "1.3496081965181772 0.04742473338735701\n",
      "1.3494501235602145 0.0451281445861022\n",
      "1.3493247141857985 0.04564638759178669\n",
      "1.3492212266831398 0.04787128565388273\n",
      "1.3491202249547263 0.050892440150088085\n",
      "1.3490078826712042 0.053902710169831745\n",
      "1.3488784989100955 0.05754492461874705\n",
      "1.348765040408705 0.05925931408117575\n",
      "1.3486505104957793 0.06351868591693899\n",
      "1.3485171418041046 0.06387982006510487\n",
      "norms 29.40982213635013 37.23830489323548 39.00067386762362\n",
      "gradients 0.06387982006510487 0.21904936176607617 0.10491068311272207\n",
      "epoch 206 took 483.2s, optimizer loss=1.351, test mae=1.109\n",
      "1.3485171418041046 0.06387982006510487\n",
      "1.3484006189913413 0.06482971113790122\n",
      "1.3482749750474323 0.06629823945912028\n",
      "1.3481194746280494 0.06444845119271354\n",
      "1.3479739486811448 0.06474720575752514\n",
      "1.3478329418743833 0.06904186346851353\n",
      "1.3477022920484343 0.06614687435643893\n",
      "1.3475838432402054 0.06990456821825478\n",
      "1.3474575524283794 0.0638097047878846\n",
      "1.3473615479828742 0.06442744979723906\n",
      "1.3472793281057605 0.06488932486845947\n",
      "1.3471676384047873 0.07725270592590026\n",
      "1.3470907558982013 0.07891301408581183\n",
      "1.3469583723348475 0.07540597009605773\n",
      "1.3467937041927898 0.07326233628497646\n",
      "1.3465917936907066 0.09005700792362716\n",
      "1.3464021658724266 0.08536023914938659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3461940555227594 0.08147570090555997\n",
      "1.345957898179557 0.07510350043894784\n",
      "1.3457357284819944 0.07497566305601074\n",
      "1.345527509158964 0.06784887973317982\n",
      "norms 29.420315474342182 37.24038160385353 39.011523941689134\n",
      "gradients 0.06784887973317982 0.20678163394797847 0.1577263849528417\n",
      "epoch 207 took 490.3s, optimizer loss=1.349, test mae=1.108\n",
      "1.345527509158964 0.06784887973317982\n",
      "1.3453534519997912 0.06735161129794401\n",
      "1.3451828560439503 0.07220419020762099\n",
      "1.345048494032442 0.06895513097368515\n",
      "1.3449190311475674 0.06752473344477207\n",
      "1.3447611987607395 0.06609001692906319\n",
      "1.3445791149578117 0.06470129439804503\n",
      "1.3443840832526792 0.066580436507993\n",
      "1.3441798591679177 0.06421959395350241\n",
      "1.3439786615240765 0.06699634255279509\n",
      "1.3437797758983783 0.0627247731054311\n",
      "1.3435947487147109 0.06458456863010482\n",
      "1.343420658307152 0.05987391631535731\n",
      "1.3432623578809242 0.05932228825783687\n",
      "1.3431128440180007 0.06140322497714016\n",
      "1.342970937751382 0.05402943182380459\n",
      "1.3428685972009462 0.054976045728890054\n",
      "1.3427688823445683 0.052864945405004506\n",
      "1.3426696545554822 0.05071620864641746\n",
      "1.342569250219915 0.05447249799717723\n",
      "1.3424814322205176 0.05371075727605405\n",
      "norms 29.43690549688271 37.24370014193763 39.027239360810704\n",
      "gradients 0.05371075727605405 0.2145067082561218 0.13780224730742568\n",
      "epoch 208 took 487.7s, optimizer loss=1.346, test mae=1.107\n",
      "1.3424814322205176 0.05371075727605405\n",
      "1.3423849714165015 0.059810531491502984\n",
      "1.342307665399994 0.06225301027666082\n",
      "1.3422100126978218 0.06258383280204252\n",
      "1.3420864792240623 0.06541463455368086\n",
      "1.3419504298083527 0.06786948511089431\n",
      "1.3418097372643274 0.07036688976203843\n",
      "1.3416631268976755 0.07254402339802252\n",
      "1.3414966978466998 0.07849083928524542\n",
      "1.3413732284362758 0.07771555281162577\n",
      "1.341250118070702 0.07948543296949206\n",
      "1.3411061054878914 0.0771978538440807\n",
      "1.3409917447257962 0.07408901456008309\n",
      "1.34088342151168 0.07215193109649766\n",
      "1.340754439264837 0.0689842535504192\n",
      "1.3406588289869816 0.06504757721104186\n",
      "1.3405534560905426 0.06258361112475466\n",
      "1.340420667202632 0.05596669306384316\n",
      "1.3402791452181713 0.053557151693613574\n",
      "1.340168951076005 0.05017064560267534\n",
      "1.3400504964506377 0.052919876871111216\n",
      "norms 29.435480044191447 37.241131811679786 39.0268018751507\n",
      "gradients 0.052919876871111216 0.21242728821630966 0.22854569712980943\n",
      "epoch 209 took 487.2s, optimizer loss=1.342, test mae=1.105\n",
      "1.3400504964506377 0.052919876871111216\n",
      "1.339936492467551 0.050826620848681336\n",
      "1.3398223955385282 0.051878653489707945\n",
      "1.3396615252400563 0.050312488171043813\n",
      "1.3395116431009317 0.053111961819540074\n",
      "1.3393419898130379 0.054804155653785686\n",
      "1.3391531647732275 0.057595498391379944\n",
      "1.3389727315960682 0.05836716627723407\n",
      "1.3388019582318906 0.05951147177577859\n",
      "1.3386358897609074 0.05902460610022077\n",
      "1.338479091987966 0.0586534261266782\n",
      "1.3383238888768083 0.05922266731864775\n",
      "1.3381656355684801 0.05571138097588024\n",
      "1.3380190602753474 0.0568725482008787\n",
      "1.3379073685077425 0.05576774700495376\n",
      "1.3378102559270586 0.05387153412938141\n",
      "1.3377198179652956 0.05409201822653715\n",
      "1.337627992657249 0.054010340739582156\n",
      "1.33753327889905 0.05413773919713334\n",
      "1.3374239245482806 0.05738373736143394\n",
      "1.3373135566616692 0.05756471282453382\n",
      "norms 29.45698302686195 37.24663901921075 39.04672268784831\n",
      "gradients 0.05756471282453382 0.19743683092787873 0.38251731682876683\n",
      "epoch 210 took 482.8s, optimizer loss=1.34, test mae=1.103\n",
      "1.3373135566616692 0.05756471282453382\n",
      "1.3372033033302397 0.05796569277743122\n",
      "1.33706805754375 0.058737571252308975\n",
      "1.336914272281313 0.059206328110712345\n",
      "1.3367632639898688 0.05577399225279214\n",
      "1.3366391927921748 0.05630953574199144\n",
      "1.3365202510869378 0.052941399865105536\n",
      "1.336405226701875 0.05089289889112624\n",
      "1.3362905987206715 0.05669320022448897\n",
      "1.3362079847058297 0.0519111404285826\n",
      "1.3361253927106693 0.04702437205906313\n",
      "1.3360300800676472 0.042812945648039585\n",
      "1.335927131000374 0.03936796664703322\n",
      "1.3358232664066148 0.03718365227372038\n",
      "1.335726067384261 0.03834444538008142\n",
      "1.3356286872808838 0.04021295743485189\n",
      "1.3355329417033275 0.04402305293957519\n",
      "1.3354417349596477 0.047703133514618454\n",
      "1.3353411389792424 0.052776783550841186\n",
      "1.3352352901980498 0.05569975367871984\n",
      "1.3351119390945572 0.061569211820445315\n",
      "norms 29.463321720528437 37.24793715614016 39.053329672786255\n",
      "gradients 0.061569211820445315 0.19777109222398073 0.27054603972820856\n",
      "epoch 211 took 481.6s, optimizer loss=1.337, test mae=1.104\n",
      "1.3351119390945572 0.061569211820445315\n",
      "1.3349752087122082 0.06330044902524606\n",
      "1.3348267838566268 0.06556968815626119\n",
      "1.334722200715588 0.06580744257439841\n",
      "1.334607790959318 0.06658539832624707\n",
      "1.3344858610337555 0.06437732241496397\n",
      "1.334358307869589 0.0656580606025226\n",
      "1.3342337839383518 0.06138053940270635\n",
      "1.334125777277046 0.05864611107975403\n",
      "1.3340054548005904 0.056302932483187094\n",
      "1.3338992742083047 0.05279469470029664\n",
      "1.3337925643902513 0.055427447534899164\n",
      "1.3336829919524276 0.052112649290072234\n",
      "1.3335848145243248 0.05189227469585176\n",
      "1.333473917386744 0.054160136446695414\n",
      "1.3333845044333188 0.05340299269618657\n",
      "1.333297394733973 0.054752195392621675\n",
      "1.3332010588797127 0.0543513939121486\n",
      "1.3330921143181933 0.05518908851877092\n",
      "1.3329723470488857 0.05362784731204019\n",
      "1.33284641289502 0.05567520733402974\n",
      "norms 29.47305705260098 37.25010403689144 39.06344026045694\n",
      "gradients 0.05567520733402974 0.19548635253605287 0.21400555220452971\n",
      "epoch 212 took 484.5s, optimizer loss=1.335, test mae=1.1\n",
      "1.33284641289502 0.05567520733402974\n",
      "1.332734033256873 0.05317618168953632\n",
      "1.332616242922741 0.0531519101328494\n",
      "1.3324808611213321 0.04945673750807845\n",
      "1.332350742571824 0.04887739709448778\n",
      "1.3322315634712072 0.04636140480953997\n",
      "1.3321142565977369 0.04798482342925887\n",
      "1.3319971783087783 0.045191021634502664\n",
      "1.3318834874137908 0.05099765659396084\n",
      "1.331764804479969 0.04956660207734717\n",
      "1.3316800712340862 0.05235119126554179\n",
      "1.331591616714934 0.0564279300060442\n",
      "1.331489272939186 0.0584958404842562\n",
      "1.3314015483140824 0.06183129166805856\n",
      "1.3313094866850954 0.06258415517145752\n",
      "1.3311964810336319 0.06900734786872174\n",
      "1.3311002969983738 0.06818931653355124\n",
      "1.3309967495854993 0.06895831468805375\n",
      "1.330856481060197 0.06852663916872895\n",
      "1.3307561461158903 0.06648018091612792\n",
      "1.3306473966113535 0.06430514794575859\n",
      "norms 29.47810637627394 37.250939405834565 39.06891528440662\n",
      "gradients 0.06430514794575859 0.20084798752292435 0.2212532782301645\n",
      "epoch 213 took 481.4s, optimizer loss=1.333, test mae=1.101\n",
      "1.3306473966113535 0.06430514794575859\n",
      "1.330544108113224 0.09528160454938873\n",
      "1.3304433958074504 0.0853723659068081\n",
      "1.330332798152813 0.07561646282213912\n",
      "1.3302184021272385 0.06639819406997623\n",
      "1.33010518934117 0.058909879795481145\n",
      "1.3299962862825623 0.05374504496354964\n",
      "1.329891741203676 0.0512474787876184\n",
      "1.329790404391812 0.05103604746680343\n",
      "1.3296901750297465 0.05281902469167766\n",
      "1.3295879649366729 0.05532023464564434\n",
      "1.3294799304420775 0.06073851974007562\n",
      "1.3293634124471947 0.06437134817565542\n",
      "1.329278554847849 0.06731619772563892\n",
      "1.329183920952182 0.07754609987699616\n",
      "1.329095647787177 0.07766041046790753\n",
      "1.3289765644511522 0.0892815977599996\n",
      "1.3288698704868471 0.08880037000432005\n",
      "1.3287226142108899 0.08692410790591999\n",
      "1.3285157360216506 0.0872559720974285\n",
      "1.3282952146822569 0.08106063804827429\n",
      "norms 29.45936182175043 37.24187646719121 39.0544854908691\n",
      "gradients 0.08106063804827429 0.19817396712201144 0.6176451367605721\n",
      "epoch 214 took 513.9s, optimizer loss=1.331, test mae=1.099\n",
      "1.3282952146822569 0.08106063804827429\n",
      "1.3280808963085955 0.08212478005929678\n",
      "1.32787027539811 0.07361946720095279\n",
      "1.3276903775247755 0.07393350232819501\n",
      "1.3275148101035623 0.07272621312814981\n",
      "1.3273905737250664 0.07060866299683471\n",
      "1.327280174683089 0.06943743277886852\n",
      "1.3271518384766345 0.06865164569496388\n",
      "1.3270580932211309 0.06833995109518344\n",
      "1.3269661724793054 0.06648881952122414\n",
      "1.3268510250431869 0.07484494040318718\n",
      "1.3267629365237212 0.07292897832276163\n",
      "1.3266772512551366 0.07125228788047842\n",
      "1.3265628549823374 0.07453375868505795\n",
      "1.3264753444589383 0.07218839950354815\n",
      "1.3263620230922217 0.06656823221804235\n",
      "1.3262281852858662 0.061646662629007275\n",
      "1.3261030451212978 0.05610112563351566\n",
      "1.325987707466914 0.05138813932170826\n",
      "1.3258862172244692 0.04672990762101838\n",
      "1.3257922689215085 0.04616494527195759\n",
      "norms 29.480422482181098 37.247065972730105 39.0741115071283\n",
      "gradients 0.04616494527195759 0.19219148929349747 0.14268432553622892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 215 took 490.1s, optimizer loss=1.328, test mae=1.097\n",
      "1.3257922689215085 0.04616494527195759\n",
      "1.3257031468314955 0.04372671002678861\n",
      "1.325622400103747 0.04831452797515068\n",
      "1.3255636371708563 0.048817450050331064\n",
      "1.3255076223155244 0.05097527356268021\n",
      "1.325436265393274 0.05468460047962352\n",
      "1.3253809918723138 0.05618373134364492\n",
      "1.3253217424583175 0.05772055579021766\n",
      "1.325239498014506 0.060207851357396626\n",
      "1.325158833652904 0.06209093793555149\n",
      "1.3250691034200177 0.06183975558407632\n",
      "1.3249739173235404 0.06220156762968418\n",
      "1.3248703182772597 0.06127955003071929\n",
      "1.3247424390906275 0.0582206211037032\n",
      "1.324624565757614 0.05697648984515117\n",
      "1.3245004147136716 0.052971656121291004\n",
      "1.3243819140356923 0.05193443781898987\n",
      "1.3242557573312648 0.04824316739936043\n",
      "1.3241189302565952 0.04845050130528928\n",
      "1.3239787452179737 0.04645049943079541\n",
      "1.3238412206534884 0.048254275489539256\n",
      "norms 29.49771945596775 37.253175685040645 39.0916111616075\n",
      "gradients 0.048254275489539256 0.20020474597937873 0.15142748270095951\n",
      "epoch 216 took 486.4s, optimizer loss=1.326, test mae=1.097\n",
      "1.3238412206534884 0.048254275489539256\n",
      "1.323704711629033 0.048098686185664104\n",
      "1.323561770011118 0.054752336096644746\n",
      "1.3234099336774952 0.05355453459917287\n",
      "1.3232928126047545 0.05582774903352198\n",
      "1.3231855707473457 0.05889297656084928\n",
      "1.3230561821767157 0.057375600411658456\n",
      "1.322944383156593 0.05893700796425674\n",
      "1.3228324520673114 0.05842642183087934\n",
      "1.3226799934668776 0.058412030559010196\n",
      "1.322537205361572 0.05869057897251727\n",
      "1.3223831998570033 0.05539089770832064\n",
      "1.3222108594678734 0.056318945475517086\n",
      "1.3220457715385827 0.05183412529498943\n",
      "1.3218875182511685 0.05136310399994594\n",
      "1.3217303175479347 0.05053183073739804\n",
      "1.321599098926735 0.04658535761332354\n",
      "1.3214950582586953 0.046133031565866935\n",
      "1.321399704068531 0.05427945840616055\n",
      "1.3213197547130242 0.05118759034605693\n",
      "1.3212424689078637 0.049728995619047874\n",
      "norms 29.506744295868746 37.255626001289464 39.10188832894059\n",
      "gradients 0.049728995619047874 0.21147588871760023 0.2165991411898341\n",
      "epoch 217 took 484.4s, optimizer loss=1.324, test mae=1.096\n",
      "1.3212424689078637 0.049728995619047874\n",
      "1.3211480201946355 0.04621350591056487\n",
      "1.3210390295568817 0.046756803048557\n",
      "1.320931419791078 0.04236045997865648\n",
      "1.320829837841366 0.04208476877949605\n",
      "1.3207229915853511 0.03851038445183899\n",
      "1.320629218433221 0.0363455889048217\n",
      "1.3205381425807927 0.037730047827851304\n",
      "1.3204467126929778 0.03579968533234262\n",
      "1.3203483525926796 0.036248738198222795\n",
      "1.3202272205170218 0.040063287570222796\n",
      "1.3201107031496953 0.0386474823988396\n",
      "1.3199829834665562 0.05558255333704493\n",
      "1.319873432702655 0.052901645655257325\n",
      "1.3197490014159141 0.050344477925548414\n",
      "1.3195895988088253 0.052004418697035676\n",
      "1.319407129954766 0.04758321739619566\n",
      "1.319228391674933 0.05111869170809329\n",
      "1.3190652011790132 0.046594671160529316\n",
      "1.3189186292136805 0.04695625259139552\n",
      "1.318779778724763 0.042642931234477925\n",
      "norms 29.525403661366283 37.26223132517814 39.122240706062406\n",
      "gradients 0.042642931234477925 0.18544183515453544 0.09650170738516965\n",
      "epoch 218 took 486.3s, optimizer loss=1.321, test mae=1.095\n",
      "1.318779778724763 0.042642931234477925\n",
      "1.3186499594786276 0.04649620067699705\n",
      "1.3185363636901792 0.041804533377955595\n",
      "1.318445061448328 0.042793350605581267\n",
      "1.3183576925724512 0.04674685498166578\n",
      "1.31827430540196 0.04625933652220004\n",
      "1.318204369086419 0.05098590232822651\n",
      "1.318136459138164 0.05243456584419428\n",
      "1.3180659971307855 0.05697273947331131\n",
      "1.3179904867581655 0.05871415230410673\n",
      "1.3179093813113 0.06413662798072228\n",
      "1.3178267303412792 0.0658222373640667\n",
      "1.3177402320963032 0.06987675561224795\n",
      "1.3176454456665592 0.07078911869070377\n",
      "1.3175497175762434 0.07375610187358285\n",
      "1.3174525161837352 0.0738041966904359\n",
      "1.317352492862143 0.07542982741624532\n",
      "1.3172516361361761 0.07340119699013624\n",
      "1.317151742980665 0.07742073499437636\n",
      "1.317054965150399 0.07275194320446617\n",
      "1.3169779433978692 0.07274521180163872\n",
      "norms 29.528911999518847 37.2634283956637 39.12690377270616\n",
      "gradients 0.07274521180163872 0.20099727775874623 0.1554665813140341\n",
      "epoch 219 took 488.6s, optimizer loss=1.319, test mae=1.094\n",
      "1.3169779433978692 0.07274521180163872\n",
      "1.3168918337230997 0.06743249704067387\n",
      "1.3168148290103696 0.06529330665135687\n",
      "1.3167371238999945 0.0648748832609852\n",
      "1.3166415932325575 0.06064139447018825\n",
      "1.31655224588749 0.05984081414397954\n",
      "1.3164569769786778 0.05845154292005079\n",
      "1.3163486813928458 0.0603495512107613\n",
      "1.3162359020902654 0.05700950323476831\n",
      "1.3161209731173917 0.06121151852028845\n",
      "1.3160167704906525 0.05967234313822635\n",
      "1.3158960507575208 0.07519967257437929\n",
      "1.315804726572686 0.0715004716281223\n",
      "1.3156997649502915 0.0689498853052768\n",
      "1.3155703047745118 0.0654704144814744\n",
      "1.3154235894040969 0.0635024588945656\n",
      "1.3152706962850786 0.06113052097462293\n",
      "1.3151198853450004 0.06115497279158426\n",
      "1.3149799445429187 0.06010962199870158\n",
      "1.3148534269991774 0.06098010377955119\n",
      "1.3147412848707187 0.06063854469318396\n",
      "norms 29.55758005073155 37.27373613171751 39.15576633700999\n",
      "gradients 0.06063854469318396 0.18938819457935518 0.10808702182050965\n",
      "epoch 220 took 490.1s, optimizer loss=1.317, test mae=1.091\n",
      "1.3147412848707187 0.06063854469318396\n",
      "1.3146396775693516 0.06071763834832905\n",
      "1.3145440605528376 0.06112580517419401\n",
      "1.3144478213061563 0.06032400366664548\n",
      "1.3143586340468127 0.0587810566711708\n",
      "1.3142900875147063 0.059426271168340074\n",
      "1.3142284045763615 0.05709894954689894\n",
      "1.3141662525976185 0.057483988867230496\n",
      "1.3141035742100824 0.05478029874796079\n",
      "1.3140397765004108 0.05351572697591724\n",
      "1.31397176597301 0.050515351975634754\n",
      "1.3138928380174528 0.05067689063999054\n",
      "1.313812683137114 0.04768082282865068\n",
      "1.3137366322899702 0.046362293258261945\n",
      "1.31364945996864 0.0505146395889868\n",
      "1.3135739237440591 0.04860966482727644\n",
      "1.3134961686788151 0.05357812357337435\n",
      "1.3133999887985215 0.04920544790068394\n",
      "1.3133210983886632 0.050865659098581366\n",
      "1.3132324404972469 0.05350718851129444\n",
      "1.3131180798709863 0.052018092535855334\n",
      "norms 29.555462549532187 37.27298905576411 39.15650010035837\n",
      "gradients 0.052018092535855334 0.20078316868664076 0.10598720646612489\n",
      "epoch 221 took 486.7s, optimizer loss=1.315, test mae=1.093\n",
      "1.3131180798709863 0.052018092535855334\n",
      "1.3130258235260412 0.05464338653620287\n",
      "1.3129350407386826 0.05628928622745249\n",
      "1.3128150180472484 0.06274562412955596\n",
      "1.3127356526965013 0.0636509462891568\n",
      "1.3126445623384428 0.06371299333458631\n",
      "1.3125128107000488 0.08039559628848147\n",
      "1.3124030253053802 0.07741623344358084\n",
      "1.3122850575621539 0.0755883569354689\n",
      "1.3121400225943365 0.0720150692132061\n",
      "1.3120015406439223 0.06997015077153022\n",
      "1.3118529902785714 0.06559774554076521\n",
      "1.3116732381872849 0.06485109938691021\n",
      "1.3115439120663905 0.06277410280842087\n",
      "1.3113975334191037 0.06465861300733644\n",
      "1.311240528553154 0.06281374047429704\n",
      "1.3110695339008822 0.07001703515277417\n",
      "1.3109292241623818 0.06788394788525705\n",
      "1.310776477093119 0.0650478241544978\n",
      "1.3105772507050206 0.07089103756796136\n",
      "1.3103977071469604 0.06717306702671393\n",
      "norms 29.576210447634406 37.281691840430454 39.179810974543926\n",
      "gradients 0.06717306702671393 0.19309143533199785 0.13762361314122712\n",
      "epoch 222 took 492.3s, optimizer loss=1.313, test mae=1.091\n",
      "1.3103977071469604 0.06717306702671393\n",
      "1.3102224659857986 0.06869636003204688\n",
      "1.310029503750205 0.06313952398013783\n",
      "1.3098568935038983 0.06197067542951321\n",
      "1.309682385423347 0.05944069452350269\n",
      "1.3095266780465293 0.055606886672854815\n",
      "1.3093715404556328 0.055002767723383326\n",
      "1.3092120403800591 0.0496828773949111\n",
      "1.309057257972923 0.04938991722644807\n",
      "1.3089061117529364 0.04485538144642374\n",
      "1.3087613117143966 0.04519032130010015\n",
      "1.3086173687044829 0.043029458181258064\n",
      "1.3084707634843025 0.04606873378979561\n",
      "1.3083274899980442 0.047215946192045465\n",
      "1.3082268028684074 0.04912205795502142\n",
      "1.308134336900377 0.0504280428461676\n",
      "1.3080264586942156 0.057746823020715224\n",
      "1.3079386915140547 0.05897060846600493\n",
      "1.3078568921770857 0.059460542472904075\n",
      "1.3077561472173727 0.05841055242699635\n",
      "1.3076454199428498 0.0556634816126763\n",
      "norms 29.59836904125457 37.2906635135435 39.20295304525871\n",
      "gradients 0.0556634816126763 0.1804033710465052 0.09049668934275834\n",
      "epoch 223 took 482.9s, optimizer loss=1.31, test mae=1.091\n",
      "1.3076454199428498 0.0556634816126763\n",
      "1.3075358948839748 0.057014381950111125\n",
      "1.3074202858521022 0.053729992021203994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3073072109112844 0.052315236994807536\n",
      "1.3072096144378091 0.05064976580932317\n",
      "1.3071233868031258 0.04690673337560584\n",
      "1.30704057379531 0.04687136073113793\n",
      "1.306959168600606 0.03752733275799795\n",
      "1.3068851120446632 0.03955582628156143\n",
      "1.3068240042104717 0.03756253036544696\n",
      "1.306765696540785 0.03892925148295193\n",
      "1.3067020678397665 0.03953576490401488\n",
      "1.3066226636360878 0.040409878883498056\n",
      "1.3065391552776189 0.04318260151819774\n",
      "1.3064435593390564 0.0480506323520614\n",
      "1.3063625934657055 0.05091989152883107\n",
      "1.306262176494308 0.05192359781179936\n",
      "1.3061211870676923 0.05687344288613328\n",
      "1.3060139737416183 0.05586978831602366\n",
      "1.3058886052148733 0.0584028131824051\n",
      "1.3057366751435553 0.055978016983281136\n",
      "norms 29.611480839547447 37.29577716993329 39.21702167648879\n",
      "gradients 0.055978016983281136 0.18108155602438186 0.35828713983092453\n",
      "epoch 224 took 483.5s, optimizer loss=1.308, test mae=1.089\n",
      "1.3057366751435553 0.055978016983281136\n",
      "1.3055841755828361 0.05421856811015837\n",
      "1.3054139451868911 0.05709516134825024\n",
      "1.3053041277519966 0.05294309523568418\n",
      "1.3051921517599463 0.05155832491260977\n",
      "1.305061678284318 0.047932320455960506\n",
      "1.3049319328481432 0.04514518437735226\n",
      "1.3048251061715632 0.04463586508434648\n",
      "1.304723290109749 0.04498617371513727\n",
      "1.3046232074710664 0.046192511949757854\n",
      "1.304536692477572 0.048075053971256596\n",
      "1.3044388644600784 0.05276552954241626\n",
      "1.3043602426929708 0.05309363363093827\n",
      "1.3042752293474984 0.05504879599624262\n",
      "1.304162640653184 0.060865341072778266\n",
      "1.3040842610433419 0.05897898697255837\n",
      "1.303985121772048 0.058169619490465625\n",
      "1.3038595875521357 0.05690782849174237\n",
      "1.3037149897698126 0.05535521004971153\n",
      "1.3035634159958673 0.05561237656253388\n",
      "1.3034145060177011 0.05212624869797095\n",
      "norms 29.61763155428854 37.29758602444728 39.22627634528557\n",
      "gradients 0.05212624869797095 0.200530689311163 0.139543711467249\n",
      "epoch 225 took 485.8s, optimizer loss=1.306, test mae=1.088\n",
      "1.3034145060177011 0.05212624869797095\n",
      "1.3032837009490277 0.05176091409246798\n",
      "1.3031694413883022 0.049591273069695255\n",
      "1.3030715270440998 0.04873841423540605\n",
      "1.302983001175483 0.04844377043924702\n",
      "1.3029008845707801 0.04738160389285478\n",
      "1.3028189964610912 0.05262293914324039\n",
      "1.3027357057745435 0.049337216338007875\n",
      "1.3026716143452448 0.053718291246805105\n",
      "1.3026020198541794 0.05123107656449731\n",
      "1.3025379282962077 0.05416177039366763\n",
      "1.3024683974710891 0.05682039521551143\n",
      "1.3023954257878896 0.055591158743739495\n",
      "1.3023118634242816 0.06335754084632296\n",
      "1.3022288560233997 0.06131325959590352\n",
      "1.3021304838047676 0.06554420244734901\n",
      "1.3020199566403146 0.06410259212659218\n",
      "1.301896311910783 0.06962115436481045\n",
      "1.301793483033198 0.0670917061474848\n",
      "1.3016827325705005 0.0662635782703911\n",
      "1.3015400152976955 0.06245369498298336\n",
      "norms 29.625167188333773 37.30044821373545 39.235017860777326\n",
      "gradients 0.06245369498298336 0.1918688973341242 0.1760191564458841\n",
      "epoch 226 took 486.5s, optimizer loss=1.303, test mae=1.085\n",
      "1.3015400152976955 0.06245369498298336\n",
      "1.3014302183297783 0.0593012696648248\n",
      "1.301316815775532 0.05852546763432485\n",
      "1.3011679568268977 0.05500598979562452\n",
      "1.301054894957105 0.05349600909283045\n",
      "1.3009240156648918 0.05396646499290049\n",
      "1.300763414756955 0.05113014300676825\n",
      "1.3006012674003526 0.05302957817848177\n",
      "1.3004427390494746 0.049607922799960465\n",
      "1.3002976383049707 0.050649236405446184\n",
      "1.3001582454041107 0.048633487706408525\n",
      "1.300027886106484 0.04916409696502312\n",
      "1.2998999361954535 0.051054524502535885\n",
      "1.299770686639728 0.05071840471792023\n",
      "1.2996456400442766 0.06111710000443504\n",
      "1.2995409665703976 0.0604344732070844\n",
      "1.2994440537212841 0.06241671903819113\n",
      "1.2993211971893646 0.06366525824560616\n",
      "1.2992197186486334 0.06518932013344819\n",
      "1.2991132552783813 0.06676703815058281\n",
      "1.2989795484259397 0.06937329904319253\n",
      "norms 29.622929313759204 37.29860621238508 39.23593254036059\n",
      "gradients 0.06937329904319253 0.20422336068883332 0.4676195564286868\n",
      "epoch 227 took 489.5s, optimizer loss=1.302, test mae=1.083\n",
      "1.2989795484259397 0.06937329904319253\n",
      "1.2988465510886855 0.0710742520584186\n",
      "1.2987082989515613 0.0741582073733249\n",
      "1.2985423797982607 0.07636383909813108\n",
      "1.2983710350615787 0.08005532454912352\n",
      "1.2981827956509242 0.08096263498992923\n",
      "1.2980566307116672 0.08068303452705373\n",
      "1.2979159177788993 0.08027909748103275\n",
      "1.2977482924147197 0.07842845903839539\n",
      "1.297579590662143 0.07631846632462698\n",
      "1.2974287920592453 0.07195198872007665\n",
      "1.2972744277900228 0.07473243971064064\n",
      "1.2971435906298479 0.0675971359599465\n",
      "1.2970299146845492 0.06498889172283918\n",
      "1.2969306952639534 0.06089458748080662\n",
      "1.2968448645381696 0.05722578418957069\n",
      "1.296739795079653 0.054845983658745386\n",
      "1.2966333077286425 0.05070695729399588\n",
      "1.2965241312589024 0.04793694030037019\n",
      "1.296396259306957 0.04631744260896779\n",
      "1.2962900821695644 0.04322753007297339\n",
      "norms 29.632492802308484 37.30202743310264 39.2483050184044\n",
      "gradients 0.04322753007297339 0.19938246781174054 0.16033940343013445\n",
      "epoch 228 took 487.9s, optimizer loss=1.299, test mae=1.08\n",
      "1.2962900821695644 0.04322753007297339\n",
      "1.2961850177481509 0.04403318442369162\n",
      "1.2960682639985686 0.040719747971153936\n",
      "1.2959606912413142 0.042722734404215146\n",
      "1.2958627053876426 0.04273566369810471\n",
      "1.2957575112920365 0.050526866401289484\n",
      "1.2956666323111947 0.048992298214432316\n",
      "1.2955869081436504 0.05334780449554261\n",
      "1.2954960903024932 0.051684236873166825\n",
      "1.295410452900188 0.0540754390542044\n",
      "1.2953188237769417 0.05724869150836607\n",
      "1.2952039353224525 0.05560481453150517\n",
      "1.2951139473924127 0.05785592740086515\n",
      "1.295012495067091 0.05943779272078694\n",
      "1.2948832053913442 0.06030983534937983\n",
      "1.2947651618402654 0.062194606328058205\n",
      "1.294630511218158 0.06162016549107377\n",
      "1.2944869485285675 0.06325710212989179\n",
      "1.294337419483784 0.06286834887900032\n",
      "1.2941990976079898 0.06308655059141376\n",
      "1.2940605580817652 0.06267147404902834\n",
      "norms 29.64064994662536 37.30492451793965 39.25897115441159\n",
      "gradients 0.06267147404902834 0.19102402817233946 0.2999856363667934\n",
      "epoch 229 took 488.1s, optimizer loss=1.296, test mae=1.078\n",
      "1.2940605580817652 0.06267147404902834\n",
      "1.293903488828046 0.06320940799524306\n",
      "1.2937479961843656 0.06221655748651129\n",
      "1.293619201191608 0.06169132072386862\n",
      "1.2935153489102567 0.06099732812981297\n",
      "1.293418258924783 0.060255824871873497\n",
      "1.2933232696530754 0.06097350899795519\n",
      "1.2932461538017639 0.0588018124082571\n",
      "1.2931730636824894 0.05942300849079701\n",
      "1.2930935249549473 0.057412433295347474\n",
      "1.2930126207307295 0.0580373079048881\n",
      "1.2929265991221577 0.056588665055704084\n",
      "1.292832714503468 0.056606364274770525\n",
      "1.29273391589861 0.05644267524388892\n",
      "1.2926423325958063 0.055508169012\n",
      "1.2925480111503362 0.05428506093677184\n",
      "1.292430462620934 0.056309716441624254\n",
      "1.2923272136545587 0.05438250594418032\n",
      "1.292222870382252 0.05652186791905018\n",
      "1.2921119061175168 0.054458313109633005\n",
      "1.2920141928504358 0.055063606352022273\n",
      "norms 29.651208041053543 37.30882011065773 39.271149542485766\n",
      "gradients 0.055063606352022273 0.19327414731154668 0.3060295300561054\n",
      "epoch 230 took 487.9s, optimizer loss=1.294, test mae=1.075\n",
      "1.2920141928504358 0.055063606352022273\n",
      "1.2919076621333998 0.05496258628422812\n",
      "1.2918001258915839 0.05477387637589419\n",
      "1.291683320103175 0.05681136796557059\n",
      "1.2915402363065063 0.05675272179659188\n",
      "1.2914222930994805 0.057934831256751744\n",
      "1.291300678833262 0.0641927110592957\n",
      "1.2911625324006824 0.06314468438132513\n",
      "1.2910318800404743 0.06837704732870067\n",
      "1.2909089257303659 0.07040909972801151\n",
      "1.2907988593004167 0.0705219827635416\n",
      "1.290679091084326 0.08941920827589561\n",
      "1.2905776877019188 0.08696506948658482\n",
      "1.2904651681215271 0.08337020002566779\n",
      "1.2903207341218652 0.08061687951289655\n",
      "1.2901601444733044 0.07397087250161616\n",
      "1.2899976794047474 0.06999472387174767\n",
      "1.2898306072676855 0.06752917055678755\n",
      "1.289698486940022 0.06059857584578427\n",
      "1.2895952488230678 0.059092804062977476\n",
      "1.2894894804448502 0.05806927289327213\n",
      "norms 29.66459175231937 37.31425279463988 39.28576628542561\n",
      "gradients 0.05806927289327213 0.1969151136416894 0.15581134452703815\n",
      "epoch 231 took 484.4s, optimizer loss=1.292, test mae=1.072\n",
      "1.2894894804448502 0.05806927289327213\n",
      "1.2893871935959786 0.05404902673414299\n",
      "1.2892922107062084 0.05547131866932598\n",
      "1.2891732106382552 0.05280059923431108\n",
      "1.2890636958929782 0.05153836003858019\n",
      "1.2889392777736637 0.0567545046568029\n",
      "1.2888378697384855 0.05507846726324041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2887362226455314 0.0553892836321918\n",
      "1.2885523859463355 0.05461866011804956\n",
      "1.2883173491205022 0.05348597800925884\n",
      "1.2880676345551683 0.05691734784289196\n",
      "1.2878149087191442 0.05298081648715222\n",
      "1.2876026830241192 0.05435202760822672\n",
      "1.2874374748529491 0.05187644692489636\n",
      "1.2873033670051628 0.05209349445764444\n",
      "1.2871805410394213 0.05081699824333098\n",
      "1.2870407801858594 0.05289179795031628\n",
      "1.286880919181522 0.05075734766311666\n",
      "1.2867417194914144 0.05168018732213557\n",
      "1.2866275502105329 0.05248128424372362\n",
      "1.2865182233143422 0.05115146658047951\n",
      "norms 29.695469443908003 37.328621395156254 39.321917790419555\n",
      "gradients 0.05115146658047951 0.19880148327764288 0.21406233559545615\n",
      "epoch 232 took 489.1s, optimizer loss=1.289, test mae=1.072\n",
      "1.2865182233143422 0.05115146658047951\n",
      "1.2864180828774758 0.05233935335700372\n",
      "1.2863305202622544 0.050055972837560456\n",
      "1.286233548220278 0.051813328774459357\n",
      "1.2861497173760368 0.04920279770603463\n",
      "1.2860663787658815 0.048690119362265256\n",
      "1.285974396989471 0.04915900947020674\n",
      "1.2859036756554625 0.045958245615041596\n",
      "1.285841086244601 0.04511876715698417\n",
      "1.2857682306879235 0.050837340770513884\n",
      "1.2857075054215934 0.04969812758391816\n",
      "1.2856388555163722 0.04846878801387098\n",
      "1.2855515837640412 0.04770490440083599\n",
      "1.285449855303413 0.05061782533233833\n",
      "1.2853397550670131 0.0497184826269397\n",
      "1.2852431985292783 0.05079866495682396\n",
      "1.2851268166585945 0.053917632439302\n",
      "1.285025316045148 0.05433151360504562\n",
      "1.2848888930978613 0.05288557353855824\n",
      "1.2847077814391221 0.05385251863472822\n",
      "1.2845016767431268 0.05270356141774569\n",
      "norms 29.716825825847742 37.338675041904274 39.34521024968607\n",
      "gradients 0.05270356141774569 0.1978875826164545 0.10548717434381212\n",
      "epoch 233 took 485.3s, optimizer loss=1.287, test mae=1.068\n",
      "1.2845016767431268 0.05270356141774569\n",
      "1.284276085091683 0.0534653049717205\n",
      "1.2840656784122448 0.05535994007790875\n",
      "1.2838649350103424 0.053721572387293186\n",
      "1.2836846012209489 0.05545575488247436\n",
      "1.2835159247017673 0.05540900630702335\n",
      "1.2833404946458489 0.06365606240704373\n",
      "1.2831783853308119 0.061874940666101316\n",
      "1.2830383144580368 0.06419316646826989\n",
      "1.2828835591457357 0.06463071646245243\n",
      "1.2827485134542265 0.06519703822626298\n",
      "1.282609999342085 0.06630623408488251\n",
      "1.2824508604849427 0.07110238548709151\n",
      "1.2822920969507732 0.06777293803691425\n",
      "1.2821594679964972 0.07487096993167623\n",
      "1.2820220566422362 0.07027325065091473\n",
      "1.2819009368351708 0.07173791080444153\n",
      "1.28177363408811 0.06946516480071739\n",
      "1.2816486132425038 0.06796609973374705\n",
      "1.2815281283207454 0.06637986393563136\n",
      "1.281408298540564 0.06549731767072817\n",
      "norms 29.72088026894259 37.34040015167405 39.35274289531503\n",
      "gradients 0.06549731767072817 0.20547901492978554 0.16031336899901263\n",
      "epoch 234 took 486.2s, optimizer loss=1.285, test mae=1.065\n",
      "1.281408298540564 0.06549731767072817\n",
      "1.2812883543638383 0.0626472577602975\n",
      "1.2811645414252257 0.06525511950699567\n",
      "1.2810771144146138 0.06361010454615942\n",
      "1.2809998114873167 0.061690337146304716\n",
      "1.2809072419903904 0.06326072361902288\n",
      "1.2808418859426836 0.06118448582672813\n",
      "1.2807778751468304 0.05894791649981453\n",
      "1.280694381371021 0.05849165374323989\n",
      "1.2805922379420558 0.05438543283026778\n",
      "1.280484695915323 0.05398386444864201\n",
      "1.2803691542917626 0.049876237282023714\n",
      "1.2802504674938144 0.050311250204306816\n",
      "1.2801251068313848 0.047907312290184446\n",
      "1.2800037404284197 0.047593358027442\n",
      "1.2798818904838822 0.050497553165023884\n",
      "1.2797759281750514 0.0496561863161474\n",
      "1.279678398588595 0.05236639491946153\n",
      "1.2795579419272651 0.05195724189815317\n",
      "1.2794677727905908 0.05242046253097261\n",
      "1.2793696025443884 0.053510921158947095\n",
      "norms 29.735411336022118 37.347602237116945 39.37033764618832\n",
      "gradients 0.053510921158947095 0.19353043917678936 0.225013545255104\n",
      "epoch 235 took 483.6s, optimizer loss=1.281, test mae=1.064\n",
      "1.2793696025443884 0.053510921158947095\n",
      "1.27923451136119 0.06254125905037933\n",
      "1.2790995731474288 0.06142015134209928\n",
      "1.2789673668715966 0.06259979209614307\n",
      "1.278818256557336 0.06315848449945889\n",
      "1.278671348555276 0.06143149654222528\n",
      "1.2785472592738087 0.06128277281700618\n",
      "1.2784183200803139 0.0587400297192744\n",
      "1.27829958481644 0.05737576187359013\n",
      "1.278187789213289 0.05554334947923102\n",
      "1.27806724676043 0.054765062529440946\n",
      "1.277945538171005 0.054631169035948404\n",
      "1.2778183365980822 0.05369132557248704\n",
      "1.2777095937484197 0.05414555523625399\n",
      "1.2775980422215711 0.05638129411905293\n",
      "1.2774911701571978 0.05829310331853176\n",
      "1.2773977512441548 0.05937240569819941\n",
      "1.2772775697700933 0.06590272855151406\n",
      "1.2771666402012833 0.06646473848061929\n",
      "1.2770356975566095 0.06707982353980699\n",
      "1.276879062243655 0.06510055926678728\n",
      "norms 29.73293321078616 37.34663354734056 39.371135907040944\n",
      "gradients 0.06510055926678728 0.1888366877823765 0.14771294476051408\n",
      "epoch 236 took 487.7s, optimizer loss=1.279, test mae=1.063\n",
      "1.276879062243655 0.06510055926678728\n",
      "1.276727170486425 0.06430182430499025\n",
      "1.2765779150927408 0.05958758626724307\n",
      "1.2764436137826263 0.05682320727968916\n",
      "1.2763357702142701 0.055124054656487925\n",
      "1.2762418251706078 0.05238739936649089\n",
      "1.2761628722291376 0.05072266619775764\n",
      "1.276077401944018 0.05094230652323023\n",
      "1.2760147722329713 0.04912390455077131\n",
      "1.2759540471954625 0.050258984379284305\n",
      "1.275880145554045 0.04824958893908168\n",
      "1.275811419042619 0.049307458569635934\n",
      "1.275728592858682 0.04880685913567905\n",
      "1.2756288662055817 0.050407621613930594\n",
      "1.275507909010347 0.05175430668544159\n",
      "1.2753863506379948 0.05196294204260933\n",
      "1.275258361583871 0.05289049401035916\n",
      "1.2751008370049937 0.05718236531470845\n",
      "1.2749748975576396 0.055309179954311644\n",
      "1.2748255418864205 0.0544791174220252\n",
      "1.274647329912723 0.05220322171659527\n",
      "norms 29.71281237151142 37.33636144776468 39.35222228708434\n",
      "gradients 0.05220322171659527 0.19393650829277287 0.45988705814587766\n",
      "epoch 237 took 486.4s, optimizer loss=1.277, test mae=1.063\n",
      "1.274647329912723 0.05220322171659527\n",
      "1.274454596186598 0.048445736139014504\n",
      "1.2743147511781991 0.04640226421286727\n",
      "1.2741773978225286 0.04658418673331148\n",
      "1.2740536976871129 0.04549368037996714\n",
      "1.2739395980967487 0.045543601409355226\n",
      "1.2738118523769268 0.051854165785489695\n",
      "1.273691580586853 0.053008736240783026\n",
      "1.273579026831705 0.05513275966688345\n",
      "1.2734812660366115 0.05799628444117699\n",
      "1.2733967471674092 0.057574004711109975\n",
      "1.27329485135185 0.06967267982563123\n",
      "1.273210262112699 0.06757543278995815\n",
      "1.2731234999729222 0.070026971025622\n",
      "1.2730087443794202 0.06417023534841147\n",
      "1.272930042682349 0.06288169996861352\n",
      "1.272839858064087 0.06303535839921857\n",
      "1.2727199749209286 0.06034670321596365\n",
      "1.2726216804223873 0.05799683396567573\n",
      "1.2725167034045033 0.05810668713960801\n",
      "1.2723948070716926 0.053666685192777557\n",
      "norms 29.730927341887643 37.345534943523816 39.37454976617019\n",
      "gradients 0.053666685192777557 0.19104403065757614 0.16847391905615555\n",
      "epoch 238 took 489.8s, optimizer loss=1.275, test mae=1.062\n",
      "1.2723948070716926 0.053666685192777557\n",
      "1.2722693701273824 0.052439245811918896\n",
      "1.27214843562216 0.05175341347932713\n",
      "1.2720414984596826 0.04906296766293982\n",
      "1.2719461963521594 0.049906200548692735\n",
      "1.2718503241149701 0.04737546185751384\n",
      "1.2717598515545538 0.04914625326429458\n",
      "1.271671689912701 0.04958083672836771\n",
      "1.271582848780561 0.051700943587904724\n",
      "1.2714884752626918 0.05693018080471691\n",
      "1.2713952519055605 0.05790157388583917\n",
      "1.2713154088342502 0.061390490242362185\n",
      "1.2712230740919543 0.06373401956272919\n",
      "1.271127132484739 0.06654879945348967\n",
      "1.2710317904077104 0.06752802668139443\n",
      "1.270931195256684 0.0682937999414599\n",
      "1.2708295326989736 0.06883448292099394\n",
      "1.2707094002037107 0.06731012526502973\n",
      "1.2706018867187352 0.06618425179152873\n",
      "1.270503107617973 0.06347711938964677\n",
      "1.2703994603997673 0.06335959534950325\n",
      "norms 29.76211805750863 37.36113391911864 39.40975298076839\n",
      "gradients 0.06335959534950325 0.19130315298290307 0.12598493229946992\n",
      "epoch 239 took 484.7s, optimizer loss=1.272, test mae=1.064\n",
      "1.2703994603997673 0.06335959534950325\n",
      "1.2703046689303994 0.05960042853190996\n",
      "1.2702225548097783 0.05703488390025335\n",
      "1.2701381450780824 0.054534875199700884\n",
      "1.2700551798067827 0.052167454349231285\n",
      "1.269971691665108 0.05033903255581496\n",
      "1.26988195932807 0.04999278042827542\n",
      "1.2698001395370584 0.0490520461083772\n",
      "1.2697294788023266 0.0499981087228573\n",
      "1.2696379598925793 0.051088728913899914\n",
      "1.2695215107173599 0.05195038843295181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2693828767373707 0.057727462319220324\n",
      "1.2692571898811567 0.05727344819401359\n",
      "1.2691232843695734 0.05725810867616589\n",
      "1.2689647357776166 0.0619695446446524\n",
      "1.2688222903608284 0.05904048007022809\n",
      "1.2686819492397117 0.05900087648676733\n",
      "1.2685215262792449 0.05440045459331031\n",
      "1.2683810206198463 0.053127412152092754\n",
      "1.2682649955711305 0.05048264381225331\n",
      "1.2681611071756307 0.04980193567081152\n",
      "norms 29.765491040509247 37.36226018119851 39.41437551621001\n",
      "gradients 0.04980193567081152 0.19004984115043586 0.2753530961862266\n",
      "epoch 240 took 487.9s, optimizer loss=1.27, test mae=1.065\n",
      "1.2681611071756307 0.04980193567081152\n",
      "1.2680610357424 0.048565244958245615\n",
      "1.2679681945934562 0.048566532623771\n",
      "1.2678776552706328 0.0516379436506023\n",
      "1.267792821675092 0.050290447434584724\n",
      "1.2677197159010924 0.054938386654956414\n",
      "1.2676467182487534 0.055215210482941104\n",
      "1.2675799325304462 0.05891234664959012\n",
      "1.2675130211165195 0.06043069910428434\n",
      "1.267436929536306 0.06395961955223559\n",
      "1.2673568865262812 0.06469721020347702\n",
      "1.2672729495378754 0.06903232250216298\n",
      "1.2671773019927592 0.06640896316334204\n",
      "1.2670843771283469 0.07070284523763404\n",
      "1.2670073222953009 0.06975143379043525\n",
      "1.2669227599860076 0.07615845887592411\n",
      "1.2668584406413346 0.07405737988731369\n",
      "1.2667906915776928 0.07310581176933015\n",
      "1.266686796624186 0.07011136880632735\n",
      "1.2665943545638343 0.06815890537236546\n",
      "1.266474882706146 0.06606154836741593\n",
      "norms 29.765015580797602 37.36144127641147 39.415059600913324\n",
      "gradients 0.06606154836741593 0.19213966171670865 0.24997790003731796\n",
      "epoch 241 took 487.1s, optimizer loss=1.268, test mae=1.065\n",
      "1.266474882706146 0.06606154836741593\n",
      "1.2663522015533437 0.0632532135289239\n",
      "1.2662292593276117 0.06129410110153613\n",
      "1.266099012480057 0.05803104668408153\n",
      "1.2659811412353106 0.05834768870960465\n",
      "1.265873045709955 0.055331381539258005\n",
      "1.2657859167546786 0.05456306108562252\n",
      "1.2656891388252947 0.05698423843675889\n",
      "1.265591920279466 0.05662186898732101\n",
      "1.265472879487659 0.05438649246453983\n",
      "1.2653118689054752 0.05764163873552235\n",
      "1.2651360495890798 0.05066613857798176\n",
      "1.2650079145922644 0.04864173091768743\n",
      "1.2648817298165471 0.046885342689121236\n",
      "1.2647316316130561 0.05765490937228809\n",
      "1.2646387972119597 0.05142075026017328\n",
      "1.2645442887899896 0.047258052878460036\n",
      "1.264436609948651 0.04000759083329506\n",
      "1.264342450534206 0.035845311531374484\n",
      "1.2642585572451053 0.03525097346537122\n",
      "1.2641654952284271 0.033340529651791226\n",
      "norms 29.764214888067567 37.35907355421094 39.41237504694609\n",
      "gradients 0.033340529651791226 0.18453379461127054 0.11738516219353543\n",
      "epoch 242 took 486.9s, optimizer loss=1.266, test mae=1.068\n",
      "1.2641654952284271 0.033340529651791226\n",
      "1.264088447702928 0.03537484433079438\n",
      "1.2640010654475282 0.03942847494412134\n",
      "1.2638933917098831 0.04320148668526772\n",
      "1.263774951927139 0.050053778509785166\n",
      "1.2636897417047024 0.05385280016992106\n",
      "1.263598198423392 0.0590740566207978\n",
      "1.2634929019492822 0.06011750956580943\n",
      "1.2633978577917242 0.06279439483487942\n",
      "1.2632997062718883 0.06167840317384123\n",
      "1.2632001884975055 0.06246408393278696\n",
      "1.263103795955739 0.0605313927963988\n",
      "1.2629974319056496 0.060388397056296936\n",
      "1.2629152226666633 0.05828215173147269\n",
      "1.2628392869712828 0.05508556322585352\n",
      "1.262749966928942 0.054744773046432244\n",
      "1.262680285724715 0.05193192922992122\n",
      "1.262604444327052 0.050615036555209963\n",
      "1.2625298898124482 0.04886069075620834\n",
      "1.262452155453786 0.046983011943096946\n",
      "1.2623547045659445 0.04685899326517133\n",
      "norms 29.775000581058965 37.36365105563401 39.423597760267654\n",
      "gradients 0.04685899326517133 0.17206085289116774 0.22702475921379495\n",
      "epoch 243 took 484.8s, optimizer loss=1.264, test mae=1.067\n",
      "1.2623547045659445 0.04685899326517133\n",
      "1.2622646995858144 0.045635435169784674\n",
      "1.262161784820777 0.0506301904321313\n",
      "1.2620561735639275 0.04928640290793036\n",
      "1.2619403822543067 0.054756136643635454\n",
      "1.2618127780838555 0.052640569990812944\n",
      "1.2616900799115078 0.05554261564789484\n",
      "1.2615570562881118 0.053128517752821465\n",
      "1.2614532942432266 0.05240448039671257\n",
      "1.2613327019722969 0.051865038095462224\n",
      "1.2612076145595246 0.05010357211220723\n",
      "1.2610636890525446 0.0515833768137122\n",
      "1.2608671367959399 0.051845834139275224\n",
      "1.2607348276720063 0.048856486901504646\n",
      "1.2606101027111258 0.04729013422548522\n",
      "1.2604671673339127 0.04905727298091299\n",
      "1.2602985539373097 0.044999514734706654\n",
      "1.2601584400958605 0.046169979838338954\n",
      "1.2600404243304402 0.04827142962708845\n",
      "1.2599265892310485 0.04689069658030952\n",
      "1.2598212691340058 0.051901287453532854\n",
      "norms 29.768868726742042 37.36050363860698 39.42116268790316\n",
      "gradients 0.051901287453532854 0.19578043926875904 0.2504591718275761\n",
      "epoch 244 took 486.6s, optimizer loss=1.262, test mae=1.065\n",
      "1.2598212691340058 0.051901287453532854\n",
      "1.2597221554825544 0.05131101097269844\n",
      "1.2596348252672867 0.05427186909510724\n",
      "1.2595467342965807 0.053696675718150615\n",
      "1.2594531284693053 0.05670660041334807\n",
      "1.2593695950540642 0.05718437064076511\n",
      "1.259266166969594 0.0642219867631113\n",
      "1.259170177884082 0.06422044760006274\n",
      "1.2590689387945482 0.0659149930123587\n",
      "1.2589301889922664 0.06763458004849099\n",
      "1.258822588679714 0.06696483255594171\n",
      "1.2586713687063493 0.07699242529407652\n",
      "1.2585466201917739 0.07645592919744315\n",
      "1.2583825227470955 0.07572415004499738\n",
      "1.2582001235687135 0.07107390915875639\n",
      "1.2580273920713931 0.07007569205650983\n",
      "1.2578621764397546 0.06540858031823724\n",
      "1.257716182181565 0.06238186186227951\n",
      "1.2575816685951096 0.06178341850023399\n",
      "1.2574519954275565 0.057011486234847294\n",
      "1.2573429859789655 0.05824956213889715\n",
      "norms 29.77861306585586 37.36646827158544 39.4365221807004\n",
      "gradients 0.05824956213889715 0.19639244564362163 0.10434437475546789\n",
      "epoch 245 took 484.6s, optimizer loss=1.26, test mae=1.065\n",
      "1.2573429859789655 0.05824956213889715\n",
      "1.2572434405015323 0.054392284190435194\n",
      "1.2571653393714641 0.05333936852483693\n",
      "1.257076267012779 0.05336136189336989\n",
      "1.2569958757625928 0.05212483860099725\n",
      "1.2569133219113808 0.051908994637806065\n",
      "1.2568146202639847 0.04796741903008098\n",
      "1.256722460592753 0.04937771804165666\n",
      "1.2566260288459332 0.04829717047109045\n",
      "1.2565531715603608 0.04742464120026276\n",
      "1.2564749387366774 0.04687484249820094\n",
      "1.2563796799877538 0.04545674903865204\n",
      "1.256276910630206 0.04370836463259876\n",
      "1.2561849559686586 0.04332009398367106\n",
      "1.2560974173967567 0.0412958929930856\n",
      "1.2560255372377953 0.04139329826826005\n",
      "1.25595492031666 0.03944257262585901\n",
      "1.2558843800997381 0.0408698706154025\n",
      "1.2558205741423683 0.040154602570218016\n",
      "1.2557376948825505 0.04571023488531082\n",
      "1.2556593603065342 0.046691509443406616\n",
      "norms 29.786393384964526 37.371171416513654 39.447949702213506\n",
      "gradients 0.046691509443406616 0.1899790219005703 0.1470111239003094\n",
      "epoch 246 took 485.5s, optimizer loss=1.257, test mae=1.064\n",
      "1.2556593603065342 0.046691509443406616\n",
      "1.2555579303126476 0.04754420722050247\n",
      "1.2554314736936594 0.04931713622171637\n",
      "1.2553038997231802 0.05001683038984168\n",
      "1.2551764334552182 0.05166605980512501\n",
      "1.255053196973858 0.05133427111204155\n",
      "1.2549298796374107 0.05417627636129846\n",
      "1.2548321230519197 0.05343643524727453\n",
      "1.2547461588503364 0.05560576482053273\n",
      "1.2546662320858846 0.055007084791125545\n",
      "1.2545939583332353 0.056217324653344516\n",
      "1.2545315426790586 0.055712811790316195\n",
      "1.2544617766964496 0.059697956270628655\n",
      "1.2544000563557631 0.05832257447054376\n",
      "1.2543192405547685 0.05735574382597243\n",
      "1.2542098329758131 0.05734947044632661\n",
      "1.254089561462299 0.05633988389967293\n",
      "1.2539885052857924 0.054219002998755234\n",
      "1.2538815375276455 0.05123146799053511\n",
      "1.2537513474933193 0.0501126380160121\n",
      "1.253631313997053 0.04526273155022231\n",
      "norms 29.778699217020623 37.36783364951737 39.44389073107102\n",
      "gradients 0.04526273155022231 0.1857722255391758 0.4227972237707618\n",
      "epoch 247 took 485.6s, optimizer loss=1.256, test mae=1.065\n",
      "1.253631313997053 0.04526273155022231\n",
      "1.253512557173974 0.04358988005384634\n",
      "1.2533844503225489 0.051094298142783115\n",
      "1.2532932924508824 0.04809667553145311\n",
      "1.2531938650029695 0.043368091553318534\n",
      "1.253085415643499 0.042392844023473195\n",
      "1.2529883502479269 0.03807734896157926\n",
      "1.252909521000719 0.037993522394186165\n",
      "1.2528337225357842 0.038396649882131775\n",
      "1.2527697141929055 0.03824493935976628\n",
      "1.2527062954144403 0.03919540230776314\n",
      "1.252628890917072 0.03983594657941958\n",
      "1.2525481813859565 0.04041138444784729\n",
      "1.2524628626194372 0.045002389264707406\n",
      "1.2523735545634043 0.043742751459483156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2522819617274208 0.05252049062635873\n",
      "1.252204168321684 0.05172918576286757\n",
      "1.2521239530936699 0.05202553109933686\n",
      "1.2520306654916222 0.050499374449805616\n",
      "1.2519431876067058 0.05031700748233003\n",
      "1.2518502020316333 0.047460779693237705\n",
      "norms 29.79286699693865 37.37722085940178 39.46467642263765\n",
      "gradients 0.047460779693237705 0.17675226351232828 0.18446068580157063\n",
      "epoch 248 took 485.2s, optimizer loss=1.254, test mae=1.064\n",
      "1.2518502020316333 0.047460779693237705\n",
      "1.2517490050515574 0.04820410909504157\n",
      "1.2516552359970576 0.04527283227401365\n",
      "1.251576193564335 0.0461655238111356\n",
      "1.2514980710746275 0.043181226674437335\n",
      "1.251428614477678 0.0440852819776636\n",
      "1.251374695900427 0.04378728633945982\n",
      "1.2513063869553258 0.0455415644658858\n",
      "1.2512191652055376 0.04628818615082695\n",
      "1.251138912056169 0.04625092899689687\n",
      "1.2510520452904623 0.050797605026557606\n",
      "1.250978834905505 0.05058923933811905\n",
      "1.250891815906726 0.05187700860998998\n",
      "1.2507736258383713 0.05371281234883803\n",
      "1.2506700216962017 0.05286548902620371\n",
      "1.2505642344395138 0.05396427356718383\n",
      "1.2504270127597683 0.057315833008301335\n",
      "1.250328309175232 0.057002014777366376\n",
      "1.2502016933587587 0.057142448023223\n",
      "1.2500431190917678 0.0557835130298865\n",
      "1.249856616391248 0.057695493611385665\n",
      "norms 29.786929351550977 37.37412185924337 39.46067807109809\n",
      "gradients 0.057695493611385665 0.18733724773212138 0.09190307142522804\n",
      "epoch 249 took 485.1s, optimizer loss=1.252, test mae=1.062\n"
     ]
    }
   ],
   "source": [
    "assert model.optimizable_weights\n",
    "himem = True\n",
    "if himem and len(all_losses)==0:\n",
    "    composition, radial_spectrum, spherical_expansions, energies, _ = next(iter(train_dataloader_no_batch))\n",
    "\n",
    "for epoch in range(250):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    def single_step():\n",
    "        global composition, radial_spectrum, spherical_expansions, energies\n",
    "        optimizer.zero_grad()\n",
    "        if device==\"cuda\":\n",
    "            print(f\"mem. before:  {torch.cuda.memory_stats()['allocated_bytes.all.current']/1e6} MB allocated, {torch.cuda.memory_stats()['reserved_bytes.all.current']/1e6} MB reserved \")\n",
    "        loss = torch.zeros(size=(1,), device=device)\n",
    "        if himem:\n",
    "            predicted, _ = model(composition, radial_spectrum, spherical_expansions, forward_forces=False)\n",
    "            loss += loss_mse(predicted, energies)\n",
    "        else:\n",
    "            for composition, radial_spectrum, spherical_expansions, energies, _ in train_dataloader:\n",
    "                try:\n",
    "                    predicted, _ = model(composition, radial_spectrum, spherical_expansions, forward_forces=False)\n",
    "                except:\n",
    "                    if device==\"cuda\":\n",
    "                        print(f\"mem. during:  {torch.cuda.memory_stats()['allocated_bytes.all.current']/1e6} MB allocated, {torch.cuda.memory_stats()['reserved_bytes.all.current']/1e6} MB reserved \")\n",
    "                    raise\n",
    "                loss += loss_mse(predicted, energies)\n",
    "        loss /= n_train\n",
    "        if model.composition_model is not None:\n",
    "            loss += TORCH_REGULARIZER_COMPOSITION * torch.linalg.norm(model.composition_model.weights)\n",
    "        if model.radial_spectrum_model is not None:\n",
    "            loss += TORCH_REGULARIZER_RADIAL_SPECTRUM * torch.linalg.norm(model.radial_spectrum_model.weights)\n",
    "        if model.power_spectrum_model is not None:\n",
    "            loss += TORCH_REGULARIZER_POWER_SPECTRUM * torch.linalg.norm(model.power_spectrum_model.weights)\n",
    "\n",
    "        loss.backward(retain_graph=False)\n",
    "        print(loss.item(), np.linalg.norm(model.composition_model.weights.grad.numpy()))\n",
    "        return loss\n",
    "            \n",
    "    loss = optimizer.step(single_step)\n",
    "    loss = loss.item()\n",
    "    all_losses.append(loss)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"norms\", np.linalg.norm(0 if model.composition_model is None else model.composition_model.weights.detach().cpu().numpy()),\n",
    "                  np.linalg.norm(0 if model.radial_spectrum_model is None else model.radial_spectrum_model.weights.detach().cpu().numpy()),\n",
    "                  np.linalg.norm(0 if model.power_spectrum_model is None else model.power_spectrum_model.weights.detach().cpu().numpy())\n",
    "                 )\n",
    "        print(\"gradients\", \n",
    "                  np.linalg.norm(0 if model.composition_model is None else model.composition_model.weights.grad.detach().cpu().numpy()),\n",
    "                  np.linalg.norm(0 if model.radial_spectrum_model is None else model.radial_spectrum_model.weights.grad.detach().cpu().numpy()),\n",
    "                  np.linalg.norm(0 if model.power_spectrum_model is None else model.power_spectrum_model.weights.grad.detach().cpu().numpy())\n",
    "                 )\n",
    "        with torch.no_grad():\n",
    "            predicted = []\n",
    "            reference = []\n",
    "            for tcomposition, tradial_spectrum, tspherical_expansions, tenergies, _ in test_dataloader:\n",
    "                reference.append(tenergies)\n",
    "                predicted_e, _ = model(tcomposition, tradial_spectrum, tspherical_expansions, forward_forces=False)\n",
    "                predicted.append(predicted_e)\n",
    "\n",
    "            reference = torch.vstack(reference)\n",
    "            predicted = torch.vstack(predicted)\n",
    "            test_mae = loss_mae(predicted, reference)/n_test\n",
    "\n",
    "            output.write(f\"{n_epochs_total} {loss} {test_mae}\\n\")\n",
    "            output.flush()\n",
    "        all_tests.append(test_mae.item())\n",
    "        print(f\"epoch {n_epochs_total} took {epoch_time:.4}s, optimizer loss={loss:.4}, test mae={test_mae:.4}\")\n",
    "    \n",
    "    del loss\n",
    "    n_epochs_total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc5682bff10>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(all_losses)\n",
    "plt.loglog(all_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041985273361206055"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset._getitemtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.089063167572021"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset._collatetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MAE: 0.032659706016333896 eV/at\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predicted.numpy(), reference.numpy(), 'b.')\n",
    "print(f\"TEST MAE: {test_mae.item()/len(frames[0])} eV/at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tpredicted = []\n",
    "    treference = []\n",
    "    for tcomposition, tradial_spectrum, tspherical_expansions, tenergies, _ in train_dataloader:\n",
    "        treference.append(tenergies)\n",
    "        predicted_e, _ = model(tcomposition, tradial_spectrum, tspherical_expansions, forward_forces=False)\n",
    "        tpredicted.append(predicted_e)\n",
    "\n",
    "    treference = torch.vstack(treference)\n",
    "    tpredicted = torch.vstack(tpredicted)\n",
    "    tmae = loss_mae(tpredicted, treference)/n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN MAE: 0.024634098247149933 eV/at\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tpredicted, treference, 'b.')\n",
    "print(f\"TRAIN MAE: {tmae.item()/len(frames[0])} eV/at\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting forces with forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_forces(model, dataloader):\n",
    "    predicted_e = []\n",
    "    reference_e = []\n",
    "    predicted_f = []\n",
    "    reference_f = []\n",
    "    for frame, radial_spectrum, spherical_expansions, energies, forces in dataloader:\n",
    "        reference_e.append(energies)\n",
    "        reference_f.append(forces)\n",
    "        e, f = model(frame, radial_spectrum, spherical_expansions, forward_forces=True)\n",
    "        predicted_e.append(e)\n",
    "        predicted_f.append(f)\n",
    "\n",
    "    reference_e = torch.vstack(reference_e)\n",
    "    predicted_e = torch.vstack(predicted_e)\n",
    "\n",
    "    reference_f = torch.vstack(reference_f)\n",
    "    predicted_f = torch.vstack(predicted_f)\n",
    "\n",
    "    return reference_e, predicted_e, reference_f, predicted_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity_plots(reference_e, predicted_e, reference_f, predicted_f):\n",
    "    predicted_e = predicted_e.detach()\n",
    "    predicted_f = predicted_f.detach()\n",
    "\n",
    "    fig, (ax_e, ax_f) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    mae = loss_mae(predicted_e, reference_e)\n",
    "    rmse = loss_rmse(predicted_e, reference_e)\n",
    "    ax_e.scatter(reference_e.cpu(), predicted_e.cpu())\n",
    "    x = (torch.min(reference_e.flatten()).item(), torch.max(reference_e.flatten()).item())\n",
    "    ax_e.plot(x, x, color=\"red\")\n",
    "    ax_e.set_title(f\"energies, MAE={mae:.4}, RMSE={rmse:.4}\")\n",
    "    ax_e.set_xlabel(\"actual\")\n",
    "    ax_e.set_ylabel(\"predicted\")\n",
    "\n",
    "    mae = loss_mae(predicted_f, reference_f)\n",
    "    rmse = loss_rmse(predicted_f, reference_f)\n",
    "    ax_f.scatter(reference_f.cpu(), predicted_f.cpu())\n",
    "    x = (torch.min(reference_f.flatten()).item(), torch.max(reference_f.flatten()).item())\n",
    "    ax_f.plot(x, x, color=\"red\")\n",
    "    ax_f.set_title(f\"forces, MAE={mae:.4}, RMSE={rmse:.4}\")\n",
    "    ax_f.set_xlabel(\"actual\")\n",
    "    ax_f.set_ylabel(\"predicted\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = parity_plots(*evaluate_model_with_forces(model, test_dataloader_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_finite_differences(model, frame, delta=1e-6):\n",
    "    frame = frame.copy()\n",
    "    delta_frame = frame.copy()\n",
    "    delta_frame.positions[3, 1] += delta\n",
    "\n",
    "    dataset = AtomisticDataset(\n",
    "        [frame, delta_frame], \n",
    "        all_species, \n",
    "        HYPERS_GRAD, \n",
    "        torch.zeros(2, 1), \n",
    "        radial_spectrum_n_max=12,\n",
    "    )\n",
    "    dataloader = create_dataloader(\n",
    "        dataset,\n",
    "        batch_size=len(dataset),\n",
    "        shuffle=False,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    for frame, radial_spectrum, spherical_expansions, _, _ in dataloader:\n",
    "        predicted_e, predicted_f = model(frame, radial_spectrum, spherical_expansions, forward_forces=True)\n",
    "\n",
    "    finite_diff = - (predicted_e[1] - predicted_e[0]) / delta\n",
    "    print(\"finite difference =\", finite_diff.item())\n",
    "    print(\"computed gradient =\", predicted_f[3, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.get_default_dtype() == torch.float64:\n",
    "    delta = 1e-6\n",
    "else:\n",
    "    delta = 1e-3\n",
    "\n",
    "check_finite_differences(model, train_frames[22], delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fdb860dc9f423b713ecf03ec3bee4ef4df65400e892ebee65bf9175396f229c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
