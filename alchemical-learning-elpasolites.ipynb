{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torchviz\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ase.io\n",
    "\n",
    "from utils.soap import compute_spherical_expansion_librascal\n",
    "\n",
    "from utils import models\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 250 training frames\n"
     ]
    }
   ],
   "source": [
    "n_test = 1000\n",
    "n_train = 250\n",
    "\n",
    "frames = ase.io.read(\"data/elpasolites_10590.xyz\", f\":\")\n",
    "energies = torch.tensor(np.loadtxt(\"data/elpasolites_10590_evpa.dat\"))\n",
    "\n",
    "train_frames = frames[:n_train]\n",
    "test_frames = frames[-n_test:]\n",
    "\n",
    "train_energies = energies[:n_train]\n",
    "test_energies = energies[-n_test:]\n",
    "\n",
    "print(f\"using {n_train} training frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_species = set()\n",
    "for frame in frames:\n",
    "    global_species.update(frame.numbers)\n",
    "\n",
    "global_species = list(map(lambda u: int(u), global_species))\n",
    "\n",
    "HYPERS_SMALL = {\n",
    "    \"interaction_cutoff\": 5.0,\n",
    "    \"max_angular\": 6,\n",
    "    \"max_radial\": 8,\n",
    "    \"gaussian_sigma_constant\": 0.3,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"radial_basis\": \"GTO\",\n",
    "    \"compute_gradients\": False,\n",
    "    \"expansion_by_species_method\": \"user defined\",\n",
    "    \"global_species\": global_species,\n",
    "}\n",
    "\n",
    "HYPERS_FROM_PAPER = {\n",
    "    \"interaction_cutoff\": 5.0,\n",
    "    \"max_angular\": 9,\n",
    "    \"max_radial\": 12,\n",
    "    \"gaussian_sigma_constant\": 0.3,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"radial_basis\": \"GTO\",\n",
    "    \"compute_gradients\": False,\n",
    "    \"expansion_by_species_method\": \"user defined\",\n",
    "    \"global_species\": global_species,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils.gap\n",
    "\n",
    "# def structure_sum(kernel):\n",
    "#     return utils.gap.common.SumStructureKernel.apply(kernel, test_slices, train_slices)\n",
    "\n",
    "# rand_kernel = torch.rand((len(test_slices), len(train_slices)), requires_grad=True)\n",
    "# torch.autograd.gradcheck(structure_sum, rand_kernel, fast_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_model(model, name, file):\n",
    "    predicted_energies_training_set = model(\n",
    "        train_spherical_expansions, train_species, train_slices\n",
    "    )\n",
    "\n",
    "    predicted_energies_test_set = model(\n",
    "        test_spherical_expansions, test_species, test_slices\n",
    "    )\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    train_loss = loss_fn(predicted_energies_training_set.squeeze(), train_energies)\n",
    "    test_loss = loss_fn(predicted_energies_test_set.squeeze(), test_energies)\n",
    "\n",
    "    train_loss *= 100 / train_energies.std()\n",
    "    test_loss *= 100 / test_energies.std()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax[0].scatter(train_energies, predicted_energies_training_set.detach().numpy())\n",
    "    x = np.linspace(train_energies.min(), train_energies.max(), 20)\n",
    "    ax[0].plot(x, x, color='r')\n",
    "\n",
    "    ax[0].set_title(f'Training set — loss = {train_loss:.3} %RMSE')\n",
    "    ax[0].set_xlabel('DFT')\n",
    "    ax[0].set_ylabel('Predicted')\n",
    "\n",
    "\n",
    "    ax[1].scatter(test_energies, predicted_energies_test_set.detach().numpy())\n",
    "    x = np.linspace(test_energies.min(), test_energies.max(), 20)\n",
    "    ax[1].plot(x, x, color='r')\n",
    "\n",
    "    ax[1].set_title(f'Test set — loss = {test_loss:.3} %RMSE')\n",
    "    ax[1].set_xlabel('DFT')\n",
    "    ax[1].set_ylabel('Predicted')\n",
    "\n",
    "    fig.suptitle(name)\n",
    "    fig.savefig(file, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomisticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, frames, hypers, energies):\n",
    "        self.spherical_expansions = []\n",
    "        for frame in frames:\n",
    "            se, slices = compute_spherical_expansion_librascal([frame], hypers)\n",
    "            self.spherical_expansions.append(se)\n",
    "        \n",
    "        self.species = [torch.tensor(frame.numbers) for frame in frames]\n",
    "        self.energies = energies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spherical_expansions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.spherical_expansions[idx], self.species[idx], self.energies[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data_cpu(data):\n",
    "    spherical_expansion = {\n",
    "        lambda_: torch.vstack([d[0][lambda_] for d in data])\n",
    "        for lambda_ in data[0][0].keys()\n",
    "    }\n",
    "\n",
    "    species = torch.hstack([d[1] for d in data])\n",
    "    energies = torch.vstack([d[2] for d in data])\n",
    "\n",
    "    slices = []\n",
    "    start = 0\n",
    "    for d in data:\n",
    "        stop = start + d[1].shape[0]\n",
    "        slices.append(slice(start, stop))\n",
    "        start = stop\n",
    "\n",
    "    return spherical_expansion, species, slices, energies\n",
    "\n",
    "def collate_data_gpu(data):\n",
    "    spherical_expansion, species, slices, energies = collate_data_cpu(data)\n",
    "\n",
    "    spherical_expansion = {\n",
    "        lambda_: se.to(device='cuda') for lambda_, se in spherical_expansion.items()\n",
    "    }\n",
    "\n",
    "    return spherical_expansion, species.to(device='cuda'), slices, energies.to(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AtomisticDataset(train_frames, HYPERS_SMALL, train_energies)\n",
    "test_dataset = AtomisticDataset(test_frames, HYPERS_SMALL, test_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_data_gpu if device == \"cuda\" else collate_data_cpu\n",
    ")\n",
    "\n",
    "# dataloader without batching for the initial fit\n",
    "train_dataloader_no_batch = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=len(train_dataset), \n",
    "    shuffle=False,\n",
    "    collate_fn=collate_data_gpu if device == \"cuda\" else collate_data_cpu\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    collate_fn=collate_data_gpu if device == \"cuda\" else collate_data_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_optimizer(predicted, actual, regularizer, weights):\n",
    "    loss = torch.linalg.norm(predicted.flatten() - actual.flatten()) ** 2\n",
    "    # regularize the loss, full dataset std\n",
    "    loss += regularizer / torch.std(train_energies.flatten()) * torch.linalg.norm(weights) ** 2\n",
    "\n",
    "    # TODO alternative: batch std\n",
    "    # loss += regularizer / torch.std(actual.flatten()) * torch.linalg.norm(weights) ** 2\n",
    "    return loss\n",
    "\n",
    "def loss_mae(predicted, actual):\n",
    "    return torch.mean(torch.abs(predicted.flatten() - actual.flatten()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PSEUDO_SPECIES = 4\n",
    "UPDATE_SUPPORT_POINTS = True\n",
    "REGULARIZER = 1e-4\n",
    "\n",
    "mixed_species_model = models.MixedSpeciesFullLinearGapModel(\n",
    "    global_species, \n",
    "    n_pseudo_species=N_PSEUDO_SPECIES, \n",
    "    lambdas=[REGULARIZER],\n",
    "    optimizable_weights=True,\n",
    "    random_initial_weights=True,\n",
    "    detach_support_points=False,\n",
    ")\n",
    "\n",
    "# mixed_species_model = models.MixedSpeciesLinearModel(\n",
    "#     global_species, \n",
    "#     n_pseudo_species=N_PSEUDO_SPECIES, \n",
    "#     lambdas=[REGULARIZER],\n",
    "#     optimizable_weights=True,\n",
    "#     random_initial_weights=True,\n",
    "# )\n",
    "\n",
    "# mixed_species_model = models.MixedSpeciesFullGapModel(\n",
    "#     global_species, \n",
    "#     n_pseudo_species=N_PSEUDO_SPECIES, \n",
    "#     lambdas=[REGULARIZER],\n",
    "#     zeta=1,\n",
    "#     optimizable_weights=True,\n",
    "#     random_initial_weights=True,\n",
    "#     detach_support_points=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_species_model.to(device=device)\n",
    "\n",
    "if mixed_species_model.optimizable_weights:\n",
    "    with torch.no_grad():\n",
    "        # initialize the weights to a nice value using fit\n",
    "        for spherical_expansions, species, slices, energies in train_dataloader_no_batch:\n",
    "            # we want to intially train the model on all frames, to ensure the\n",
    "            # support points come from the full dataset.\n",
    "            assert len(slices) == len(train_frames)\n",
    "            mixed_species_model.fit(spherical_expansions, species, slices, energies)\n",
    "\n",
    "if not mixed_species_model.optimizable_weights:\n",
    "    # we can not use batches if we are training with linear algebra, we need to\n",
    "    # have all training frames available\n",
    "    assert train_dataloader.batch_size >= len(train_frames)\n",
    "\n",
    "if mixed_species_model.optimizable_weights:\n",
    "    torch_loss_regularizer = REGULARIZER\n",
    "else:\n",
    "    torch_loss_regularizer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     mixed_species_model.parameters(), \n",
    "#     lr=lr, weight_decay=0.0\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.LBFGS(\n",
    "    mixed_species_model.parameters(), \n",
    "    lr=lr,\n",
    ")\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "filename = f\"{mixed_species_model.__class__.__name__}-{N_PSEUDO_SPECIES}-mixed-{n_train}-train\"\n",
    "if mixed_species_model.optimizable_weights:\n",
    "    filename += \"-opt-weights\"\n",
    "\n",
    "if mixed_species_model.random_initial_weights:\n",
    "    filename += \"-random-weights\"\n",
    "\n",
    "output = open(f\"{filename}.dat\", \"w\")\n",
    "output.write(\"# epoch  train_loss  test_mae\\n\")\n",
    "n_epochs_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 took 2.789s, optimizer loss=2.251e+09, test mae=725.1\n",
      "epoch 1 took 2.779s, optimizer loss=3.234e+08, test mae=169.2\n",
      "epoch 2 took 2.815s, optimizer loss=8.533e+06, test mae=24.94\n",
      "epoch 3 took 2.83s, optimizer loss=1.513e+05, test mae=7.849\n",
      "epoch 4 took 2.87s, optimizer loss=6.066e+04, test mae=5.874\n",
      "epoch 5 took 2.911s, optimizer loss=1.745e+04, test mae=5.573\n",
      "epoch 6 took 2.877s, optimizer loss=1.185e+04, test mae=3.246\n",
      "epoch 7 took 2.874s, optimizer loss=7.363e+03, test mae=2.607\n",
      "epoch 8 took 2.893s, optimizer loss=6.183e+03, test mae=2.805\n",
      "epoch 9 took 2.87s, optimizer loss=6.575e+03, test mae=2.495\n",
      "epoch 10 took 2.867s, optimizer loss=6.649e+03, test mae=2.603\n",
      "epoch 11 took 2.885s, optimizer loss=6.249e+03, test mae=2.158\n",
      "epoch 12 took 2.873s, optimizer loss=5.917e+03, test mae=2.219\n",
      "epoch 13 took 2.88s, optimizer loss=4.248e+03, test mae=1.939\n",
      "epoch 14 took 2.909s, optimizer loss=5.058e+03, test mae=1.94\n",
      "epoch 15 took 2.879s, optimizer loss=6.912e+03, test mae=2.066\n",
      "epoch 16 took 2.889s, optimizer loss=1.399e+04, test mae=2.23\n",
      "epoch 17 took 2.878s, optimizer loss=1.103e+04, test mae=2.035\n",
      "epoch 18 took 2.868s, optimizer loss=5.897e+03, test mae=1.855\n",
      "epoch 19 took 2.879s, optimizer loss=3.705e+03, test mae=1.691\n",
      "epoch 20 took 2.871s, optimizer loss=4.588e+03, test mae=1.805\n",
      "epoch 21 took 2.873s, optimizer loss=4.417e+03, test mae=1.737\n",
      "epoch 22 took 2.889s, optimizer loss=5.431e+03, test mae=1.833\n",
      "epoch 23 took 2.874s, optimizer loss=5.265e+03, test mae=1.781\n",
      "epoch 24 took 2.913s, optimizer loss=6.207e+03, test mae=1.879\n",
      "epoch 25 took 2.887s, optimizer loss=5.614e+03, test mae=1.697\n",
      "epoch 26 took 2.869s, optimizer loss=9.387e+03, test mae=1.996\n",
      "epoch 27 took 2.876s, optimizer loss=8.617e+03, test mae=1.757\n",
      "epoch 28 took 2.879s, optimizer loss=1.661e+04, test mae=2.17\n",
      "epoch 29 took 2.868s, optimizer loss=1.415e+04, test mae=2.017\n",
      "epoch 30 took 2.881s, optimizer loss=2.458e+04, test mae=2.481\n",
      "epoch 31 took 2.877s, optimizer loss=2.357e+04, test mae=2.036\n",
      "epoch 32 took 2.868s, optimizer loss=4.526e+04, test mae=3.091\n",
      "epoch 33 took 2.878s, optimizer loss=3.131e+04, test mae=2.089\n",
      "epoch 34 took 2.886s, optimizer loss=5.028e+04, test mae=2.973\n",
      "epoch 35 took 2.872s, optimizer loss=3.589e+04, test mae=2.3\n",
      "epoch 36 took 2.898s, optimizer loss=4.609e+04, test mae=2.917\n",
      "epoch 37 took 2.886s, optimizer loss=3.022e+04, test mae=2.135\n",
      "epoch 38 took 2.872s, optimizer loss=5.355e+04, test mae=3.029\n",
      "epoch 39 took 2.879s, optimizer loss=3.678e+04, test mae=2.293\n",
      "epoch 40 took 2.88s, optimizer loss=7.059e+04, test mae=2.912\n",
      "epoch 41 took 2.883s, optimizer loss=5.768e+04, test mae=2.579\n",
      "epoch 42 took 2.873s, optimizer loss=1.726e+05, test mae=4.023\n",
      "epoch 43 took 2.892s, optimizer loss=2.065e+05, test mae=12.26\n",
      "epoch 44 took 2.872s, optimizer loss=4.98e+04, test mae=3.804\n",
      "epoch 45 took 2.881s, optimizer loss=8.066e+03, test mae=2.171\n",
      "epoch 46 took 2.886s, optimizer loss=4.509e+03, test mae=2.087\n",
      "epoch 47 took 2.882s, optimizer loss=2.507e+03, test mae=2.317\n",
      "epoch 48 took 2.882s, optimizer loss=4.017e+03, test mae=1.473\n",
      "epoch 49 took 2.901s, optimizer loss=1.364e+03, test mae=1.404\n",
      "epoch 50 took 2.874s, optimizer loss=893.6, test mae=1.343\n",
      "epoch 51 took 2.887s, optimizer loss=818.6, test mae=1.315\n",
      "epoch 52 took 2.878s, optimizer loss=735.4, test mae=1.3\n",
      "epoch 53 took 2.872s, optimizer loss=783.5, test mae=1.277\n",
      "epoch 54 took 2.878s, optimizer loss=665.2, test mae=1.434\n",
      "epoch 55 took 2.88s, optimizer loss=796.6, test mae=1.907\n",
      "epoch 56 took 2.881s, optimizer loss=2.948e+03, test mae=2.372\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4026954/3876928783.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/alchemical-learning/virtualenv/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/alchemical-learning/virtualenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/alchemical-learning/virtualenv/lib/python3.8/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(400):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    if UPDATE_SUPPORT_POINTS:\n",
    "        # to update the support points, we need to have all training data at\n",
    "        # once in memory\n",
    "        for spherical_expansions, species, slices, _ in train_dataloader_no_batch:\n",
    "            assert len(slices) == len(train_frames)\n",
    "            # use `select_again=True` to re-select the same number of support\n",
    "            # points. this might make convergence slower, but maybe able to\n",
    "            # reach a lower final loss?\n",
    "            #\n",
    "            # with `select_again=False`, the environments selected in the first\n",
    "            # fit above are used as support points\n",
    "            mixed_species_model.update_support_points(\n",
    "                spherical_expansions, species, slices, select_again=False\n",
    "            )\n",
    "\n",
    "    for spherical_expansions, species, slices, energies in train_dataloader_no_batch:\n",
    "        def single_step():\n",
    "            optimizer.zero_grad()\n",
    "           \n",
    "            if not mixed_species_model.optimizable_weights:\n",
    "                mixed_species_model.fit(spherical_expansions, species, slices, energies)\n",
    "                \n",
    "            predicted = mixed_species_model(spherical_expansions, species, slices)    \n",
    "\n",
    "            loss = loss_optimizer(\n",
    "                predicted, \n",
    "                energies, \n",
    "                torch_loss_regularizer, \n",
    "                mixed_species_model.model.weights\n",
    "            )\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(single_step)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            predicted = []\n",
    "            for spherical_expansions, species, slices, energies in test_dataloader:\n",
    "                predicted.append(mixed_species_model(spherical_expansions, species, slices))\n",
    "\n",
    "            predicted = torch.vstack(predicted)\n",
    "            mae = loss_mae(predicted.cpu(), test_dataset.energies)\n",
    "\n",
    "            output.write(f\"{n_epochs_total} {loss} {mae}\\n\")\n",
    "            output.flush()\n",
    "\n",
    "        print(f\"epoch {n_epochs_total} took {epoch_time:.4}s, optimizer loss={loss.item():.4}, test mae={mae:.4}\")\n",
    "        \n",
    "    n_epochs_total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(all_losses)\n",
    "plt.semilogy(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5ef12eb5922cf8d604aeb1dfa412e3da3665f54b5e701463f21d300328cee18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
