{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ase.io\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "from utils.dataset import AtomisticDataset, create_dataloader\n",
    "from utils.soap import PowerSpectrum\n",
    "from utils.combine import CombineSpecies, CombineRadial, CombineRadialSpecies\n",
    "from utils.linear import LinearModel\n",
    "from utils.operations import SumStructures, remove_gradient\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 100 training frames\n"
     ]
    }
   ],
   "source": [
    "n_test = 100\n",
    "n_train = 100\n",
    "\n",
    "frames = ase.io.read(\"data/elpasolites_10590.xyz\", f\":\")\n",
    "energies = torch.tensor(np.loadtxt(\"data/elpasolites_10590_evpa.dat\"))\n",
    "\n",
    "train_frames = frames[:n_train]\n",
    "test_frames = frames[-n_test:]\n",
    "\n",
    "train_energies = energies[:n_train].reshape(-1, 1)\n",
    "test_energies = energies[-n_test:].reshape(-1, 1)\n",
    "\n",
    "print(f\"using {n_train} training frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_species = set()\n",
    "for frame in frames:\n",
    "    all_species.update(frame.numbers)\n",
    "\n",
    "all_species = list(map(lambda u: int(u), all_species))\n",
    "\n",
    "HYPERS_FROM_PAPER = {\n",
    "    \"cutoff\": 5.0,\n",
    "    \"max_angular\": 9,\n",
    "    \"max_radial\": 12,\n",
    "    \"atomic_gaussian_width\": 0.3,\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"gradients\": False,\n",
    "}\n",
    "\n",
    "HYPERS_SMALL = {\n",
    "    \"cutoff\": 5.0,\n",
    "    \"max_angular\": 3,\n",
    "    \"max_radial\": 4,\n",
    "    \"atomic_gaussian_width\": 0.3,\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"gradients\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AtomisticDataset(train_frames, all_species, HYPERS_SMALL, train_energies)\n",
    "test_dataset = AtomisticDataset(test_frames, all_species, HYPERS_SMALL, test_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "train_dataloader_no_batch = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=len(train_dataset),\n",
    "    shuffle=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "test_dataloader = create_dataloader(\n",
    "    test_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_optimizer(predicted, actual, regularizer, weights):\n",
    "    loss = torch.linalg.norm(predicted.flatten() - actual.flatten()) ** 2\n",
    "    # regularize the loss, full dataset std\n",
    "    loss += regularizer / torch.std(train_energies.flatten()) * torch.linalg.norm(weights) ** 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_mae(predicted, actual):\n",
    "    return torch.mean(torch.abs(predicted.flatten() - actual.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLinearModel(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "        combiner,\n",
    "        regularizer,\n",
    "        optimizable_weights,\n",
    "        random_initial_weights,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sum_structure = SumStructures()\n",
    "        self.combiner = combiner\n",
    "        self.power_spectrum = PowerSpectrum()\n",
    "        self.model = LinearModel(\n",
    "            regularizer=regularizer,\n",
    "            optimizable_weights=optimizable_weights,\n",
    "            random_initial_weights=random_initial_weights,\n",
    "        )\n",
    "\n",
    "        self.optimizable_weights = optimizable_weights\n",
    "        self.random_initial_weights = random_initial_weights\n",
    "\n",
    "    def forward(self, spherical_expansion, forward_forces=False):\n",
    "        if not forward_forces:\n",
    "            # remove gradients from the spherical expansion if we don't need it\n",
    "            spherical_expansion = remove_gradient(spherical_expansion)\n",
    "\n",
    "        combined = self.combiner(spherical_expansion)\n",
    "        power_spectrum = self.power_spectrum(combined)        \n",
    "        power_spectrum_per_structure = self.sum_structure(power_spectrum)\n",
    "        energies, forces = self.model(power_spectrum_per_structure, with_forces=forward_forces)\n",
    "        return energies, forces\n",
    "\n",
    "    def initialize_model_weights(self, spherical_expansion, energies, forces=None):\n",
    "        if forces is None:\n",
    "            # remove gradients from the spherical expansion if we don't need it\n",
    "            spherical_expansion = remove_gradient(spherical_expansion)\n",
    "\n",
    "        combined = self.combiner(spherical_expansion)\n",
    "        power_spectrum = self.power_spectrum(combined)\n",
    "        \n",
    "        power_spectrum_per_structure = self.sum_structure(power_spectrum)\n",
    "        self.model.initialize_model_weights(power_spectrum_per_structure, energies, forces)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PSEUDO_SPECIES = 4\n",
    "TORCH_REGULARIZER = 1e-2\n",
    "LINALG_REGULARIZER_ENERGIES = 1e-2\n",
    "LINALG_REGULARIZER_FORCES = 1e-1\n",
    "\n",
    "N_COMBINED_RADIAL = 4\n",
    "\n",
    "# species combination only\n",
    "combiner = CombineSpecies(species=all_species, n_pseudo_species=N_PSEUDO_SPECIES)\n",
    "\n",
    "# # species combination and then radial basis combination\n",
    "# combiner = torch.nn.Sequential(\n",
    "#     CombineSpecies(species=all_species, n_pseudo_species=N_PSEUDO_SPECIES),\n",
    "#     CombineRadial(max_radial=HYPERS_SMALL[\"max_radial\"], n_combined_radial=N_COMBINED_RADIAL),\n",
    "# )\n",
    "\n",
    "# # combine both radial and species information at the same time\n",
    "# combiner = CombineRadialSpecies(\n",
    "#     n_species=len(all_species), \n",
    "#     max_radial=HYPERS_SMALL[\"max_radial\"], \n",
    "#     n_combined_basis=N_COMBINED_RADIAL*N_PSEUDO_SPECIES,\n",
    "# )\n",
    "\n",
    "model = CombinedLinearModel(\n",
    "    combiner=combiner, \n",
    "    regularizer=[LINALG_REGULARIZER_ENERGIES, LINALG_REGULARIZER_FORCES],\n",
    "    optimizable_weights=True,\n",
    "    random_initial_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device=device, dtype=torch.get_default_dtype())\n",
    "\n",
    "# initialize the model\n",
    "with torch.no_grad():\n",
    "    for spherical_expansions, energies, _ in train_dataloader_no_batch:\n",
    "        # we want to intially train the model on all frames, to ensure the\n",
    "        # support points come from the full dataset.\n",
    "        model.initialize_model_weights(spherical_expansions, energies)\n",
    "\n",
    "if model.optimizable_weights:\n",
    "    torch_loss_regularizer = TORCH_REGULARIZER\n",
    "else:\n",
    "    torch_loss_regularizer = 0\n",
    "    # we can not use batches if we are training with linear algebra, we need to\n",
    "    # have all training frames available\n",
    "    assert train_dataloader.batch_size >= len(train_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "filename = f\"{model.__class__.__name__}-{N_PSEUDO_SPECIES}-mixed-{n_train}-train\"\n",
    "if model.optimizable_weights:\n",
    "    filename += \"-opt-weights\"\n",
    "\n",
    "if model.random_initial_weights:\n",
    "    filename += \"-random-weights\"\n",
    "\n",
    "output = open(f\"{filename}.dat\", \"w\")\n",
    "output.write(\"# epoch  train_loss  test_mae\\n\")\n",
    "n_epochs_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 took 0.9638s, optimizer loss=132.5, test mae=0.56\n",
      "epoch 1 took 0.9334s, optimizer loss=57.69, test mae=0.562\n",
      "epoch 2 took 0.943s, optimizer loss=42.75, test mae=0.654\n",
      "epoch 3 took 0.9536s, optimizer loss=24.07, test mae=0.62\n",
      "epoch 4 took 0.9591s, optimizer loss=7.825, test mae=0.6667\n",
      "epoch 5 took 0.9606s, optimizer loss=3.837, test mae=0.7027\n",
      "epoch 6 took 0.9896s, optimizer loss=2.306, test mae=0.7104\n",
      "epoch 7 took 0.9644s, optimizer loss=1.331, test mae=0.6919\n",
      "epoch 8 took 0.9791s, optimizer loss=0.6194, test mae=0.6288\n",
      "epoch 9 took 0.9742s, optimizer loss=0.2764, test mae=0.5961\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    for spherical_expansions, energies, _ in train_dataloader:\n",
    "        def single_step():\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            assert model.optimizable_weights\n",
    "                \n",
    "            predicted, _ = model(spherical_expansions, forward_forces=False)\n",
    "\n",
    "            loss = loss_optimizer(\n",
    "                predicted, \n",
    "                energies, \n",
    "                torch_loss_regularizer, \n",
    "                model.model.weights\n",
    "            )\n",
    "            loss.backward(retain_graph=False)\n",
    "\n",
    "            return loss\n",
    "            \n",
    "        loss = optimizer.step(single_step)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            predicted = []\n",
    "            reference = []\n",
    "            for spherical_expansions, energies, _ in test_dataloader:\n",
    "                reference.append(energies)\n",
    "                predicted_e, _ = model(spherical_expansions, forward_forces=False)\n",
    "                predicted.append(predicted_e)\n",
    "\n",
    "            reference = torch.vstack(reference)\n",
    "            predicted = torch.vstack(predicted)\n",
    "            mae = loss_mae(predicted, reference)\n",
    "\n",
    "            output.write(f\"{n_epochs_total} {loss} {mae}\\n\")\n",
    "            output.flush()\n",
    "\n",
    "        print(f\"epoch {n_epochs_total} took {epoch_time:.4}s, optimizer loss={loss.item():.4}, test mae={mae:.4}\")\n",
    "        \n",
    "    n_epochs_total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fdb860dc9f423b713ecf03ec3bee4ef4df65400e892ebee65bf9175396f229c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
