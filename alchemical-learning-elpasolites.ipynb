{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchviz\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ase.io\n",
    "\n",
    "from utils.soap import compute_spherical_expansion_librascal, PowerSpectrum\n",
    "from utils.gap import train_sparse_gap_model, train_per_species_sparse_gap_model, train_full_gap_model\n",
    "from utils.alchemical import AlchemicalCombine\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = ase.io.read(\"data/elpasolites_10590.xyz\", \":300\")\n",
    "energies = torch.tensor(np.loadtxt(\"data/elpasolites_10590_evpa.dat\")[:300])\n",
    "\n",
    "n_train = int(0.8 * len(frames))\n",
    "\n",
    "train_frames = frames[:n_train]\n",
    "test_frames = frames[n_train:]\n",
    "\n",
    "train_energies = energies[:n_train]\n",
    "test_energies = energies[n_train:]\n",
    "\n",
    "print(f\"using {n_train} training frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_species = set()\n",
    "for frame in frames:\n",
    "    global_species.update(frame.numbers)\n",
    "\n",
    "global_species = list(map(lambda u: int(u), global_species))\n",
    "\n",
    "HYPERS = {\n",
    "    \"interaction_cutoff\": 5.0,\n",
    "    \"max_angular\": 4,\n",
    "    \"max_radial\": 4,\n",
    "    \"gaussian_sigma_constant\": 0.3,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"radial_basis\": \"GTO\",\n",
    "    \"compute_gradients\": False,\n",
    "    \"expansion_by_species_method\": \"user defined\",\n",
    "    \"global_species\": global_species,\n",
    "}\n",
    "\n",
    "HYPERS_MJW = {\n",
    "    \"interaction_cutoff\": 5.0,\n",
    "    \"max_angular\": 9,\n",
    "    \"max_radial\": 12,\n",
    "    \"gaussian_sigma_constant\": 0.3,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"radial_basis\": \"GTO\",\n",
    "    \"compute_gradients\": False,\n",
    "    \"expansion_by_species_method\": \"user defined\",\n",
    "    \"global_species\": global_species,\n",
    "    # ?? central atom weight ≠ 1 ??\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_spherical_expansions, train_slices = compute_spherical_expansion_librascal(train_frames, HYPERS)\n",
    "# test_spherical_expansions, test_slices = compute_spherical_expansion_librascal(test_frames, HYPERS)\n",
    "\n",
    "# train_species = torch.hstack([torch.tensor(frame.numbers) for frame in train_frames])\n",
    "# test_species = torch.hstack([torch.tensor(frame.numbers) for frame in test_frames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils.gap\n",
    "\n",
    "# def structure_sum(kernel):\n",
    "#     return utils.gap.common.SumStructureKernel.apply(kernel, test_slices, train_slices)\n",
    "\n",
    "# rand_kernel = torch.rand((len(test_slices), len(train_slices)), requires_grad=True)\n",
    "# torch.autograd.gradcheck(structure_sum, rand_kernel, fast_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: GAP model without species combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_model(model, name, file):\n",
    "    predicted_energies_training_set = model(\n",
    "        train_spherical_expansions, train_species, train_slices\n",
    "    )\n",
    "\n",
    "    predicted_energies_test_set = model(\n",
    "        test_spherical_expansions, test_species, test_slices\n",
    "    )\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    train_loss = loss_fn(predicted_energies_training_set.squeeze(), train_energies)\n",
    "    test_loss = loss_fn(predicted_energies_test_set.squeeze(), test_energies)\n",
    "\n",
    "    train_loss *= 100 / train_energies.std()\n",
    "    test_loss *= 100 / test_energies.std()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax[0].scatter(train_energies, predicted_energies_training_set.detach().numpy())\n",
    "    x = np.linspace(train_energies.min(), train_energies.max(), 20)\n",
    "    ax[0].plot(x, x, color='r')\n",
    "\n",
    "    ax[0].set_title(f'Training set — loss = {train_loss:.3} %RMSE')\n",
    "    ax[0].set_xlabel('DFT')\n",
    "    ax[0].set_ylabel('Predicted')\n",
    "\n",
    "\n",
    "    ax[1].scatter(test_energies, predicted_energies_test_set.detach().numpy())\n",
    "    x = np.linspace(test_energies.min(), test_energies.max(), 20)\n",
    "    ax[1].plot(x, x, color='r')\n",
    "\n",
    "    ax[1].set_title(f'Test set — loss = {test_loss:.3} %RMSE')\n",
    "    ax[1].set_xlabel('DFT')\n",
    "    ax[1].set_ylabel('Predicted')\n",
    "\n",
    "    fig.suptitle(name)\n",
    "    fig.savefig(file, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullGap(torch.nn.Module):\n",
    "    def __init__(self, zeta, lambdas):\n",
    "        super().__init__()\n",
    "        self.power_spectrum = PowerSpectrum()\n",
    "\n",
    "        self.zeta = zeta\n",
    "        self.lambdas = lambdas\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, spherical_expansion, all_species, structures_slices, energies):\n",
    "        power_spectrum = self.power_spectrum(spherical_expansion)\n",
    "\n",
    "        self.model = train_full_gap_model(\n",
    "            power_spectrum,\n",
    "            all_species,\n",
    "            structures_slices,\n",
    "            energies, \n",
    "            zeta=self.zeta, \n",
    "            lambdas=self.lambdas\n",
    "        )\n",
    "\n",
    "    def forward(self, spherical_expansion, all_species, structures_slices):\n",
    "        ps = self.power_spectrum(spherical_expansion)\n",
    "        return self.model(ps, all_species, structures_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_gap = FullGap(zeta=2, lambdas=[1e-6, 1e-6])\n",
    "# full_gap.fit(train_spherical_expansions, [], train_slices, train_energies)\n",
    "\n",
    "# evaluate_and_plot_model(\n",
    "#     full_gap, \n",
    "#     f\"Full GAP model\",\n",
    "#     \"full-gap-model.pdf\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGap(torch.nn.Module):\n",
    "    def __init__(self, n_support, zeta, lambdas):\n",
    "        super().__init__()\n",
    "        self.power_spectrum = PowerSpectrum()\n",
    "\n",
    "        self.n_support = n_support\n",
    "        self.zeta = zeta\n",
    "        self.lambdas = lambdas\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, spherical_expansion, all_species, structures_slices, energies):\n",
    "        power_spectrum = self.power_spectrum(spherical_expansion)\n",
    "\n",
    "        self.model = train_sparse_gap_model(\n",
    "            power_spectrum, \n",
    "            all_species,\n",
    "            structures_slices,\n",
    "            energies, \n",
    "            self.n_support, \n",
    "            zeta=self.zeta, \n",
    "            lambdas=self.lambdas\n",
    "        )\n",
    "\n",
    "    def forward(self, spherical_expansion, all_species, structures_slices):\n",
    "        ps = self.power_spectrum(spherical_expansion)\n",
    "        return self.model(ps, all_species, structures_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_support = 100\n",
    "\n",
    "# sparse_gap = SparseGap(n_support=n_support, zeta=2, lambdas=[1e-6, 1e-6])\n",
    "# sparse_gap.fit(train_spherical_expansions, [], train_slices, train_energies)\n",
    "\n",
    "# evaluate_and_plot_model(\n",
    "#     sparse_gap, \n",
    "#     f\"Sparse GAP model — {n_support} GAP support point\",\n",
    "#     \"sparse-gap-model.pdf\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse GAP, one model per central atom species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerSpeciesSparseGap(torch.nn.Module):\n",
    "    def __init__(self, n_support, zeta, lambdas):\n",
    "        super().__init__()\n",
    "        self.power_spectrum = PowerSpectrum()\n",
    "\n",
    "        self.n_support = n_support\n",
    "        self.zeta = zeta\n",
    "        self.lambdas = lambdas\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, spherical_expansion, all_species, structures_slices, energies):\n",
    "        power_spectrum = self.power_spectrum(spherical_expansion)\n",
    "\n",
    "        self.model = train_per_species_sparse_gap_model(\n",
    "            power_spectrum, \n",
    "            all_species,\n",
    "            structures_slices,\n",
    "            energies, \n",
    "            self.n_support, \n",
    "            zeta=self.zeta, \n",
    "            lambdas=self.lambdas\n",
    "        )\n",
    "\n",
    "    def forward(self, spherical_expansion, all_species, structures_slices):\n",
    "        ps = self.power_spectrum(spherical_expansion)\n",
    "        return self.model(ps, all_species, structures_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_support = {\n",
    "#     species: 5 for species in global_species\n",
    "# }\n",
    "\n",
    "# per_species_sparse_model = PerSpeciesSparseGap(n_support, zeta=2, lambdas=[1e-6, 1e-6])\n",
    "# per_species_sparse_model.fit(train_spherical_expansions, train_species, train_slices, train_energies)\n",
    "\n",
    "# evaluate_and_plot_model(\n",
    "#     per_species_sparse_model, \n",
    "#     f\"Basic sparse model — {sum(n_support.values())} GAP support point\",\n",
    "#     \"basic-sparse-model.pdf\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot computational graph on a smaller dataset\n",
    "# small_hypers = copy.deepcopy(HYPERS)\n",
    "# small_hypers[\"max_angular\"] = 1\n",
    "# small_hypers[\"max_radial\"] = 1\n",
    "# small_hypers[\"global_species\"] = [6, 1]\n",
    "\n",
    "# small_train_frames = methane_frames[2:]\n",
    "# small_test_frames = methane_frames[:2]\n",
    "\n",
    "# small_train_energies = torch.tensor([f.info[\"energy\"] for f in small_train_frames])\n",
    "\n",
    "# small_train_spherical_expansions, small_train_slices = compute_spherical_expansion_librascal(\n",
    "#     small_train_frames, small_hypers\n",
    "# )\n",
    "# small_train_species = torch.hstack([torch.tensor(frame.numbers) for frame in small_train_frames])\n",
    "\n",
    "# small_test_spherical_expansions, small_test_slices = compute_spherical_expansion_librascal(\n",
    "#     small_test_frames, small_hypers\n",
    "# )\n",
    "# small_test_species = torch.hstack([torch.tensor(frame.numbers) for frame in small_test_frames])\n",
    "\n",
    "# small_n_support = {1: 10, 6: 10}\n",
    "\n",
    "# small_model = BasicSparseGap(small_n_support, zeta=2, lambdas=[1e-6, 1e-6])\n",
    "# small_model.fit(\n",
    "#     small_train_spherical_expansions, \n",
    "#     small_train_species, \n",
    "#     small_train_slices, \n",
    "#     small_train_energies\n",
    "# )\n",
    "\n",
    "# torchviz_params = {}\n",
    "# for l, sph in small_test_spherical_expansions.items():\n",
    "#     sph.requires_grad_(True)\n",
    "#     torchviz_params[f\"sph l={l}\"] = sph\n",
    "\n",
    "# for s, w in small_model.model.weights.items():\n",
    "#     w.requires_grad_(True)\n",
    "#     torchviz_params[f\"weight species={s}\"] = w\n",
    "\n",
    "# result = small_model(small_test_spherical_expansions, small_test_species, small_test_slices)\n",
    "\n",
    "# torchviz.make_dot(result, params=torchviz_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedSpeciesFullGap(torch.nn.Module):\n",
    "    def __init__(self, species, n_pseudo_species, zeta, lambdas, optimizable_weights):\n",
    "        super().__init__()\n",
    "        self.power_spectrum = PowerSpectrum()\n",
    "        self.alchemical = AlchemicalCombine(species, n_pseudo_species)\n",
    "\n",
    "        self.zeta = zeta\n",
    "        self.lambdas = lambdas\n",
    "\n",
    "        self.optimizable_weights = optimizable_weights\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, spherical_expansion, all_species, structures_slices, energies):\n",
    "        combined = self.alchemical(spherical_expansion)\n",
    "        power_spectrum = self.power_spectrum(combined)\n",
    "        \n",
    "        self.model = train_full_gap_model(\n",
    "            power_spectrum, \n",
    "            all_species,\n",
    "            structures_slices,\n",
    "            energies, \n",
    "            zeta=self.zeta, \n",
    "            lambdas=self.lambdas,\n",
    "            optimizable_weights=self.optimizable_weights,\n",
    "        )\n",
    "\n",
    "    def forward(self, spherical_expansion, all_species, structures_slices):\n",
    "        combined = self.alchemical(spherical_expansion)\n",
    "        ps = self.power_spectrum(combined)\n",
    "        return self.model(ps, all_species, structures_slices)\n",
    "\n",
    "\n",
    "class MixedSpeciesSparseGap(torch.nn.Module):\n",
    "    def __init__(self, species, n_pseudo_species, n_support, zeta, lambdas, optimizable_weights):\n",
    "        super().__init__()\n",
    "        self.power_spectrum = PowerSpectrum()\n",
    "        self.alchemical = AlchemicalCombine(species, n_pseudo_species)\n",
    "\n",
    "        self.zeta = zeta\n",
    "        self.lambdas = lambdas\n",
    "\n",
    "        self.n_support = n_support\n",
    "\n",
    "        self.optimizable_weights = optimizable_weights\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, spherical_expansion, all_species, structures_slices, energies):\n",
    "        combined = self.alchemical(spherical_expansion)\n",
    "        power_spectrum = self.power_spectrum(combined)\n",
    "        \n",
    "        self.model = train_sparse_gap_model(\n",
    "            power_spectrum, \n",
    "            all_species,\n",
    "            structures_slices,\n",
    "            energies, \n",
    "            self.n_support, \n",
    "            zeta=self.zeta, \n",
    "            lambdas=self.lambdas,\n",
    "            optimizable_weights=self.optimizable_weights,\n",
    "        )\n",
    "\n",
    "    def forward(self, spherical_expansion, all_species, structures_slices):\n",
    "        combined = self.alchemical(spherical_expansion)\n",
    "        ps = self.power_spectrum(combined)\n",
    "        return self.model(ps, all_species, structures_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomisticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, frames, hypers, energies):\n",
    "        self.spherical_expansions = []\n",
    "        for frame in frames:\n",
    "            se, slices = compute_spherical_expansion_librascal([frame], hypers)\n",
    "            self.spherical_expansions.append(se)\n",
    "        \n",
    "        self.species = [torch.tensor(frame.numbers) for frame in frames]\n",
    "        self.energies = energies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spherical_expansions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.spherical_expansions[idx], self.species[idx], self.energies[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data_cpu(data):\n",
    "    spherical_expansion = {\n",
    "        lambda_: torch.vstack([d[0][lambda_] for d in data])\n",
    "        for lambda_ in data[0][0].keys()\n",
    "    }\n",
    "\n",
    "    species = torch.hstack([d[1] for d in data])\n",
    "    energies = torch.vstack([d[2] for d in data])\n",
    "\n",
    "    slices = []\n",
    "    start = 0\n",
    "    for d in data:\n",
    "        stop = start + d[1].shape[0]\n",
    "        slices.append(slice(start, stop))\n",
    "        start = stop\n",
    "\n",
    "    return spherical_expansion, species, slices, energies\n",
    "\n",
    "def collate_data_gpu(data):\n",
    "    spherical_expansion, species, slices, energies = collate_data_cpu(data)\n",
    "\n",
    "    spherical_expansion = {\n",
    "        lambda_: se.to(device='cuda') for lambda_, se in spherical_expansion.items()\n",
    "    }\n",
    "\n",
    "    return spherical_expansion, species.to(device='cuda'), slices, energies.to(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AtomisticDataset(train_frames, HYPERS_MJW, train_energies)\n",
    "test_dataset = AtomisticDataset(test_frames, HYPERS_MJW, test_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PSEUDO_SPECIES = 4\n",
    "\n",
    "# Full kernel, optimize everything with gradients\n",
    "mixed_species_model = MixedSpeciesFullGap(\n",
    "    global_species, \n",
    "    n_pseudo_species=N_PSEUDO_SPECIES, \n",
    "    zeta=1, \n",
    "    lambdas=[1e-3],\n",
    "    optimizable_weights=True,\n",
    ")\n",
    "\n",
    "# # Full kernel, optimize species with gradients, weights with linear algebra\n",
    "# mixed_species_model = MixedSpeciesFullGap(\n",
    "#     global_species, \n",
    "#     n_pseudo_species=N_PSEUDO_SPECIES, \n",
    "#     zeta=1, \n",
    "#     lambdas=[1e-1, 1e-6],\n",
    "#     optimizable_weights=False,\n",
    "# )\n",
    "\n",
    "# # Sparse kernel, optimize species with gradients, weights with linear algebra\n",
    "# mixed_species_model = MixedSpeciesSparseGap(\n",
    "#     global_species, \n",
    "#     n_support=100,\n",
    "#     n_pseudo_species=N_PSEUDO_SPECIES, \n",
    "#     zeta=1, \n",
    "#     lambdas=[1e-1, 1e-6],\n",
    "#     optimizable_weights=False,\n",
    "# )\n",
    "\n",
    "# # Sparse kernel, optimize everything with gradients\n",
    "# # TODO: this fails since sparse points are not re-selected at each step\n",
    "# mixed_species_model = MixedSpeciesSparseGap(\n",
    "#     global_species, \n",
    "#     n_support=100,\n",
    "#     n_pseudo_species=N_PSEUDO_SPECIES, \n",
    "#     zeta=1, \n",
    "#     lambdas=[1e-3, 1e-6],\n",
    "#     optimizable_weights=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "mixed_species_model.to(device=device)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=True,\n",
    "    collate_fn= collate_data_gpu if device == \"cuda\" else collate_data_cpu\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    collate_fn= collate_data_gpu if device == \"cuda\" else collate_data_cpu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def loss_optimizer(predicted, actual, regularizer, weights):\n",
    "    loss = torch.linalg.norm(predicted - actual) ** 2\n",
    "    # regularize the loss, full dataset std\n",
    "    loss += regularizer / torch.std(train_energies) * torch.linalg.norm(weights) ** 2\n",
    "\n",
    "    # TODO alternative: batch std\n",
    "    # loss += regularizer / torch.std(actual) * torch.linalg.norm(weights) ** 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_mae(predicted, actual):\n",
    "    return torch.mean(torch.abs(predicted - actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "regularizer = 1e-3\n",
    "\n",
    "optimizer = torch.optim.AdamW(mixed_species_model.parameters(), lr=0.1)\n",
    "# optimizer = torch.optim.LBFGS(mixed_species_model.parameters(), lr=1)\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "# initialize the weights to an OK value using fit\n",
    "if mixed_species_model.optimizable_weights:\n",
    "    for spherical_expansions, species, slices, energies in train_dataloader:\n",
    "        mixed_species_model.fit(spherical_expansions, species, slices, energies)\n",
    "        break # only use the first batch\n",
    "\n",
    "if not mixed_species_model.optimizable_weights:\n",
    "    regularizer = 0\n",
    "\n",
    "for epoch in range(500):\n",
    "    epoch_start = time.time()\n",
    "    for spherical_expansions, species, slices, energies in train_dataloader:\n",
    "        def single_step():\n",
    "            optimizer.zero_grad()\n",
    "           \n",
    "            if not mixed_species_model.optimizable_weights:\n",
    "                mixed_species_model.fit(spherical_expansions, species, slices, energies)\n",
    "                \n",
    "            predicted = mixed_species_model(spherical_expansions, species, slices)    \n",
    "\n",
    "            loss = loss_optimizer(predicted, energies, regularizer, mixed_species_model.model.weights)\n",
    "            loss.backward()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(single_step)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    if epoch % 10 == 0:\n",
    "        predicted = []\n",
    "        for spherical_expansions, species, slices, energies in test_dataloader:\n",
    "            predicted.append(mixed_species_model(spherical_expansions, species, slices))\n",
    "\n",
    "        predicted = torch.vstack(predicted)\n",
    "        mae = loss_mae(predicted.cpu(), test_dataset.energies)\n",
    "\n",
    "        print(f\"epoch {epoch} took {epoch_time:.4}s, optimizer loss={loss.item():.4}, test mae={mae:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_combining_matrix = mixed_species_model.alchemical.combining_matrix.detach().cpu().numpy()\n",
    "\n",
    "plt.scatter(species_combining_matrix[:, 0], species_combining_matrix[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for spherical_expansions, species, slices, energies in test_dataloader:\n",
    "    predicted.append(mixed_species_model(spherical_expansions, species, slices))\n",
    "\n",
    "predicted = torch.vstack(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_dataset.energies, predicted.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.abs(test_dataset.energies - predicted.cpu().squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5ef12eb5922cf8d604aeb1dfa412e3da3665f54b5e701463f21d300328cee18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
