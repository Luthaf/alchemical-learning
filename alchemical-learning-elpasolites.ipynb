{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#import torchviz\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ase.io\n",
    "\n",
    "from utils.dataset import AtomisticDataset, create_dataloader\n",
    "from utils.soap import PowerSpectrum\n",
    "from utils.alchemical import AlchemicalCombine\n",
    "from utils.linear import LinearModel\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 300 training frames\n"
     ]
    }
   ],
   "source": [
    "n_test = 100\n",
    "n_train = 300\n",
    "\n",
    "frames = ase.io.read(\"data/elpasolites_10590.xyz\", f\":\")\n",
    "energies = torch.tensor(np.loadtxt(\"data/elpasolites_10590_evpa.dat\"))\n",
    "\n",
    "# frames = ase.io.read(\"../equistore-examples/data/molecule_conformers_dftb.xyz\", \":\")\n",
    "# energies = torch.tensor([frame.info[\"energy\"] for frame in frames])\n",
    "\n",
    "train_frames = frames[:n_train]\n",
    "test_frames = frames[-n_test:]\n",
    "\n",
    "train_energies = energies[:n_train]\n",
    "test_energies = energies[-n_test:]\n",
    "\n",
    "print(f\"using {n_train} training frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_species = set()\n",
    "for frame in frames:\n",
    "    global_species.update(frame.numbers)\n",
    "\n",
    "global_species = list(map(lambda u: int(u), global_species))\n",
    "\n",
    "# HYPERS_FROM_PAPER = {\n",
    "#     \"interaction_cutoff\": 5.0,\n",
    "#     \"max_angular\": 9,\n",
    "#     \"max_radial\": 12,\n",
    "#     \"gaussian_sigma_constant\": 0.3,\n",
    "#     \"gaussian_sigma_type\": \"Constant\",\n",
    "#     \"cutoff_smooth_width\": 0.5,\n",
    "#     \"radial_basis\": \"GTO\",\n",
    "#     \"compute_gradients\": False,\n",
    "#     \"expansion_by_species_method\": \"user defined\",\n",
    "#     \"global_species\": global_species,\n",
    "# }\n",
    "\n",
    "HYPERS_SMALL = {\n",
    "    \"cutoff\": 5.0,\n",
    "    \"max_angular\": 3,\n",
    "    \"max_radial\": 4,\n",
    "    \"atomic_gaussian_width\": 0.3,\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"gradients\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "train_dataset = AtomisticDataset(train_frames, HYPERS_SMALL, train_energies)\n",
    "test_dataset = AtomisticDataset(test_frames, HYPERS_SMALL, test_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "train_dataloader_no_batch = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=len(train_dataset),\n",
    "    shuffle=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "test_dataloader = create_dataloader(\n",
    "    test_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_optimizer(predicted, actual, regularizer, weights):\n",
    "    loss = torch.linalg.norm(predicted.flatten() - actual.flatten()) ** 2\n",
    "    # regularize the loss, full dataset std\n",
    "    loss += regularizer / torch.std(train_energies.flatten()) * torch.linalg.norm(weights) ** 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_mae(predicted, actual):\n",
    "    return torch.mean(torch.abs(predicted.flatten() - actual.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedSpeciesLinearModel(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "        species, \n",
    "        n_pseudo_species, \n",
    "        regularizer,\n",
    "        optimizable_weights,\n",
    "        random_initial_weights,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.alchemical = AlchemicalCombine(species, n_pseudo_species)\n",
    "        self.power_spectrum = PowerSpectrum()\n",
    "        self.model = LinearModel(\n",
    "            normalize=True, \n",
    "            regularizer=regularizer,\n",
    "            optimizable_weights=optimizable_weights,\n",
    "            random_initial_weights=random_initial_weights,\n",
    "        )\n",
    "\n",
    "        self.optimizable_weights = optimizable_weights\n",
    "        self.random_initial_weights = random_initial_weights\n",
    "\n",
    "    def forward(self, spherical_expansion):\n",
    "        combined = self.alchemical(spherical_expansion)\n",
    "        power_spectrum = self.power_spectrum(combined)\n",
    "        energies, _ = self.model(power_spectrum, with_forces=False)\n",
    "        return energies\n",
    "\n",
    "    def initialize_parameters(self, spherical_expansion, energies):\n",
    "        combined = self.alchemical(spherical_expansion)\n",
    "        power_spectrum = self.power_spectrum(combined)\n",
    "        self.model.initialize_parameters(power_spectrum, energies)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PSEUDO_SPECIES = 4\n",
    "REGULARIZER = 1e-2\n",
    "\n",
    "mixed_species_model = MixedSpeciesLinearModel(\n",
    "    global_species, \n",
    "    n_pseudo_species=N_PSEUDO_SPECIES, \n",
    "    regularizer=[REGULARIZER],\n",
    "    optimizable_weights=True,\n",
    "    random_initial_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_species_model.to(device=device)\n",
    "\n",
    "# initialize the model\n",
    "with torch.no_grad():\n",
    "    for spherical_expansions, energies in train_dataloader_no_batch:\n",
    "        # we want to intially train the model on all frames, to ensure the\n",
    "        # support points come from the full dataset.\n",
    "        mixed_species_model.initialize_parameters(spherical_expansions, energies)\n",
    "\n",
    "if mixed_species_model.optimizable_weights:\n",
    "    torch_loss_regularizer = REGULARIZER\n",
    "else:\n",
    "    torch_loss_regularizer = 0\n",
    "    # we can not use batches if we are training with linear algebra, we need to\n",
    "    # have all training frames available\n",
    "    assert train_dataloader.batch_size >= len(train_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     mixed_species_model.parameters(), \n",
    "#     lr=lr, weight_decay=0.0\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.LBFGS(\n",
    "    mixed_species_model.parameters(), \n",
    "    lr=lr,\n",
    ")\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "filename = f\"{mixed_species_model.__class__.__name__}-{N_PSEUDO_SPECIES}-mixed-{n_train}-train\"\n",
    "if mixed_species_model.optimizable_weights:\n",
    "    filename += \"-opt-weights\"\n",
    "\n",
    "if mixed_species_model.random_initial_weights:\n",
    "    filename += \"-random-weights\"\n",
    "\n",
    "output = open(f\"{filename}.dat\", \"w\")\n",
    "output.write(\"# epoch  train_loss  test_mae\\n\")\n",
    "n_epochs_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 took 15.44s, optimizer loss=713.1, test mae=0.7252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=33'>34</a>\u001b[0m         loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=35'>36</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=37'>38</a>\u001b[0m     loss \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mstep(single_step)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=38'>39</a>\u001b[0m     all_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=40'>41</a>\u001b[0m epoch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m epoch_start\n",
      "File \u001b[0;32m~/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py:437\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py?line=431'>432</a>\u001b[0m \u001b[39mif\u001b[39;00m n_iter \u001b[39m!=\u001b[39m max_iter:\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py?line=432'>433</a>\u001b[0m     \u001b[39m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py?line=433'>434</a>\u001b[0m     \u001b[39m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py?line=434'>435</a>\u001b[0m     \u001b[39m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py?line=435'>436</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py?line=436'>437</a>\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(closure())\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py?line=437'>438</a>\u001b[0m     flat_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/optim/lbfgs.py?line=438'>439</a>\u001b[0m     opt_cond \u001b[39m=\u001b[39m flat_grad\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb Cell 18'\u001b[0m in \u001b[0;36msingle_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mixed_species_model\u001b[39m.\u001b[39moptimizable_weights:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=23'>24</a>\u001b[0m     mixed_species_model\u001b[39m.\u001b[39minitialize_parameters(spherical_expansions, energies)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=25'>26</a>\u001b[0m predicted \u001b[39m=\u001b[39m mixed_species_model(spherical_expansions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_optimizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=28'>29</a>\u001b[0m     predicted, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=29'>30</a>\u001b[0m     energies, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=30'>31</a>\u001b[0m     torch_loss_regularizer, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=31'>32</a>\u001b[0m     mixed_species_model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mweights\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=32'>33</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000014?line=33'>34</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb Cell 10'\u001b[0m in \u001b[0;36mMixedSpeciesLinearModel.forward\u001b[0;34m(self, spherical_expansion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000009?line=23'>24</a>\u001b[0m combined \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malchemical(spherical_expansion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000009?line=24'>25</a>\u001b[0m power_spectrum \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower_spectrum(combined)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000009?line=25'>26</a>\u001b[0m energies, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(power_spectrum, with_forces\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/code/alchemical-learning/alchemical-learning-elpasolites.ipynb#ch0000009?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m energies\n",
      "File \u001b[0;32m~/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/code/alchemical-learning/virtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/alchemical-learning/utils/linear.py:101\u001b[0m, in \u001b[0;36mLinearModel.forward\u001b[0;34m(self, power_spectrum, with_forces)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/linear.py?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/linear.py?line=98'>99</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcall initialize_weights first\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/guillaume/code/alchemical-learning/utils/linear.py?line=100'>101</a>\u001b[0m ps_per_structure \u001b[39m=\u001b[39m structure_sum(power_spectrum)\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/utils/linear.py?line=102'>103</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize:\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/utils/linear.py?line=103'>104</a>\u001b[0m     ps_per_structure \u001b[39m=\u001b[39m normalize(ps_per_structure)\n",
      "File \u001b[0;32m~/code/alchemical-learning/utils/operations.py:97\u001b[0m, in \u001b[0;36mstructure_sum\u001b[0;34m(descriptor, sum_properties)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=92'>93</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, block \u001b[39min\u001b[39;00m descriptor:\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=93'>94</a>\u001b[0m     \u001b[39m# no lambda kernels for now\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=94'>95</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(block\u001b[39m.\u001b[39mcomponents) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=96'>97</a>\u001b[0m     summed_values, structures \u001b[39m=\u001b[39m SumStructures\u001b[39m.\u001b[39;49mapply(block\u001b[39m.\u001b[39;49mvalues, block\u001b[39m.\u001b[39;49msamples)\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=98'>99</a>\u001b[0m     new_block \u001b[39m=\u001b[39m TensorBlock(\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=99'>100</a>\u001b[0m         values\u001b[39m=\u001b[39msummed_values,\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=100'>101</a>\u001b[0m         samples\u001b[39m=\u001b[39mLabels([\u001b[39m\"\u001b[39m\u001b[39mstructure\u001b[39m\u001b[39m\"\u001b[39m], structures\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)),\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=101'>102</a>\u001b[0m         components\u001b[39m=\u001b[39mblock\u001b[39m.\u001b[39mcomponents,\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=102'>103</a>\u001b[0m         properties\u001b[39m=\u001b[39mblock\u001b[39m.\u001b[39mproperties,\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=103'>104</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=105'>106</a>\u001b[0m     blocks\u001b[39m.\u001b[39mappend(new_block)\n",
      "File \u001b[0;32m~/code/alchemical-learning/utils/operations.py:68\u001b[0m, in \u001b[0;36mSumStructures.forward\u001b[0;34m(ctx, values, samples)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=65'>66</a>\u001b[0m     mask \u001b[39m=\u001b[39m samples[\u001b[39m\"\u001b[39m\u001b[39mstructure\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m structure\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=66'>67</a>\u001b[0m     structures_masks\u001b[39m.\u001b[39mappend(mask)\n\u001b[0;32m---> <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=67'>68</a>\u001b[0m     output[structure_i, :] \u001b[39m=\u001b[39m values[mask, :]\u001b[39m.\u001b[39;49msum(dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=69'>70</a>\u001b[0m ctx\u001b[39m.\u001b[39mstructures_masks \u001b[39m=\u001b[39m structures_masks\n\u001b[1;32m     <a href='file:///Users/guillaume/code/alchemical-learning/utils/operations.py?line=70'>71</a>\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # if UPDATE_SUPPORT_POINTS:\n",
    "    #     # to update the support points, we need to have all training data at\n",
    "    #     # once in memory\n",
    "    #     for spherical_expansions, species, slices, _ in train_dataloader_no_batch:\n",
    "    #         assert len(slices) == len(train_frames)\n",
    "    #         # use `select_again=True` to re-select the same number of support\n",
    "    #         # points. this might make convergence slower, but maybe able to\n",
    "    #         # reach a lower final loss?\n",
    "    #         #\n",
    "    #         # with `select_again=False`, the environments selected in the first\n",
    "    #         # fit above are used as support points\n",
    "    #         mixed_species_model.update_support_points(\n",
    "    #             spherical_expansions, species, slices, select_again=False\n",
    "    #         )\n",
    "\n",
    "    for spherical_expansions, energies in train_dataloader:\n",
    "        def single_step():\n",
    "            optimizer.zero_grad()\n",
    "           \n",
    "            if not mixed_species_model.optimizable_weights:\n",
    "                mixed_species_model.initialize_parameters(spherical_expansions, energies)\n",
    "                \n",
    "            predicted = mixed_species_model(spherical_expansions)\n",
    "\n",
    "            loss = loss_optimizer(\n",
    "                predicted, \n",
    "                energies, \n",
    "                torch_loss_regularizer, \n",
    "                mixed_species_model.model.weights\n",
    "            )\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            return loss\n",
    "            \n",
    "        loss = optimizer.step(single_step)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            predicted = []\n",
    "            reference = []\n",
    "            for spherical_expansions, energies in test_dataloader:\n",
    "                reference.append(energies)\n",
    "                predicted.append(mixed_species_model(spherical_expansions))\n",
    "\n",
    "            reference = torch.vstack(reference)\n",
    "            predicted = torch.vstack(predicted)\n",
    "            mae = loss_mae(predicted.cpu(), reference)\n",
    "\n",
    "            output.write(f\"{n_epochs_total} {loss} {mae}\\n\")\n",
    "            output.flush()\n",
    "\n",
    "        print(f\"epoch {n_epochs_total} took {epoch_time:.4}s, optimizer loss={loss.item():.4}, test mae={mae:.4}\")\n",
    "        \n",
    "    n_epochs_total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5ef12eb5922cf8d604aeb1dfa412e3da3665f54b5e701463f21d300328cee18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
