{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from copy import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EquistoreDummy:\n",
    "    z: torch.Tensor\n",
    "    val: torch.Tensor\n",
    "    idx: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Embedding_one_hot(embedding):\n",
    "    pass\n",
    "\n",
    "def EmbeddingFactory(elements:List[int],one_hot:bool) -> torch.nn.Embedding:\n",
    "    \"\"\"Returns an Embedding of dim max_Z,n_unique_elements\n",
    "    max_Z = 9, n_unique = 2, elements = [1,8]\n",
    "    Embedding(tensor([8])) -> tensor([0.0,1.0]) (if one hot)\n",
    "    \"\"\"\n",
    "    \n",
    "    max_int = max(elements) + 1\n",
    "    n_species = len(elements)\n",
    "\n",
    "    embedding = torch.nn.Embedding(max_int,n_species)\n",
    "    \n",
    "    if one_hot:\n",
    "        weights = torch.zeros(max_int, n_species)\n",
    "        \n",
    "        for idx, Z in enumerate(elements):\n",
    "            weights[Z, idx] = 1.0\n",
    "\n",
    "        embedding.weight.data = weights\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    \"\"\" A simple MLP \n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: add n_hidden_layers, activation function option\n",
    "    def __init__(self, dim_input: int, dim_output: int, layer_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.dim_input, self.layer_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.layer_size, self.layer_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.layer_size, self.dim_output),\n",
    "        )\n",
    "    def forward(self,x: torch.tensor) -> torch.tensor:\n",
    "        return self.nn(x)\n",
    "\n",
    "class MultiMLP(torch.nn.Module):    \n",
    "    \"\"\" A Multi MLP that contains N_species * SimpleMLPs\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_input: int, dim_output: int, layer_size: int, species: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_output = dim_output\n",
    "        self.dim_input = dim_input\n",
    "        self.layer_size = layer_size\n",
    "        self.species = species \n",
    "        self.n_species = len(self.species)\n",
    "        self.species_nn = torch.nn.ModuleList([ SimpleMLP(dim_input,dim_output,layer_size) for _ in self.species])\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return torch.cat([nn(x) for nn in self.species_nn],dim=1)\n",
    "\n",
    "\n",
    "class MultiMLP_skip(MultiMLP):\n",
    "    \"\"\" A Multi MLP that contains N_species * SimpleMLPs\n",
    "        This Implementation does only batchwise evaluation of neural networks?\n",
    "        As this implementation skips \n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: torch.tensor, batch_z: torch.tensor) -> torch.tensor:\n",
    "        #will this work with autograd?\n",
    "        \n",
    "        unique_z_in_batch = torch.unique(batch_z)\n",
    "        \n",
    "        model_out = torch.empty((x.shape[0],self.n_species))\n",
    "\n",
    "        for n, (z, nn) in enumerate(zip(self.species, self.species_nn)):\n",
    "            if z in unique_z_in_batch:\n",
    "                model_out[batch_z == z, n] = nn(x[batch_z == z]).flatten()\n",
    "                model_out[batch_z != z, n] = torch.zeros(x[batch_z != z].shape[0])\n",
    "            else:\n",
    "                model_out[:, n] = torch.zeros(x.shape[0])\n",
    "\n",
    "        return model_out\n",
    "\n",
    "\n",
    "class MultiSpeciesMLP(torch.nn.Module):\n",
    "    \n",
    "    \"\"\" Implements a MultiSpecies Behler Parinello neural network\n",
    "    This implementation scales O(Nspecies*Natoms), but it has a learnable weight matrix, that combines species wise energies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, species, n_in, n_out, n_hidden, one_hot, embedding_trainable) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #just a precaution\n",
    "        species = copy(species)\n",
    "        species.sort()\n",
    "\n",
    "        #print(species)\n",
    "\n",
    "        self.species = species\n",
    "        self.nn = MultiMLP(n_in,n_out,n_hidden,species)\n",
    "        self.embedding = EmbeddingFactory(species, one_hot)\n",
    "\n",
    "        if not embedding_trainable:\n",
    "            self.embedding.requires_grad_ = False\n",
    "            \n",
    "\n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "        \n",
    "        x = descriptor.val \n",
    "        z = descriptor.z # something like descriptor.\n",
    "\n",
    "        #The embedding serves a a multiplicative \"mask\" -> not so nice overall complexity scales as O(N_species*N_samples)\n",
    "        # whereas an implementation that could \"skip\" NN evaluations should only scale as O(N_samples)\n",
    "        return torch.sum(self.nn(x) * self.embedding(z),dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiSpeciesMLP_skip(torch.nn.Module):\n",
    "    \n",
    "    \"\"\" Implements a MultiSpecies Behler Parinello neural network\n",
    "    This implementation should scale O(Natoms) as it skips the neural network evaluations that would be otherwise only multiplied with zeros\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, species, n_in, n_out, n_hidden) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        #just a precaution\n",
    "        species = copy(species)\n",
    "        species.sort()\n",
    "\n",
    "        #print(species)\n",
    "        \n",
    "        #TODO: Implement this properly in the MultiSpeciesMLP class\n",
    "        self.n_out = n_out\n",
    "\n",
    "        self.species = species\n",
    "        self.nn = MultiMLP_skip(n_in,n_out,n_hidden,species)\n",
    "        self.embedding = EmbeddingFactory(species, True)\n",
    "        self.embedding.requires_grad_ = False\n",
    "            \n",
    "\n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "        \n",
    "        x = descriptor.val  # something like descriptor.\n",
    "        z = descriptor.z \n",
    "\n",
    "        # here the embedding multiplication should only introduce a minor overhead\n",
    "        return torch.sum(self.nn(x,z) * self.embedding(z),dim=1)\n",
    "\n",
    "class MultiSpeciesMLP_skip_w_index_add(MultiSpeciesMLP_skip):\n",
    "    \n",
    "    \"\"\" For testing purposes I have added the atomic-contributions to structure wise properties addition\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "                \n",
    "        x = descriptor.val  \n",
    "        z = descriptor.z \n",
    "        idx = descriptor.idx\n",
    "        num_structures = len(torch.unique(idx))\n",
    "\n",
    "        out_batch = torch.zeros((num_structures,self.n_out))\n",
    "        atomic_contributions = torch.sum(self.nn(x,z) * self.embedding(z),dim=1)\n",
    "\n",
    "        return out_batch.index_add_(0,idx,atomic_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_block = EquistoreDummy(torch.tensor([1,1,8,1,7]),torch.ones(5,3),torch.tensor([0,0,0,1,1]))\n",
    "multimlp = MultiSpeciesMLP_skip([1,8,7],3,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_out = multimlp.forward(a_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0575, 0.0575, 0.3746, 0.0575, 0.1834], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimlp_index_add = MultiSpeciesMLP_skip_w_index_add([1,8,7],3,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0895,  0.0895,  0.0044,  0.0895, -0.1999], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1834],\n",
       "        [-0.1104]], grad_fn=<IndexAddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimlp_index_add.forward(a_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2781"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([-0.0236, -0.0236, -0.2309])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f481d707f485e5e545fcbec8e23552587d82164162b1e7024dc6dabd3ca7d616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
