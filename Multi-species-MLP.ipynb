{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This implementation is an adaptation of the schnetpack implementation of atom wise-BPNNs \n",
    "#  https://schnetpack.readthedocs.io/en/stable/_modules/schnetpack/nn/blocks.html#GatedNetwork\n",
    "\n",
    "# It uses an embedding matrix that calculates Nspecies*Nsamples NN atom-wise contributions\n",
    "# If the embedding is one-hot for each species it multiplies the \"off diagonal\" NN contributions with zero\n",
    "# ie NN_hydrogen(X_central_species_is_Oxygen) * 0 + NN_oxygen(X_central_species_is_Oxygen) * 1\n",
    "# but this embedding can also be learned assigning:\n",
    "# w_oxygen, w_hydrogen for every species that are not one-hot\n",
    "# Unfortunately this embedding changes the cost of the BPNN to O(N_species*N_atoms)\n",
    "# I have added a second implementation where, if the one_hot_encoding is chosen, \n",
    "# the other network evaluations are supposed to be skipped (multiplied with zeros anyway...)\n",
    "# reducing the overall complexity to ~O(N_atoms) assuming that N_species << N_atoms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from copy import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EquistoreDummy:\n",
    "    z: torch.Tensor\n",
    "    val: torch.Tensor\n",
    "    idx: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Embedding_one_hot(embedding):\n",
    "    pass\n",
    "\n",
    "def EmbeddingFactory(elements:List[int],one_hot:bool) -> torch.nn.Embedding:\n",
    "    \"\"\"Returns an Embedding of dim max_Z,n_unique_elements\n",
    "    max_Z = 9, n_unique = 2, elements = [1,8]\n",
    "    Embedding(tensor([8])) -> tensor([0.0,1.0]) (if one hot)\n",
    "    \"\"\"\n",
    "    \n",
    "    # embedding \"technically\" starts at zero, Z at one\n",
    "    max_int = max(elements) + 1\n",
    "    n_species = len(elements)\n",
    "\n",
    "    #randomly initialize the Embedding\n",
    "    #TODO: add a initialize_weights routine\n",
    "    #TODO: maybe solve it with a decorator?\n",
    "    embedding = torch.nn.Embedding(max_int,n_species)\n",
    "    \n",
    "    # If the embedding is one-hot, the weight matrix is diagonal\n",
    "    if one_hot:\n",
    "        weights = torch.zeros(max_int, n_species)\n",
    "        \n",
    "        for idx, Z in enumerate(elements):\n",
    "            weights[Z, idx] = 1.0\n",
    "\n",
    "        embedding.weight.data = weights\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    \"\"\" A simple MLP \n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: add n_hidden_layers, activation function option\n",
    "    def __init__(self, dim_input: int, dim_output: int, layer_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.dim_input, self.layer_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.layer_size, self.layer_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.layer_size, self.dim_output),\n",
    "        )\n",
    "    def forward(self,x: torch.tensor) -> torch.tensor:\n",
    "        return self.nn(x)\n",
    "\n",
    "class MultiMLP(torch.nn.Module):    \n",
    "    \"\"\" A Multi MLP that contains N_species * SimpleMLPs\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_input: int, dim_output: int, layer_size: int, species: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_output = dim_output\n",
    "        self.dim_input = dim_input\n",
    "        self.layer_size = layer_size\n",
    "        self.species = species \n",
    "        self.n_species = len(self.species)\n",
    "\n",
    "        # initialize as many SimpleMLPs as atomic species\n",
    "        self.species_nn = torch.nn.ModuleList([ SimpleMLP(dim_input,dim_output,layer_size) for _ in self.species])\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return torch.cat([nn(x) for nn in self.species_nn],dim=1)\n",
    "\n",
    "\n",
    "class MultiMLP_skip(MultiMLP):\n",
    "    \"\"\" A Multi MLP that contains N_species * SimpleMLPs\n",
    "        This Implementation does only batchwise evaluation of neural networks?\n",
    "        As this implementation skips \n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: torch.tensor, batch_z: torch.tensor) -> torch.tensor:\n",
    "        #will this work with autograd? -> I think it does\n",
    "        \n",
    "        #get the unique zs in batch\n",
    "        unique_z_in_batch = torch.unique(batch_z)\n",
    "\n",
    "        #initializes an empty torch tensor of shape (N_samples,N_species)\n",
    "        model_out = torch.empty((x.shape[0],self.n_species))\n",
    "\n",
    "        #loops over n_total_species\n",
    "        for n, (z, nn) in enumerate(zip(self.species, self.species_nn)):\n",
    "            \n",
    "            # if a z is in a global batch -> then use the NN_central_species on the X_central species\n",
    "            # fill the rest with zeros\n",
    "            if z in unique_z_in_batch:\n",
    "                model_out[batch_z == z, n] = nn(x[batch_z == z]).flatten()\n",
    "                model_out[batch_z != z, n] = torch.zeros(x[batch_z != z].shape[0])\n",
    "            \n",
    "            #else: if z is not in batch at all, simply fill everything with zeros\n",
    "            else:\n",
    "                model_out[:, n] = torch.zeros(x.shape[0])\n",
    "\n",
    "        return model_out\n",
    "\n",
    "\n",
    "class MultiSpeciesMLP(torch.nn.Module):\n",
    "    \n",
    "    \"\"\" Implements a MultiSpecies Behler Parinello neural network\n",
    "    This implementation scales O(Nspecies*Natoms), but it has a learnable weight matrix, that combines species wise energies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, species, n_in, n_out, n_hidden, one_hot, embedding_trainable) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #just a precaution\n",
    "        species = copy(species)\n",
    "        species.sort()\n",
    "\n",
    "        #print(species)\n",
    "\n",
    "        self.species = species\n",
    "        self.nn = MultiMLP(n_in,n_out,n_hidden,species)\n",
    "        self.embedding = EmbeddingFactory(species, one_hot)\n",
    "\n",
    "        if not embedding_trainable:\n",
    "            self.embedding.requires_grad_ = False\n",
    "            \n",
    "\n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "        \n",
    "        x = descriptor.val \n",
    "        z = descriptor.z # something like descriptor.\n",
    "\n",
    "        #The embedding serves a a multiplicative \"mask\" -> not so nice overall complexity scales as O(N_species*N_samples)\n",
    "        # whereas an implementation that could \"skip\" NN evaluations should only scale as O(N_samples)\n",
    "        return torch.sum(self.nn(x) * self.embedding(z),dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiSpeciesMLP_skip(torch.nn.Module):\n",
    "    \n",
    "    \"\"\" Implements a MultiSpecies Behler Parinello neural network\n",
    "    This implementation should scale O(Natoms) as it skips the neural network evaluations that would be otherwise only multiplied with zeros\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, species, n_in, n_out, n_hidden) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        #just a precaution\n",
    "        species = copy(species)\n",
    "        species.sort()\n",
    "\n",
    "        #print(species)\n",
    "    \n",
    "        #TODO: Implement this properly in the MultiSpeciesMLP class\n",
    "\n",
    "        self.n_out = n_out\n",
    "        self.species = species\n",
    "\n",
    "        # if we want to skip the NN evaluations the Embedding has to be non trainable\n",
    "        # therefore -> MultiMLP_skip has no trainable kwargs and one_hot in EmbeddingFactory is always true\n",
    "        # TODO: Implement this properly in the MultiSpeciesMLP class\n",
    "\n",
    "        self.nn = MultiMLP_skip(n_in,n_out,n_hidden,species)\n",
    "        self.embedding = EmbeddingFactory(species, True)\n",
    "        self.embedding.requires_grad_ = False\n",
    "\n",
    "    def forward(self, x: torch.tensor, z:torch.tensor) -> torch.tensor:\n",
    "        # here the embedding multiplication should only introduce a minor overhead \n",
    "        return torch.sum(self.nn(x,z) * self.embedding(z),dim=1,keepdim=True)   \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "        \n",
    "        x = descriptor.val  # something like descriptor.\n",
    "        z = descriptor.z \n",
    "\n",
    "        # here the embedding multiplication should only introduce a minor overhead \n",
    "        return torch.sum(self.nn(x,z) * self.embedding(z),dim=1)\n",
    "    \"\"\"\n",
    "\n",
    "class MultiSpeciesMLP_skip_w_index_add(MultiSpeciesMLP_skip):\n",
    "    \n",
    "    \"\"\" For testing purposes I have added the atomic-contributions to structure wise properties addition\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "                \n",
    "        x = descriptor.val  \n",
    "        z = descriptor.z \n",
    "        idx = descriptor.idx\n",
    "\n",
    "        x.requires_grad_(True)\n",
    "        num_structures = len(torch.unique(idx))\n",
    "        \n",
    "        structure_wise_properties = torch.zeros((num_structures,self.n_out))\n",
    "\n",
    "        # In the summation of the atomic contirbutions the dimensions should be kept for autograds\n",
    "        atomic_contributions = torch.sum(self.nn(x,z) * self.embedding(z),dim=1,keepdim=True)\n",
    "\n",
    "        print(atomic_contributions.flatten())\n",
    "        \n",
    "        structure_wise_properties.index_add_(0,idx,atomic_contributions)\n",
    "\n",
    "        nn_grads = torch.autograd.grad(\n",
    "                structure_wise_properties,\n",
    "                x,\n",
    "                grad_outputs=torch.ones_like(structure_wise_properties),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )\n",
    "\n",
    "        return structure_wise_properties, nn_grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_block = EquistoreDummy(torch.tensor([1,1,8,1,7]),torch.ones(5,4),torch.tensor([0,0,0,1,1]))\n",
    "#multimlp = MultiSpeciesMLP_skip([1,8,7],3,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimlp_index_add = MultiSpeciesMLP_skip_w_index_add([1,8,7],4,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1124, 0.1124, 0.0163, 0.1124, 0.0560],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2410],\n",
       "         [0.1684]], grad_fn=<IndexAddBackward0>),\n",
       " (tensor([[ 0.0689, -0.0983,  0.0124, -0.0464],\n",
       "          [ 0.0689, -0.0983,  0.0124, -0.0464],\n",
       "          [-0.0497, -0.0061,  0.0588,  0.1362],\n",
       "          [ 0.0689, -0.0983,  0.0124, -0.0464],\n",
       "          [-0.0140,  0.0829,  0.0698, -0.1322]], grad_fn=<AddBackward0>),))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimlp_index_add.forward(a_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2506"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([-0.0752, -0.0752, -0.1002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09860000000000001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([-0.0752,  0.1738])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combine it with the existing NN/BPNN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils.operations import StructureMap\n",
    "\n",
    "\n",
    "class NNModel(torch.nn.Module):\n",
    "    def __init__(self, layer_size=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nn = None\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "    # build a combined \n",
    "    def initialize_model_weights(self, descriptor, energies, forces=None, seed=None):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        X = descriptor.block().values\n",
    "\n",
    "        # initialize nn with zero weights ??\n",
    "        def init_zero_weights(m):\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                m.weight.data.fill_(0)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        # \n",
    "\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(X.shape[-1], self.layer_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.layer_size, self.layer_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.layer_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, descriptor, with_forces=False):\n",
    "        if self.nn is None:\n",
    "            raise Exception(\"call initialize_weights first\")\n",
    "\n",
    "        ps_block = descriptor.block()\n",
    "        ps_tensor = ps_block.values\n",
    "\n",
    "        if with_forces:\n",
    "            # TODO(guillaume): can this have unintended side effects???\n",
    "            ps_tensor.requires_grad_(True)\n",
    "\n",
    "        structure_map, new_samples, _ = StructureMap(\n",
    "            ps_block.samples[\"structure\"], ps_tensor.device\n",
    "        )\n",
    "\n",
    "        nn_per_atom = self.nn(ps_tensor)\n",
    "        \n",
    "        #structure is actually atomic envs\n",
    "        nn_per_structure = torch.zeros((len(new_samples), 1), device=ps_tensor.device)\n",
    "        \n",
    "        #adds atomic contributions per structure\n",
    "        nn_per_structure.index_add_(0, structure_map, nn_per_atom)\n",
    "\n",
    "        energies = nn_per_structure\n",
    "        if with_forces:\n",
    "            \n",
    "            # computes dnn/dg for dnn/dg dg/dx\n",
    "            nn_grads = torch.autograd.grad(\n",
    "                nn_per_structure,\n",
    "                ps_tensor,\n",
    "                grad_outputs=torch.ones_like(nn_per_structure),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )\n",
    "\n",
    "\n",
    "            ps_gradient = descriptor.block().gradient(\"positions\")\n",
    "            ps_tensor_grad = ps_gradient.data.reshape(-1, 3, ps_tensor.shape[-1])\n",
    "\n",
    "            gradient_samples_Aj = np.asarray(\n",
    "                ps_gradient.samples[[\"structure\", \"atom\"]], dtype=tuple\n",
    "            )\n",
    "\n",
    "            #why is a unique gradient necessary\n",
    "            unique_gradient, unique_gradient_idx = np.unique(\n",
    "                gradient_samples_Aj, return_index=True\n",
    "            )\n",
    "            # new_gradient_samples = gradient_samples_Aj[np.sort(unique_gradient_idx)]\n",
    "\n",
    "            # the logic is analogous to that for the structures: we have to map\n",
    "            # positions in the full (A,i,j) vector to the position where they\n",
    "            # will have to be accumulated\n",
    "            gradient_replace_rule = dict(\n",
    "                zip(unique_gradient, range(len(unique_gradient)))\n",
    "            )\n",
    "\n",
    "\n",
    "            gradient_map = torch.tensor(\n",
    "                [gradient_replace_rule[i] for i in gradient_samples_Aj],\n",
    "                dtype=torch.long,\n",
    "                device=ps_tensor.device,\n",
    "            )\n",
    "\n",
    "            new_gradient_data = torch.zeros(\n",
    "                (len(unique_gradient), 3, 1),\n",
    "                device=ps_tensor.device,\n",
    "            )\n",
    "            # ... and then contracting the gradients is just one call\n",
    "            nn_per_atom_forces = -torch.sum(\n",
    "                ps_tensor_grad * nn_grads[0][gradient_map][:, None, :], -1\n",
    "            )\n",
    "\n",
    "            #why the index add here ?\n",
    "            new_gradient_data.index_add_(\n",
    "                0, gradient_map, nn_per_atom_forces[:, :, None]\n",
    "            )\n",
    "            forces = new_gradient_data.reshape(-1, 3)\n",
    "        else:\n",
    "            forces = None\n",
    "        return energies, forces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeciesWiseBPNN(torch.nn.Module):\n",
    "    def __init__(self, layer_size=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nn = None\n",
    "        self.layer_size = layer_size\n",
    "        # for now we only want to predict energies\n",
    "        self.n_out = 1\n",
    "\n",
    "    # build a combined \n",
    "    def initialize_model_weights(self, descriptor, energies, forces=None, seed=None):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        X = descriptor.block().values\n",
    "        z = torch.tensor(descriptor.block().samples[\"species_center\"])\n",
    "\n",
    "        species_unique = torch.unique(z).tolist()\n",
    "\n",
    "\n",
    "        # initialize nn with zero weights ??\n",
    "        def init_zero_weights(m):\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                m.weight.data.fill_(0)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        # \n",
    "        n_feat_descriptor = X.shape[-1]\n",
    "\n",
    "\n",
    "        #MultiSpeciesMLP_skip: feat --> species wise NN, skipping evals --> atomic contributions out\n",
    "        self.nn = MultiSpeciesMLP_skip(species_unique,n_feat_descriptor,self.n_out,self.layer_size)\n",
    "\n",
    "    def forward(self, descriptor, with_forces=False):\n",
    "        if self.nn is None:\n",
    "            raise Exception(\"call initialize_weights first\")\n",
    "\n",
    "        ps_block = descriptor.block()\n",
    "        ps_tensor = ps_block.values #is this a torch tensor? check\n",
    "\n",
    "        # obtaining central species of batch\n",
    "        ps_z = torch.tensor(ps_block.samples[\"species_center\"])\n",
    "\n",
    "        if with_forces:\n",
    "            # TODO(guillaume): can this have unintended side effects???\n",
    "            ps_tensor.requires_grad_(True)\n",
    "\n",
    "        structure_map, new_samples, _ = StructureMap(\n",
    "            ps_block.samples[\"structure\"], ps_tensor.device\n",
    "        )\n",
    "\n",
    "        nn_per_atom = self.nn(ps_tensor, ps_z)\n",
    "        \n",
    "        #structure is actually atomic envs\n",
    "        nn_per_structure = torch.zeros((len(new_samples), 1), device=ps_tensor.device)\n",
    "        \n",
    "        #adds atomic contributions per structure\n",
    "        nn_per_structure.index_add_(0, structure_map, nn_per_atom)\n",
    "\n",
    "        energies = nn_per_structure\n",
    "        if with_forces:\n",
    "            \n",
    "            # computes dnn/dg for dnn/dg dg/dx\n",
    "            nn_grads = torch.autograd.grad(\n",
    "                nn_per_structure,\n",
    "                ps_tensor,\n",
    "                grad_outputs=torch.ones_like(nn_per_structure),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )\n",
    "\n",
    "\n",
    "            ps_gradient = descriptor.block().gradient(\"positions\")\n",
    "            ps_tensor_grad = ps_gradient.data.reshape(-1, 3, ps_tensor.shape[-1])\n",
    "\n",
    "            gradient_samples_Aj = np.asarray(\n",
    "                ps_gradient.samples[[\"structure\", \"atom\"]], dtype=tuple\n",
    "            )\n",
    "\n",
    "            #why is a unique gradient necessary\n",
    "            unique_gradient, unique_gradient_idx = np.unique(\n",
    "                gradient_samples_Aj, return_index=True\n",
    "            )\n",
    "            # new_gradient_samples = gradient_samples_Aj[np.sort(unique_gradient_idx)]\n",
    "\n",
    "            # the logic is analogous to that for the structures: we have to map\n",
    "            # positions in the full (A,i,j) vector to the position where they\n",
    "            # will have to be accumulated\n",
    "            gradient_replace_rule = dict(\n",
    "                zip(unique_gradient, range(len(unique_gradient)))\n",
    "            )\n",
    "\n",
    "\n",
    "            gradient_map = torch.tensor(\n",
    "                [gradient_replace_rule[i] for i in gradient_samples_Aj],\n",
    "                dtype=torch.long,\n",
    "                device=ps_tensor.device,\n",
    "            )\n",
    "\n",
    "            new_gradient_data = torch.zeros(\n",
    "                (len(unique_gradient), 3, 1),\n",
    "                device=ps_tensor.device,\n",
    "            )\n",
    "            # ... and then contracting the gradients is just one call\n",
    "            nn_per_atom_forces = -torch.sum(\n",
    "                ps_tensor_grad * nn_grads[0][gradient_map][:, None, :], -1\n",
    "            )\n",
    "\n",
    "            #why the index add here ?\n",
    "            new_gradient_data.index_add_(\n",
    "                0, gradient_map, nn_per_atom_forces[:, :, None]\n",
    "            )\n",
    "            forces = new_gradient_data.reshape(-1, 3)\n",
    "        else:\n",
    "            forces = None\n",
    "        return energies, forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the z-wise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing for key2prop 0.008439064025878906\n",
      "Timing for key2prop 0.008617877960205078\n",
      "Timing for key2prop 0.01029205322265625\n",
      "Timing for key2prop 0.007021188735961914\n",
      "Timing for key2prop 0.008301019668579102\n",
      "Timing for key2prop 0.010734081268310547\n",
      "Timing for key2prop 0.015706777572631836\n",
      "Timing for key2prop 0.01627206802368164\n",
      "Species mixing init with eigenvalues  [0.99898595 1.00101405]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import ase.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import rascaline\n",
    "from utils.combine import UnitCombineSpecies, CombineSpecies\n",
    "from utils.dataset import AtomisticDataset, create_dataloader\n",
    "from utils.model import AlchemicalModel, SoapBpnn, CombinedPowerSpectrum\n",
    "from utils.soap import PowerSpectrum\n",
    "from ase.io import read\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "def extract_energy_forces(frames):\n",
    "    energies = (\n",
    "        torch.tensor([frame.info[\"TotEnergy\"] for frame in frames])\n",
    "        .reshape(-1, 1)\n",
    "        .to(dtype=torch.get_default_dtype())\n",
    "    )\n",
    "\n",
    "    forces = [\n",
    "        torch.tensor(frame.arrays[\"force\"]).to(dtype=torch.get_default_dtype())\n",
    "        for frame in frames\n",
    "    ]\n",
    "\n",
    "    return energies, forces\n",
    "\n",
    "frames = read(\"./data/water_converted.xyz\",index=\":2\")\n",
    "energies, forces = extract_energy_forces(frames)\n",
    "\n",
    "hypers_rs = {\n",
    "        \"cutoff\": 6.0,\n",
    "        \"max_angular\": 0,\n",
    "        \"max_radial\": 12,\n",
    "        \"atomic_gaussian_width\": 0.25,\n",
    "        \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "        \"radial_basis\": {\"SplinedGto\": {\"accuracy\": 1e-6}},\n",
    "        \"center_atom_weight\": 1.0,\n",
    "        \"radial_scaling\":  {\"Willatt2018\": { \"scale\": 2.0, \"rate\": 0.8, \"exponent\": 2}}\n",
    "    }\n",
    "hypers_ps = {\n",
    "    \"cutoff\": 4.0,\n",
    "    \"max_angular\": 4,\n",
    "    \"max_radial\": 8,\n",
    "    \"atomic_gaussian_width\": 0.3,\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"radial_basis\": {\"SplinedGto\": {\"accuracy\": 1e-6}},\n",
    "    \"center_atom_weight\": 1.0,\n",
    "    \"radial_scaling\":  {\"Willatt2018\": { \"scale\": 2.0, \"rate\": 0.8, \"exponent\": 2}}\n",
    "}\n",
    "\n",
    "atomic_data = AtomisticDataset(frames,all_species=[1,8],hypers={\"radial_spectrum\": hypers_rs, \"spherical_expansion\": hypers_ps},energies=energies,forces=forces,do_gradients=True)\n",
    "train_dataloader_no_batch = create_dataloader(\n",
    "    atomic_data,\n",
    "    batch_size=len(atomic_data),\n",
    "    shuffle=False,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "composition, radial_spectrum, spherical_expansions, energies, forces = next(\n",
    "    iter(train_dataloader_no_batch)\n",
    ")\n",
    "\n",
    "combiner = UnitCombineSpecies([1,8],2)\n",
    "soap_map = CombinedPowerSpectrum(combiner)\n",
    "\n",
    "SOAPS = soap_map.forward(spherical_expansions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpeciesWiseBPNN(layer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize_model_weights(SOAPS,energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[31.6962],\n",
       "         [31.7011]], grad_fn=<IndexAddBackward0>),\n",
       " tensor([[ 6.8181e-03, -1.6958e-04, -3.8749e-03],\n",
       "         [-1.7751e-05,  2.9390e-03, -2.8651e-03],\n",
       "         [ 8.0560e-03,  7.4388e-04, -1.0027e-03],\n",
       "         ...,\n",
       "         [ 6.6841e-03,  3.8505e-04, -4.9294e-03],\n",
       "         [-1.7343e-03,  1.0702e-03, -3.0131e-03],\n",
       "         [-3.4347e-03,  6.9243e-05,  3.1525e-03]],\n",
       "        grad_fn=<ReshapeAliasBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(SOAPS,with_forces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN = NNModel(layer_size=10)\n",
    "model_NN.initialize_model_weights(SOAPS,energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-38.3957],\n",
       "         [-38.4049]], grad_fn=<IndexAddBackward0>),\n",
       " tensor([[ 0.0050,  0.0004, -0.0024],\n",
       "         [ 0.0019, -0.0004,  0.0014],\n",
       "         [-0.0053, -0.0004, -0.0007],\n",
       "         ...,\n",
       "         [ 0.0056, -0.0024,  0.0003],\n",
       "         [ 0.0021,  0.0029,  0.0082],\n",
       "         [ 0.0019, -0.0037, -0.0083]], grad_fn=<ReshapeAliasBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NN(SOAPS,with_forces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alchemical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5b845dcd15a6975956fc0c0fd48c3756d51d0dfc5e367541816a6b20236fbb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
