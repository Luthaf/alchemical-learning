{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This implementation is an adaptation of the schnetpack implementation of atom wise-BPNNs \n",
    "#  https://schnetpack.readthedocs.io/en/stable/_modules/schnetpack/nn/blocks.html#GatedNetwork\n",
    "\n",
    "# It uses an embedding matrix that calculates Nspecies*Nsamples NN atom-wise contributions\n",
    "# If the embedding is one-hot for each species it multiplies the \"off diagonal\" NN contributions with zero\n",
    "# ie NN_hydrogen(X_central_species_is_Oxygen) * 0\n",
    "# but this embedding can also be learned\n",
    "# I have added a second implementation where, if the one_hot_encoding is chosen, the other network evaluations are supposed to be skipped\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from copy import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EquistoreDummy:\n",
    "    z: torch.Tensor\n",
    "    val: torch.Tensor\n",
    "    idx: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Embedding_one_hot(embedding):\n",
    "    pass\n",
    "\n",
    "def EmbeddingFactory(elements:List[int],one_hot:bool) -> torch.nn.Embedding:\n",
    "    \"\"\"Returns an Embedding of dim max_Z,n_unique_elements\n",
    "    max_Z = 9, n_unique = 2, elements = [1,8]\n",
    "    Embedding(tensor([8])) -> tensor([0.0,1.0]) (if one hot)\n",
    "    \"\"\"\n",
    "    \n",
    "    max_int = max(elements) + 1\n",
    "    n_species = len(elements)\n",
    "\n",
    "    embedding = torch.nn.Embedding(max_int,n_species)\n",
    "    \n",
    "    if one_hot:\n",
    "        weights = torch.zeros(max_int, n_species)\n",
    "        \n",
    "        for idx, Z in enumerate(elements):\n",
    "            weights[Z, idx] = 1.0\n",
    "\n",
    "        embedding.weight.data = weights\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    \"\"\" A simple MLP \n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: add n_hidden_layers, activation function option\n",
    "    def __init__(self, dim_input: int, dim_output: int, layer_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.dim_input, self.layer_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.layer_size, self.layer_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.layer_size, self.dim_output),\n",
    "        )\n",
    "    def forward(self,x: torch.tensor) -> torch.tensor:\n",
    "        return self.nn(x)\n",
    "\n",
    "class MultiMLP(torch.nn.Module):    \n",
    "    \"\"\" A Multi MLP that contains N_species * SimpleMLPs\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_input: int, dim_output: int, layer_size: int, species: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_output = dim_output\n",
    "        self.dim_input = dim_input\n",
    "        self.layer_size = layer_size\n",
    "        self.species = species \n",
    "        self.n_species = len(self.species)\n",
    "        self.species_nn = torch.nn.ModuleList([ SimpleMLP(dim_input,dim_output,layer_size) for _ in self.species])\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return torch.cat([nn(x) for nn in self.species_nn],dim=1)\n",
    "\n",
    "\n",
    "class MultiMLP_skip(MultiMLP):\n",
    "    \"\"\" A Multi MLP that contains N_species * SimpleMLPs\n",
    "        This Implementation does only batchwise evaluation of neural networks?\n",
    "        As this implementation skips \n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: torch.tensor, batch_z: torch.tensor) -> torch.tensor:\n",
    "        #will this work with autograd?\n",
    "        \n",
    "        unique_z_in_batch = torch.unique(batch_z)\n",
    "        \n",
    "        model_out = torch.empty((x.shape[0],self.n_species))\n",
    "\n",
    "        for n, (z, nn) in enumerate(zip(self.species, self.species_nn)):\n",
    "            if z in unique_z_in_batch:\n",
    "                model_out[batch_z == z, n] = nn(x[batch_z == z]).flatten()\n",
    "                model_out[batch_z != z, n] = torch.zeros(x[batch_z != z].shape[0])\n",
    "            else:\n",
    "                model_out[:, n] = torch.zeros(x.shape[0])\n",
    "\n",
    "        return model_out\n",
    "\n",
    "\n",
    "class MultiSpeciesMLP(torch.nn.Module):\n",
    "    \n",
    "    \"\"\" Implements a MultiSpecies Behler Parinello neural network\n",
    "    This implementation scales O(Nspecies*Natoms), but it has a learnable weight matrix, that combines species wise energies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, species, n_in, n_out, n_hidden, one_hot, embedding_trainable) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #just a precaution\n",
    "        species = copy(species)\n",
    "        species.sort()\n",
    "\n",
    "        #print(species)\n",
    "\n",
    "        self.species = species\n",
    "        self.nn = MultiMLP(n_in,n_out,n_hidden,species)\n",
    "        self.embedding = EmbeddingFactory(species, one_hot)\n",
    "\n",
    "        if not embedding_trainable:\n",
    "            self.embedding.requires_grad_ = False\n",
    "            \n",
    "\n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "        \n",
    "        x = descriptor.val \n",
    "        z = descriptor.z # something like descriptor.\n",
    "\n",
    "        #The embedding serves a a multiplicative \"mask\" -> not so nice overall complexity scales as O(N_species*N_samples)\n",
    "        # whereas an implementation that could \"skip\" NN evaluations should only scale as O(N_samples)\n",
    "        return torch.sum(self.nn(x) * self.embedding(z),dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiSpeciesMLP_skip(torch.nn.Module):\n",
    "    \n",
    "    \"\"\" Implements a MultiSpecies Behler Parinello neural network\n",
    "    This implementation should scale O(Natoms) as it skips the neural network evaluations that would be otherwise only multiplied with zeros\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, species, n_in, n_out, n_hidden) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        #just a precaution\n",
    "        species = copy(species)\n",
    "        species.sort()\n",
    "\n",
    "        #print(species)\n",
    "        \n",
    "        #TODO: Implement this properly in the MultiSpeciesMLP class\n",
    "        self.n_out = n_out\n",
    "\n",
    "        self.species = species\n",
    "        self.nn = MultiMLP_skip(n_in,n_out,n_hidden,species)\n",
    "        self.embedding = EmbeddingFactory(species, True)\n",
    "        self.embedding.requires_grad_ = False\n",
    "            \n",
    "\n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "        \n",
    "        x = descriptor.val  # something like descriptor.\n",
    "        z = descriptor.z \n",
    "\n",
    "        # here the embedding multiplication should only introduce a minor overhead\n",
    "        return torch.sum(self.nn(x,z) * self.embedding(z),dim=1)\n",
    "\n",
    "class MultiSpeciesMLP_skip_w_index_add(MultiSpeciesMLP_skip):\n",
    "    \n",
    "    \"\"\" For testing purposes I have added the atomic-contributions to structure wise properties addition\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, descriptor: EquistoreDummy) -> torch.tensor:\n",
    "                \n",
    "        x = descriptor.val  \n",
    "        z = descriptor.z \n",
    "        idx = descriptor.idx\n",
    "\n",
    "        x.requires_grad_(True)\n",
    "        num_structures = len(torch.unique(idx))\n",
    "        \n",
    "        structure_wise_properties = torch.zeros((num_structures,self.n_out))\n",
    "        atomic_contributions = torch.sum(self.nn(x,z) * self.embedding(z),dim=1,keepdim=True)\n",
    "\n",
    "        print(atomic_contributions.flatten())\n",
    "        \n",
    "        structure_wise_properties.index_add_(0,idx,atomic_contributions)\n",
    "\n",
    "        nn_grads = torch.autograd.grad(\n",
    "                structure_wise_properties,\n",
    "                x,\n",
    "                grad_outputs=torch.ones_like(structure_wise_properties),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )\n",
    "\n",
    "        return structure_wise_properties, nn_grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_block = EquistoreDummy(torch.tensor([1,1,8,1,7]),torch.ones(5,4),torch.tensor([0,0,0,1,1]))\n",
    "#multimlp = MultiSpeciesMLP_skip([1,8,7],3,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimlp_index_add = MultiSpeciesMLP_skip_w_index_add([1,8,7],4,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0752, -0.0752, -0.1002, -0.0752,  0.1738],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2505],\n",
       "         [ 0.0986]], grad_fn=<IndexAddBackward0>),\n",
       " (tensor([[-0.0495,  0.0438,  0.0283,  0.0377],\n",
       "          [-0.0495,  0.0438,  0.0283,  0.0377],\n",
       "          [ 0.0351,  0.0911, -0.0211, -0.0484],\n",
       "          [-0.0495,  0.0438,  0.0283,  0.0377],\n",
       "          [ 0.0325,  0.0733, -0.1261,  0.0340]], grad_fn=<AddBackward0>),))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimlp_index_add.forward(a_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2506"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([-0.0752, -0.0752, -0.1002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09860000000000001"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([-0.0752,  0.1738])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f481d707f485e5e545fcbec8e23552587d82164162b1e7024dc6dabd3ca7d616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
